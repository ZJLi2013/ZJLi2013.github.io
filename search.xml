<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[vmware virtualization]]></title>
    <url>%2F2020%2F03%2F28%2Fvmware-virtualization%2F</url>
    <content type="text"><![CDATA[backgroundsooner or later, we are in the direction of gpu virtualization, previously hypervisor and gpu virtual is the first blog. recently, I’d went through openstack/kvm, vnc and now vmware. there is no doubt, any licensed product is not my prefer, but on the hand, it’s more expensive to hire an openstack engineer, compared to pay vmware. vmware is not a windows-only, I had to throw my old mind first. the basic or back-end idea looks very similar to kvm. anyway, the core is hypervisor layer. once get understand one type of virtualization, it’s really easy to understand another. VMware ESXiESXi hypervisor is a Type1 or “bare metal” hypervisor, also called vmware hypervisor, is a thin layer of software that interacts with the underlying resources of a physical computer(host machine), and allocates those resources to other os(guest machine).and can support remotely access, just like kvm. check vSphere doc about how to set BIOS and manage ESXi remotely. BIOS boot configuration can be setted by configuring the boot order in BIOS during startup or by selecting a boot device from the boot device selection menu. the system BIOS has two options. One is for the boot sequence (floppy, CD-ROM, hard disk) and another for the hard disk boot order (USB key, local hard disk). VMware workstationworkstation support multi guest os in a single host os(either windows or Linux), is a Type2 hypervisor, run as an app on host OS. one limitation of workstation is it only works in local host, can’t access remotely. free version, workstation player licensed version, workstation prof in one word, workstation is good enough to multiple hardware usage, but not useful if remote access is required VMware vSpherethe arch of vSphere has three layers: virtualization layer management layer interface layer(web, sdk, cli, virtual console) ESXi is the core hypervisor for VMware products, and also is the core of the vSphere package, the other two is: vSphere client and vSphere server. vSphere server is enterprise-oriented, which is run based on ESXi layer. vSphere client is a client console/terminal. free version: vSphere hypervisor licensed fee version : vSphere with vServer nowadays there are no limitations on Physical CPU or RAM for Free ESXi. here 12345Specifications: Number of cores per physical CPU: No limitNumber of physical CPUs per host: No limitNumber of logical CPUs per host: 480Maximum vCPUs per virtual machine: 8 virtual desktop integration (VDI)VDI virtualize a desktop OS on a server. VDI offers centralized desktop management. the vmware tool is VMware Horizon HorizonVMware Horizon can run VDI and apps in IT data center, and make VDI and apps as services to users. Horizon auto manage VDI and apps by simple setup configure files, then deliver apps or data from data center to end-user. the modules in Horizon is extended-able and plug-in available: physical layer, virtualization layer, desktop resource layer, app resource layer and user access. Horizon basically delivers desktops and apps as a service. there are three versions: Horizon standard, a simple VDI. Horizon advanced, can deliver desktops and apps through a unified workspace Horizon enterprise, with a closed-loop management adn automation the Horizon 7 has new features: Blast extrem display protocol instant clone provisioning vm app volumes app delivery user env manager integrated into remote desktop session host(RDSH) sessions gpu support in vmwarefor vSphere, PCI passthrough can be used with any vSphere, including free vSphere Hypervisor. the only limitation is HW, may not supprot virtual well. GPU remotely accessible is our first-priority concern. but vmware recommend their Horzion with NV’s vGPUwhich has better flexibility and scalability. Horizon support vGPU, the user must install the appropriate vendor driver on the guest vm, all graphics commands are passed directly to GPU without having to be translated by the hypervisor. a vSphere Installation Bundle(VIB) is installed, which aids or perform the scheduling. depending on the card, up to 24 vm can share a GPU. most NV’s GPU which has vGPU feature can support. on the other hand, the price of vGPU products(e.g. T4, M10, P6, V100, RTX8000 e.t.c) are 5 ~ 10 times higher than normal customer GPUs. e.g. GeFS not 5~10 times better. and the license fee for vgpu is horrible. however, most enterprise still choice the Horzion and vGPU solution, even with this high cost. VMware compatibility guide GPU VDI service in cloud tencent gpu cloud the gpu types for video decoding is Tesla P4, for AI and HPC is Tesla v100, and for image workstation(VDI) is an AMD S7150. ali gpu cloud desktop the product is call GA1(S7150), which is specially for cloud desktop. s7150x2 MxGPU with Horizon 7.5 vnc vs vmvirtual network computing (vnc), applications running on one computer but displaying their windows on another. VNC provides remote control of a computer at some other location, any resources that are avaiable at the remote computer are available. vpn simply connect you to a remote network. no doubt, vm is much heavier than vnc. check this blog for compartion from vdi(a vm app) to vnc. vnc can’t tell if the remote is a physical server or a virtual server. come to our user case, we need about 100 separated user space, so virtualization provide better HW efficient and security, compared to deploy a single bare metal OS on the physical machine. there are a few Linux based vnc client/server, e.g. vncviewer CLI, as well as which supports OpenGL well, which helps to support better GPU usage. virtualGL x11vnc refeean essential vmware introduction from IBM what are vsphere, esxi, vcenter in Chienese vSphere Hypervisor vmware vsphere doc how to enable nvidia gpu in passthrough mode on vSphere nvidia vgpu for vmware release notes how to enable vmware vm for GPU pass-through openstack PCI passthrough how can openGL graphics be displayed remotely using VNC vmware Horizon introduction in Chinese vmware ESXi 7.0 U1 test env build up 云游戏能接盘矿卡市场吗 王哥哥的博客 企业存储的博客 in-depth: nv grid vGPU with vmware horizon 6.1 nvidia gpus recommended for virtualization GPU SRIOV and AMD S7150]]></content>
      <tags>
        <tag>vmware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kvm in linux (2)]]></title>
    <url>%2F2020%2F03%2F27%2Fkvm-in-linux-2%2F</url>
    <content type="text"><![CDATA[history of terminalin history, computer is so huge and hosted at a big room, while the operator stay in another office room with another small machine as terminal to the host computer. as the host computer can run multi-users, each of which needs a terminal. when times goes to personal computer(PC), there is no need for multi-users login, so each PC has integrated with I/O device(monitor and keybord) nowadays, we have plenty of end-user terminals, e.g. smart phone, smart watch, Ipad e.t.c, all of these are terminal, as the real host computer is in cloud now. in a word, any device that can accept input and display output is a terminal, which play the role as the human-machine interface. three kinds of terminalssh is TCP/IP protocol, what’s inside is the remote terminal streaming flow. the TCP/IP plays as the tunnel. of course, any communication protocol can play the role as the tunnel. local terminal, with usb to keyboard and monitor. serial terminal, the guest machine connect to the host machine, who has keyboard and monitor. basically the guest machine borrow the host machine’s IO, which needs the host machine to run a terminal elmulator. tcp/ip tunnel terminal, e.g. ssh both local terminal and serial terminal, directly connect to physical device, e.g. VGA interface, usb, serial e.t.c, so both are called physical terminal. ssh has nothing to do with physical device. ttyin Linux, /dev/ttyX represents a physical-terminal. from tty1 to tty63 are all local terminal. during Linux kernal init, 63 local terminals are generated, which can be switched by Fn-Alt-Fx, (x can be 1, 2, 3…). each current terminal is called focus terminal. focus terminal is taken as global variable, so any input will transfer to current focus terminal. for serial, there is no focus terminal. in Linux, /dev/console represents current focus terminal, consider as a pointer to current focus terminal, wherever write sth to /dev/console will present at current focus terminal. /dev/tty is the global variable itself. whichever terminal you are working with, when write to /dev/tty, it will be present. /dev/ttyS#num# represents serial terminal. getty &amp; loginin multi-terminal time, each terminal must bind to one user, user must first login the terminal, getty is the login process, which is called in init. after login successfully, the terminal tty#num# can be owned by the user. there are a few differenet version of getty, e.g. agetty e.t.c. pty &amp; ptspty stands for pseudo-tty, pty is a (master-slave) pair terminal device, including: pts pseudo-terminal slave, and ptmx pseudo-terminal master. a few concepts as following: serial terminal /dev/ttySn pseudo-termianl /dev/pty/ controlling terminal /dev/tty console terminal /dev/console Linux Serial Consoleserial communicationin old-style PC, serial is COM interface, also called DB9 interface, with RS-232 standard. each user can connect to host machine through a terminal. console is same as terminal to connect user to host machine, but console is higher priority than terminal. nowadays less difference between terminal and console. Teletype is the earliest terminal device, tty is physical or pseudo terminal connect to the host machine, nowadays tty also used for serial device. serial is the connection from terminal/keyboard to dev-board. 1ls /dev/tty* configurationLinux kernel must be configured to use the serial port as its console, which is done by passing the kernel the console parameter when the kernel is started by the boot loader. the init system should keep a process runnign to monitor the serial console for logins, the monitoring process is traditionally named getty a number of system utilities need to be configured tomake them aware of the console, or configured to prevent them from disrupting the console. serial portLinux names as the first serial port has the file name /dev/ttys0, the second serial port has the file name /dev/ttyS1 and so on. most boot loaders have yet another naming scheme, the first serial port is numbered 0, the second serial port is numbered 1 configure GRUB boot loaderconfigure GRUB to use the serial console 1234info grub/boot/grub/grub.cfgserial --unit=0 --speed=9600 --word=8 --parity=no --stop=1 terminal serial init systemgetty is started by init: 1co:2345:respawn:/sbin/getty ttyS0 CON9600 vt102 co is an arbitrary entry, representing console 2345, run levels where this entry gets started. respawn, re-run the program if it dies /sbin/getty ttyS0 CON9600 vt102, getty connecting to /dev/ttyS0 with the settings for CON9600bps, and the terminal is VT100 model virsh consolehow to connect ubuntu kvm virtual machine through serial console. in earlier version and distributions, it need to configure serial console in grub file, but in Ubuntu it’s very easy adn reliable as most of configurations and settings are already configured in OS. setupruns ubuntu14.04 guest mahcine on ubuntu 16.04 host machine. how to setup serial console, we have to connect guest machine and login on as root user login through SSH connect on KVM guest machine through ssh from host machine 12ssh 192.168.122.1hostname connect through VNCconenct guest machine through VNC viewer and setup serial console. There are times when we need to troubleshoot Virtual Machines with unknown status like Hang in between, IP address issues, password problems, Serial console Hang etc. In case scenarios, we could relay on VNC configuration of KVM Guest Machines. vnc viewer is a graphic viewer, so only need add graphics component in config.xml: 1&lt;graphics type='vnc' port='-1' autoport='yes' passwd='mypassword'/&gt; run virsh vncdisplay #vm_name# we can get our vnc (server) IP, which then can be accessed by vnc guest viewer. here, kvm virtual machine has implemented a vnc server, and any vnc viewer in the same physical machine, can access this vnc server, even without external networking. configure serial console in ubuntu guestafter getting login console, we can start serial console and enable it with: 123# systemctl start serial-getty@ttyS0# systemctl enable serial-getty@ttyS0Created symlink /etc/systemd/system/getty.target.wants/serial-getty@ttyS0.service → /lib/systemd/system/serial-getty@.service. now we can connect serial console with virsh console: 1virsh console vm_name after installnation, reboot first, then the physical machine has dual-OS: GuestOS and HostOS, which can exit GuestOS by Ctrl + ], or login GuestOS by virsh consoel #guest_vm# in summary, virsh console implement a serial console for kvm guest machine, which connect the guest machine to host machine through serial, which is not a ssh, need new knowledges about serial. virsh console hangsvirsh console vm hangs at: Escape character is ^], which can exit by ctrl + ] sol1:go to guest machine/terminal, and edit /etc/default/grub, appending; 12GRUB_TERMINAL=serialGRUB_SERIAL_COMMAND="serial --unit=0 --speed=115200 --word=8 --parity=no --stop=1" then execute: 12update-grubreboot the problem here, as the KVM vm shares the kernel of the host machine, if update grub, the host machine will reboot with serial connection(?) Centos virsh console hangs go to /etc/securetty and append ttyS0 append S0:12345:respawn:/sbin/agetty ttyS0 115200 to /etc/init/ttyS0.conf /etc/grub.conf(Centos) I only found /boot/grub/grub.cfg in ubuntu. In the kernel line, appending console=ttyS0. but there is no kernel line in ubuntu grub.cfg. Ubuntu virsh console hangs123systemctl disable systemd-networkd-wait-onlinesystemctl enable serial-getty@ttyS0.servicesystemctl start serial-getty@ttyS0.service which gives: 12Mar 27 11:17:18 ubuntu systemd[1]: Started Serial Getty on ttyS0.Mar 27 11:17:18 ubuntu agetty[445120]: /dev/ttyS0: not a tty check in /dev/ttyS* : 123crw-rw---- 1 root dialout 4, 73 Mar 24 09:20 ttyS9crw--w---- 1 root tty 4, 64 Mar 24 09:20 ttyS0crw-rw---- 1 root dialout 4, 65 Mar 24 09:20 ttyS1 interesting here, ttyS0 belongs to tty group, all otehr ttyS#num# belongs to dialout group. tty and dialoutchange /dev/ttyS0 to tty group can’t access /dev/ttyS add USER to tty/dialout group 12sudo usermod -a -G tty $USERsudo usermod -a -G dialout $USER reboot and go on referremote serial console HOWTO remote serial console HOWTO in Chinese understand Linux terminal history in Chinese Linux terminal introduction in Chinese serial communication in Chinese arhLinux: working with the serial console gnu org: grub geekpills: start vnc remote access for guest operating systems]]></content>
      <tags>
        <tag>linux</tag>
        <tag>kvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kvm/libvert in linux (1)]]></title>
    <url>%2F2020%2F03%2F24%2Fkvm-libvert-in-linux-1%2F</url>
    <content type="text"><![CDATA[kvm backgroundkernel based virtual machine(KVM), is a Linux kernel module, which transfer Linux to a Hypervisor, which depends on the ability of hardware virtualization. usually the physical machine is called Host, and the virtual machine(VM) run in host is called Guest. kvm itself doesn’t do any hardware emulator, which needs guest space to set an address space through dev/kvm interface, to which provides virtual I/O, e.g. QEMU. virt-manager is a GUI tool for managing virtual machines via libvirt, mostly used by QEMU/KVM virtual machines. check kvm model info 1modinfo kvm whether CPU support hardware virtualization 12egrep -c '(vmx|svm)' /proc/cpuinfokvm-ok install kvm install libvirt and qemu packages 1234sudo apt install qemu qemu-kvm libvirt-bin bridge-utilsmodprobe kvm #load kvm modulesystemctl start libvirtd.service #vrish iface-bridge ens33 virbr0 #create a bridge network mac address add current user to libvirtd group 123sudo usermod -aG libvirtd $(whoami)sudo usermod -aG libvirt-qemu $(whoami)sudo reboot network in kvmdefault network is NAT(network address transation), when you create a new virtual machine, this forwards network traffic through your host system; if the host is connected to the Internet, then your vm have Internet access. VM manager also creates an Ethernet bridge between the host and virtual network, so can ping IP address of VM from host, also ok on the other way. List of network cards go to /sys/class/net there are a few nic: 123456789lrwxrwxrwx 1 root root 0 Mar 24 16:18 docker0 -&gt; ../../devices/virtual/net/docker0lrwxrwxrwx 1 root root 0 Mar 24 16:18 docker_gwbridge -&gt; ../../devices/virtual/net/docker_gwbridgelrwxrwxrwx 1 root root 0 Mar 24 16:18 eno1 -&gt; ../../devices/pci0000:00/0000:00:1f.6/net/eno1lrwxrwxrwx 1 root root 0 Mar 24 16:18 enp4s0f2 -&gt; ../../devices/pci0000:00/0000:00:1c.4/0000:02:00.0/0000:03:03.0/0000:04:00.2/net/enp4s0f2lrwxrwxrwx 1 root root 0 Mar 24 16:18 lo -&gt; ../../devices/virtual/net/lolrwxrwxrwx 1 root root 0 Mar 24 16:18 veth1757da9 -&gt; ../../devices/virtual/net/veth1757da9lrwxrwxrwx 1 root root 0 Mar 24 16:18 vethd4d0e7f -&gt; ../../devices/virtual/net/vethd4d0e7flrwxrwxrwx 1 root root 0 Mar 24 16:18 virbr0 -&gt; ../../devices/virtual/net/virbr0lrwxrwxrwx 1 root root 0 Mar 24 16:18 virbr0-nic -&gt; ../../devices/virtual/net/virbr0-nic multi interfaces on same MAC addresss when a switch receives a frame from an interface, it creates an entry in the mac-address table with the source mac and interface. it the source mac is known, it will update the table with the new interface. so bascially if you assign the mac address of an external-network-avialable NIC-A to the special vm, NIC-A is lost. virbr0 the default bridge NIC of libvirt is virbr0. bridge network means the guest and host share the same physical Network Cards, as well as offer the guest a special IP, which can be used to access the guest directly. the virbr0 do network address translation(NAT), basically transfer the internal IP address to an external IP address, which means the internal IP address is un-visiable from outside. to add the virbr0, when it is deleted previously: 1234brctl addbr virbr0brctl stp virbr0 onbrctl setf virbr0 0ifconfig virbr0 192.168.122.1 netmask 255.255.255.0 up to disable or delete virbr0: 1234virsh net-destroy defaultvirsh net-undefine defaultservice libvirtd restartifconfig after starting the vm, can check the bridge network by: 12virsh domiflist vm-namevirsh domifaddr vm-name and we can login the vm, (after we assign current user to libvert group), and check NAT is working: 123ssh 192.168.122.1 ping www.bing.comping 10.20.xxx.xxx # ping the host external IP basically the vm can access external website, but external web can’t access vm_name. 1attach-interface/detach-interface/domiflist create vmcreate a virtual machine, can be done either through virt-install or config.xml: virt-installvirt-install has depends on system python, pip. if current ptyhon version is 2.7, it gives warnning and return -1 due to unfound module. so make sure the #PYTHONPATH# point to the correct path if you have multi python in system. and virt-install has to run with root. then can start a virtual machine with following command options) 1234567891011121314sudo virt-install \--name v1 \--ram 2048 \# --cdrom=ubuntu-16.04.3-server-amd64.iso \--disk path=/var/lib/libvirt/images/ubuntu.qcow2 \--vcpus 2 \--virt-type kvm \--os-type linux \--os-variant ubuntu16.04 \--graphics none \--console pty, target_type=serial \--location /var/lib/libvirt/images/ubuntu-16.04.3-server-amd64.iso \--network bridge:virbr0 \ --extra-args console=ttyS0 during the installation, the process looks very much like Linux installation on a bare machine. I suppose this way, it’s like install a dual-OS in the bare machine. during the installation, there is an error failed to load installer component libc6-udeb, it’s may due to the iso or img has missing component. config.xml create volumes go to /var/lib/libvirt/images, and create volume as following: 1qemu-img create -f qcow2 ubuntu.qcow2 40G check qemu-kvm &amp; qemu-img introduction add vm image cp ubuntu.iso to /var/lib/libvirt/images as well: 12ubuntu.qcow2ubuntu-16.04.3-server-amd64.iso vm.xmlfollow an xml sample: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;domain type='kvm'&gt; &lt;name&gt;v1&lt;/name&gt; &lt;memory&gt;4048576&lt;/memory&gt; &lt;currentMemory&gt;4048576&lt;/currentMemory&gt; &lt;vcpu&gt;2&lt;/vcpu&gt; &lt;os&gt; &lt;type arch='x86_64' machine='pc'&gt;hvm&lt;/type&gt; &lt;boot dev='cdrom'/&gt; &lt;/os&gt; &lt;features&gt; &lt;acpi/&gt; &lt;apic/&gt; &lt;pae/&gt; &lt;/features&gt; &lt;clock offset='localtime'/&gt; &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt; &lt;on_reboot&gt;restart&lt;/on_reboot&gt; &lt;on_crash&gt;destroy&lt;/on_crash&gt; &lt;serial type='pty'&gt; &lt;target port='0' /&gt; &lt;/serial&gt; &lt;console type='pty' &gt; &lt;target type='serial' port='0' /&gt; &lt;/console&gt; &lt;devices&gt; &lt;emulator&gt;/usr/bin/qemu-system-x86_64&lt;/emulator&gt; &lt;disk type='file' device='disk'&gt; &lt;driver name='qemu' type='qcow2'/&gt; &lt;source file='/var/lib/libvirt/images/ubuntu.qcow2'/&gt; &lt;target dev='hda' bus='ide'/&gt; &lt;/disk&gt; &lt;disk type='file' device='cdrom'&gt; &lt;source file='/var/lib/libvirt/images/ubuntu-16.04.3-server-amd64.iso'/&gt; &lt;target dev='hdb' bus='ide'/&gt; &lt;/disk&gt; &lt;interface type='bridge' &gt; &lt;mac address='52:54:00:98:45:3b' /&gt; &lt;source bridge='virbr0' /&gt; &lt;model type='virtio' /&gt; &lt;/interface&gt; &lt;serial type='pty'&gt; &lt;target port='0' /&gt; &lt;/serial&gt; &lt;console type='pty'&gt; &lt;target type='serial' port='0' /&gt; &lt;/console&gt; &lt;input type='mouse' bus='ps2'/&gt; &lt;graphics type='vnc' port='-1' autoport='no' listen = '0.0.0.0' keymap='en-us'&gt; &lt;listen type='address' address='0.0.0.0' /&gt; &lt;/graphics&gt; &lt;/devices&gt; &lt;/domain&gt; a few tips about the xml above: \ component is necessary for network interface. if not assign a special mac address in the interface. since we had define virbr0, an automatic mac address will be assigned, which is unique from the host machine’s IP, but if ssh login to the guest (ssh username@guest_ip), it actually can ping host machine’s iP or any external ip(www.being.com) \ compoennt, is setting for console. finally run the following CLI to start vm: v1: 123virsh define vm1.xml virsh start ubuntu(the image)virsh list libvertlibvert is a software package to manage vm, including libvirtAPI, libvirtd(daemon process), and virsh tool. 12sudo systemctl restart libvirtdsystemctl status libvirtd only when libvirtd service is running, can we manage vm through libvert. all configure of the vm is stored ad /etc/libvirt/qemu. for virsh there are two mode: immediate way e.g. in host shell virsh list interactive shell e.g. by virsh to virsh shell common virsh commands123456789101112131415161718virsh &lt;command&gt; &lt;domain-id&gt; [options]virsh uri #hypervisor&apos;s URIvirsh hostnamevirsh nodeinfo virsh list (running, idel, paused, shutdown, shutoff, crashed, dying)virsh shutdown &lt;domain&gt;virsh start &lt;domain&gt;virsh destroy &lt;domain&gt;virsh undefine &lt;domain&gt;virsh create #through config.xmlvirsh connect #reconnect to hypervisor virsh nodeinfo virsh define #file domainvirsh setmem domain-id kbs #immediatelyvirsh sertmaxmem domain-id kbsvirsh setvcpus domain-id count virsh vncdisplay domain-id #listed vnc portvirsh console &lt;domain&gt; virsh network commands host configure Every standard libvirt installation provides NAT based connectivity to virtual machines out of the box. This is the so called ‘default virtual network’ 12345virsh net-list virsh net-define /usr/share/libvirt/networks/default.xmlvirsh net-start defaultvirsh net-info defaultvirsh net-dumpxml default When the libvirt default networkis running, you will see an isolated bridge device. This device explicitly does NOT have any physical interfaces added, since it uses NAT + forwarding to connect to outside world. Do not add interfaces. Libvirt will add iptables rules to allow traffic to/from guests attached to the virbr0 device in the INPUT, FORWARD, OUTPUT and POSTROUTING chains. if default.xml is not found, check fix missing default network, default.xml is sth like: 123456789101112&lt;network&gt; &lt;name&gt;default&lt;/name&gt; &lt;uuid&gt;9a05da11-e96b-47f3-8253-a3a482e445f5&lt;/uuid&gt; &lt;forward mode='nat'/&gt; &lt;bridge name='virbr0' stp='on' delay='0'/&gt; &lt;mac address='52:54:00:0a:cd:21'/&gt; &lt;ip address='192.168.122.1' netmask='255.255.255.0'&gt; &lt;dhcp&gt; &lt;range start='192.168.122.2' end='192.168.122.254'/&gt; &lt;/dhcp&gt; &lt;/ip&gt;&lt;/network&gt; then run: 123sudo virsh net-define --file default.xmlsudo virsh net-start defaultsudo virsh net-autostart --network default if bind default to virbr0 already, need delete this brige first. guest configure add the following to guest xml configure: 1234&lt;interface type='network'&gt; &lt;source network='default'/&gt; &lt;mac address='00:16:3e:1a:b3:4a'/&gt;&lt;/interface&gt; more details can check virsh networking doc snapshotssnapshots used to save the state(disk mem, time..) of a domain create a snapshot for a vm 123virsh snapshot-create-as --domain test_vm \--name "test_vm_snapshot1" \ --description "test vm snapshot " list all snapshots for vm 1virsh snapshot-list test_vm display info about a snapshot 1234567virsh snapshot-info --domain test_vm --snapshotname test_vm_snapshot1``` * delete a snapshot ```shvirsh snapshot-delete --domain test_vm --snapshotname test_vm_shapshot1 manage volumes create a storage volume 123virsh vol-create-as default test_vol.qcow2 10G# create test_vol on the deafult storage pooldu -sh /var/lib/libvirt/images/test_vol.qcow2 attach to a vm attache test-vol to vm test 123virsh attach-disk --domain test \--source /var/lib/libvirt/images/test-vol.qcow2 \--persistent --target vdb which can be check that the vm has added a block device /dev/vdb 12ssh test #how to ssh to vm lsblk --output NAME,SIZE,TYPE or directly grow disk image: 1qemu-img resize /var/lib/libvirt/images/test.qcow2 +1G detach from a vm 1virsh detach-disk --domain test --persistent --live --target vdb delete a vm 123virsh vol-delete test_vol.qcow2 --pool defaultvirsh pool-refresh defaultvirsh vol-list default fs virsh commands12virt-ls -l -d &lt;domain&gt; &lt;directory&gt;virt-cat -d &lt;domain&gt; &lt;file_path&gt; referkvm introduction in chinese kvm pre-install checklist Linux network configuration kvm installation official doc creating vm with virt-install install KVM in ubuntu jianshu: kvm network configure cloudman: understand virbr0 virsh command refer qcow2 vs raw]]></content>
      <tags>
        <tag>kvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[real-time matlab/simulink code generation]]></title>
    <url>%2F2020%2F03%2F20%2Freal-time-matlab-simulink-code-generation%2F</url>
    <content type="text"><![CDATA[real-time matlab code generationbackgroudPython and Linux vs Matlab and Windows, I prefer the front. but as a teamwork, I have to understand how matlab/Simulink code generation works, especially with real-time model. Simulink Coderpreviously named as Real-Time workshop(rtw). real time model data structureaccess rtModel data by using a set of macros analogous to the ssSetxxx and ssGetxxx macros that S-functions use to access SimStruct data, including noninlined S-functions compiled by the code generator. You need to use the set of macros rtmGetxxx and rtmSetxxx to access the real-time model data structure. The rtModel is an optimized data structure that replaces SimStruct as the top level data structure for a model. The rtmGetxxx and rtmSetxxx macros are used in the generated code as well as from the main.c or main.cpp module. Usage of rtmGetxxx and rtmSetxxx macros is the same as for the ssSetxxx and ssGetxxx versions, except that you replace SimStruct S by real-time model data structure rtM. rtm macro description rtmGetdX(rtm) get the derivatives of block continous states rtmGetNumSampleTimes(RT_MDL rtM) Get the number of sample times that a block has rtmGetSampleTime(RT_MDL rtM, int TID) Get task sample time rtmGetStepSize(RT_MDL) Return the fundamental step size of the model rtmGetT(RT_MDL,t) Get the current simulation time rtmGetErrorStatus(rtm) Get the current error status code generation to used externally :1) Install the Real-Time Workshop (RTW) Toolbox for MATLAB; 2) Create the Simulink Model and Prepare it for autocoding; 3) Correctly configure the RTW options and include a *.tcl file; 4) Build any S-Functions of the model; 5) Build the model (generate autocode including makefile); 6) Tune up the makefile with any missing options/libraries/files; 7) Integrate autocoded model in RTEMS using the wrapper. ert_main()the following is a common sample of real-time model generated C code. 1234567891011rt_OneStep(void)&#123; simulation_custom_step() ;&#125;main()&#123; simulation_initialize(); while(rtmGetErrorStatus(xx_M) == (NULL))&#123; rt_OneStep(); &#125; simulation_terminate(); return 0; if no time interupt setting up, there is a Warning: The simulation will run forever. Generated ERT main won’t simulate model step behavior. To change this behavior select the ‘MAT-file logging’ option. from real-time Matlab/Simulink model to C/C++ code, we need manually set the while-loop break statement. for example, we can run 100 times or based on some event-trigger. Timing12345678 struct &#123; uint16_T clockTick1; //base rate counter (5s) struct &#123; uint16_T TID[2]; &#125; TaskCounters; //subtask counter (0.02s) &#125; Timing; currentTime = Timing.clockTick1 * 5.0 ; absolute timer for sample time: [5.0s, 0.0s]. the resolution of this integer timer is 5.0, which is the step size of the task. so bascially, assuming to run one step need physcially 5s, but inside the module, each internal step is 0.02s. if we run a test scenario with 20s, basically we have 4 clockTick1 and inside each clockTick1, we have 250 times internal steps. a few modificationthe following is a few modification based on the auto-generated C code: redefine data structure matlab coder use most C structure to package signals. in our adas model, most signals have similar inner items, so first I’d like to define a parent structre, then define all other adas signals using typedef: 1234typedef structure parent adas_sig1 ;typedef structure parent adas_sig2 ;typedef structure parent adas_sig3 ;typedef structure parent adas_sig4 ; with the parent struct, we can define one method to handle all adas signals. add trigger model in rt_oneStep() 1234567891011121314checkTriggerSigs(&amp;FunctionSignal, outputs_name); int idx = 0; for(; idx&lt;outputs_name.size(); idx++)&#123; if ( outputs_name[idx] == "adas_sig1") &#123; double *vals = gd1_struc_2arr(adas_ig1) ; outputs_data.push_back(als); &#125; else if( outputs_name[idx] == "adas_sig2") &#123; double *vals = gd1_struc_2arr(adas_sig2) ; outputs_data.push_back(vals); &#125;// ws msg send model add sim_time to break the loop 12345real_T sim_time = 5.0 ; fflush((NULL));while (rtmGetErrorStatus(OpenLoopSimulation_M) == (NULL) &amp;&amp; (Timing.clockTick1) * 5.0 &lt;= sim_time) &#123; rt_OneStep(); &#125; in summarythe work above is the core modifcation to make real-time matlab/simulink model with trigger model translated to C/C++ code, which can be integrated in massively adas test pipeline. refermatlab code generation from rtems the joy of generating C code from MATLAB matlab coder introduction matlab code doc real-time model data structure generate code from rate-based model schedule a subsystem multiple times in a single step]]></content>
  </entry>
  <entry>
    <title><![CDATA[massively adas test pipeline]]></title>
    <url>%2F2020%2F03%2F18%2Fmassively-adas-test-pipeline%2F</url>
    <content type="text"><![CDATA[backgroundADAS engineers in OEM use Matlab/Simulink(model based design) to develop the adas algorithms, e.g. aeb. Simulink way is enough for fuction verification in L2 and below; for L2+ scenario, often need to test these adas functions in system level, basically test it in scenarios as much as possible, kind of L3 requirements. one way to do sytem level verification is through replay, basically the test vehicle fleet can collect large mount of data, then feed in a test pipeline, to check if these adas functions are triggered well or missed. for replay system test, we handle large mount of data, e.g. Pb, so the Simulink model run is too slow. the adas test pipeline with the ability to run massively is required. the previous blog high concurrent ws server mentioned about the archetecture of this massively adas test pipeline: with each adas model integrated with a websocket client, and all of these ws clients talk to a websocekt server, which has api to write database. encode in Cthe adas simulink model can be encode in c, which of course can encode as c++, while not powerful yet. as in C is more scalable then simulink/matlab. matlab/simulink encode has a few choices, massively run is mostly in Linux-like os, here we choose ert env to encode the model, after we can build and test as: 123gcc -c adas_model.c -I . gcc -c ert_main.c gcc ert_main.o adas_model.c -o mytest json-cas all messages in adas model in c is stored in program memory, first thing is to serialize these to json. here we choose json-c: install on local Ubuntu 1234sudo apt-get install autoconf automake libtool sh autogen.sh./configuremake &amp;&amp; make install then the json-c header is at: /usr/local/include/json-c and the libs at: /usr/local/lib/libjson-c.so *.al when using we can add the following flags: 123JSON_C_DIR=/path/to/json_c/installCFLAGS += -I$(JSON_C_DIR)/include/json-cLDFLAGS+= -L$(JSON_C_DIR)/lib -ljson-c the json object can be created as: 12345678910struct json_object *json_obj = json_object_new_object();struct json_object *json_arr = json_object_new_array();struct json_object *json_string = json_object_new_string(name);int i=0;for(; i&lt;20; i++)&#123; struct json_object *json_double = json_object_new_double(vals[i]); json_object_array_put_idx(json_arr, i, json_double);&#125;json_object_object_add(json_obj, "name", json_string);json_object_object_add(json_obj, "signals", json_arr); modern c++ json libs are more pretty, e.g. jsoncpp, rapidJSON, json for modern c++ wsclient-cthe first ws client I tried is: wsclient in c, with default install, can find the headers and libs, separately at: 12/usr/local/include/wsclient/usr/local/lib/libwsclient.so or *.a when using: 1gcc -o client wsclient.c -I/usr/local/include -L/usr/local/lib/ -lwsclient onopen()as we need send custom message through this client, and the default message was sent inside onopen(), I have to add additionaly argument char\* into the default function pointer onopen 123456789101112int onopen(wsclient *c, char* message) &#123; libwsclient_send(c, message); return 0;&#125; void libwsclient_onopen(wsclient *client, int (*cb)(wsclient *c, char* ), char* msg) &#123; pthread_mutex_lock(&amp;client-&gt;lock); client-&gt;onopen = cb; pthread_mutex_unlock(&amp;client-&gt;lock);&#125; if(pthread_create(&amp;client-&gt;handshake_thread, NULL, libwsclient_handshake_thread, (void *)client)) &#123; and the onopen callback is actually excuted inside handshake thread, in which is not legacy to pass char* message. further as there is no global alive status to tell the client-server channel is alive, to call libwsclient_send() in another thread sounds trouble-possible. looks wsclient-c is limited, I transfer to wsclient c++. but need make sure model in c workable with g++. wsclient c++websocketpp is a header only C++ library, there is no libs after built, but it is depends on boost, so to use this lib, we can add header and libs as following: 12/usr/local/include/websocketpp /usr/lib/x86_64-linux-gnu/libboost_*.so I am using the wsclient from sample, and define a public method as the client process: 123456789101112131415161718192021222324int client_process(std::string&amp; server_url, std::string&amp; message)&#123; websocket_endpoint endpoint; int connection_id = endpoint.connect(server_url); if (connection_id != -1) &#123; std::cout &lt;&lt; "&gt; Created connection with id " &lt;&lt; connection_id &lt;&lt; std::endl; &#125; connection_metadata::ptr mtdata = endpoint.get_metadata(connection_id); //TODO: optimized with this sleeping time boost::this_thread::sleep(boost::posix_time::milliseconds(200)); int retry_num = 0; while(mtdata-&gt;get_status() != "Open" &amp;&amp; retry_num &lt; 100)&#123; std::cout &lt;&lt; "&gt; connected is not open " &lt;&lt; connection_id &lt;&lt; std::endl; boost::this_thread::sleep(boost::posix_time::milliseconds(100)); connection_id = endpoint.connect(server_url); mtdata = endpoint.get_metadata(connection_id); &#125; if(mtdata-&gt;get_status() != "Open") &#123; std::cout &lt;&lt; "retry failed, exit -1" &lt;&lt; std::endl ; return 0; &#125; endpoint.send(connection_id, message); std::cout &lt;&lt; message &lt;&lt;" send successfully" &lt;&lt; std::endl ; there is more elegent retry client solution. to build our wsclient: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 g++ wsclient.cpp -o ec -I/usr/local/include -L/usr/lib/x86_64-linux-gnu -lpthread -lboost_system -lboost_random -lboost_thread -lboost_chrono ``` ## ws server in pythonwe implemented a simple ws server with [websockets]():```python#!/usr/bin/env python3import asyncioimport websocketsimport json from db_ model import dbwriter, adas_msg, Baseclass wsdb(object): def __init__(self, host=None, port=None): self.host = host self.port = port self.dbwriter_ = dbwriter() async def process(self, websocket, path): try: raw_ = await websocket.recv() jdata = json.loads(raw_) orm_obj = adas_msg(jdata) try: self.dbwriter_.write(orm_obj) self.dbwriter_.commit() except Exception as e: self.dbwriter_.rollback() print(e) except Exception as e: print(e) greeting = "hello from server" await websocket.send(greeting) print(f"&gt; &#123;greeting&#125;") def run(self): if self.host and self.port : start_server = websockets.serve(self.process, self.host, self.port) else: start_server = websockets.serve(self.process, "localhost", 8867) asyncio.get_event_loop().run_until_complete(start_server) asyncio.get_event_loop().run_forever() if __name__=="__main__": test1 = wsdb() test1.run() the simple orm db_writer is from sqlalchemy model. in makefile12345678910111213141516171819202122CC=g++JSONC_IDIR=/usr/local/includeCFLAGS=-I. -I$(JSONC_IDIR)OPEN_LOOP_DEPS= rtwtypes.h adas_test.h LDIR=-L/usr/local/lib/LIBS=-ljson-cBOOST_LDIR=-L/usr/lib/x86_64-linux-gnuBOOST_LIBS= -pthread -lboost_system -lboost_random -lboost_thread -lboost_chronoJSON_DEPS= wsclientpp.hobj = adas_test.o ert_main.o src=*.c$(obj): $(src) $(DEPS) $(JSON_DEPS) $(CC) -c $(src) $(CFLAGS)mytest: $(obj) $(CC) -o mytest $(obj) $(CFLAGS) $(LDIR) $(LIBS) $(BOOST_LDIR) $(BOOST_LIBS).PHONY: cleanclean: rm -f *.o so now we have encode adas simulink model to C code, and integrate this model C with a websocket client, which can talk to a ws server, which further write to database, which further can be used an data analysis model. we can add front-end web UI and system monitor UI if needed, but so far this adas test pipeline can support a few hundred adas test cores to run concurrently. refer pthread_create with multi args wscpp retry client]]></content>
      <tags>
        <tag>adas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[stanford cs linked list problems]]></title>
    <url>%2F2020%2F03%2F14%2Fstanford-cs-linked-list-problems%2F</url>
    <content type="text"><![CDATA[backgroundstanford cs gives me some confidence to deal with linked list. linked list basicsbasic pointers as well as previous blog: a pointer stores a reference to another variable(pointee). what stored inside pointer is an reference to its pointee’s type. NULL pointer, points to nothing. pointer assignment p=q, makes the two pointers point to the same pointee, namely the two pointers point to the same pointee memory. it’s a good habit to remember to check the empty list case. define the Linked-List structure: 123456struct ListNode &#123; int val ; ListNode *next; ListNode(int x): val(x), next(NULL)&#123;&#125;&#125;;typedef ListNode node_ ; iterate over the list with a local pointer 123456node_ *current = head ;while(current)&#123; current = current-&gt;next;&#125;for(current=head; current!=NULL; current=current-&gt;next)&#123;&#125;; push a new node to the front of the list 12345678910111213void Push(ListNode** headRef, int val)&#123; ListNode newNode = new ListNode(val); newNode.next = *headRef ; *headRef = newNode ; &#125;``` * changes the head pointer by a reference pointer```c++void changetoNull(ListNode** head)&#123; *head = NULL;&#125; build up a list by adding nodes at its head 1234567891011121314151617181920212223242526272829303132333435363738394041ListNode* AddatHead()&#123; ListNode *head = NULL ; for(int i=1; i&lt;5; i++)&#123; Push(&amp;head, i); &#125; return head;&#125;``` which gives a list &#123;4, 3, 2, 1&#125; * build up a list by appending nodes to the tail```c++ListNode* BuildwithSpecialCase()&#123; ListNode* head = NULL ; ListNode* tail ; Push(&amp;head, 1); tail = head ; for(int i=2; i&lt;5; i++)&#123; Push(&amp;(tail-&gt;next), i); tail = tail-&gt;next; &#125; return head; &#125;``` which gives a list &#123;1, 2, 3, 4&#125;* build up a list with dummy node ```c++ListNode* BuildWithDummy()&#123; ListNode dummy = new ListNode(0); ListNode* tail = &amp;dummy ; for(int i=1; i&lt;5; i++)&#123; Push(&amp;(tail-&gt;next), i); tail = tail-&gt;next; &#125; return dummy.next ; &#125; which returns a list {1, 2, 3, 4} appendNode(), add new node at the tail 12345678910111213141516171819202122232425262728293031323334ListNode* appendNode(ListNode** headRef, int val)&#123; ListNode *current = *headRef ; ListNode newNode = new ListNode(num); if(!current)&#123; current = &amp;newNode ; &#125;else&#123; while(current-&gt;next)&#123; current = current-&gt;next ; &#125; current-&gt;next = &amp;newNode ; &#125;&#125;``` * copyList()```c++ListNode* copyList(ListNode* head)&#123; ListNode *current = head ; ListNode *newList = NULL ; ListNode *tail = NULL ; while(current)&#123; if(!newList)&#123; newList = &amp;(new ListNode(current-&gt;val)); tail = newList ; &#125;else&#123; tail-&gt;next = &amp;(new ListNode(current-&gt;val)); tail = tail-&gt;next ; &#125; &#125; return newList ; &#125; copyList() recursive 12345678ListNode* CopyList(ListNode* head)&#123; if(!head) return NULL; else&#123; ListNode *current = head ; ListNode *newList = &amp;(ListNode(current-&gt;val)); newList-&gt;next = CopyList(current-&gt;next); &#125;&#125; linked list problems further InsertNth() 12345678910void insertNth(node_ **head, int index, int data)&#123; if(index==0) Push(head, data); else&#123; node_ * cur = *head ; for(int i=0; i&lt;index-1; i++)&#123; cur = cur-&gt;next ; &#125; Push(&amp;*cur-&gt;next), data); &#125;&#125; sortedInsert() 123456789101112131415161718192021222324252627// be notified, here we using **, to use the pointer of the head, as the head node may be updated void sortedInsert(node_ **head, node* newNode)&#123; if(*head == NULL || head-&gt;val &gt;= newNode-&gt;val)&#123; newNode-&gt;next = *head; *head = newNode; &#125;else&#123; node_ *cur = *head ; while(cur-&gt;next &amp;&amp; cur-&gt;next-&gt;val &lt; newNode-&gt;val)&#123; cur = cur-&gt;next; &#125; newNode-&gt;next = cur-&gt;next ; cur-&gt;next = newNode ; &#125;&#125;//with dummy headvoid sortedInsert(node_ **head, node* newNode)&#123; node_ dummy(0); dummy.next = *head ; node_ *cur = &amp;dummy ; while(cur-&gt;next &amp;&amp; cur-&gt;next-&gt;val &lt; newNode-&gt;val)&#123; cur = cur-&gt;next; &#125; newNode-&gt;next = cur-&gt;next ; cur-&gt;next = newNode ; *head = dummy-&gt;next;&#125; insertSort() given as unsorted list, and output with an sorted list 123456789101112void InsertSort(node_ ** head)&#123; node_ *result = NULL ; node_ *cur = *head ; node_ *next ; while(cur)&#123; next = cur-&gt;next ; // ticky- note the next pointer, before we change it sortedInsert(result, cur); cur = next ; &#125; *head = result ;&#125; append() append list b to the end of list a 123456789101112void append(node_ **a, node_ **b)&#123; node_ * cur ; if( *a == NULL)&#123; *a = *b ;&#125; else&#123; cur = *a ; while(cur-&gt;next)&#123; cur = cur-&gt;next; &#125; cur-&gt;next = *b ; &#125; *b = NULL ; &#125; frontBackSplit() given a list, split it into two sublists: one for the front half, one for the back half 1234567891011121314151617181920212223242526272829303132333435363738void frontBackSplit(node_ **head, node_ **front, node_ **back)&#123; int len = length(head); node_ *cur = *head ; if(len &lt; 2)&#123; *front = *head; *back = NULL ; &#125;else&#123; int half = len/2; for(int i=0; i&lt;half-1; i++)&#123; cur = cur-&gt;next; &#125; *front = *head; *back = cur; cur = NULL ; &#125;&#125;void frontBackSplit2(node_ **head, node_ **front, node_ **back)&#123; node_ *fast, *slow ; if(*head == NULL || (*head)-&gt;next == NULL)&#123; *front = *head ; *back = NULL ; &#125;else&#123; slow = head; fast = head-&gt;next ; while(fast)&#123; fast = fast-&gt;next; if(fast)&#123; fast = fast-&gt;next; slow = slow-&gt;next; &#125; &#125; *front = *head; *back = slow-&gt;next ; slow-&gt;next = NULL ; &#125;&#125; removeDuplicates() remove duplicated node from a sorted list 123456789101112131415161718192021222324252627282930313233void removeDuplicates(node_ ** head)&#123; node_ *slow, *fast ; if(head == NULL || head-&gt;next == NULL)&#123; return ; &#125; slow = *head ; fast = (*head)-&gt;next ; while(fast)&#123; if(slow-&gt;val == fast-&gt;val)&#123; node_ * needfree = fast ; fast = fast-&gt;next ; free(needfree); &#125;else&#123; slow = slow-&gt;next; fast = fast-&gt;next; &#125; &#125;&#125;void removeDuplicate(node_ **head)&#123; node_ *cur = *head; if(cur==NULL) return ; while(cur-&gt;next)&#123; if(cur-&gt;val == cur-&gt;next-&gt;val)&#123; node_ *nextNext = cur-&gt;next-&gt;next ; free(cur-&gt;next); cur-&gt;next = nextNext; &#125;else&#123; cur = cur-&gt;next; &#125; &#125;&#125; moveNode() given two list, remove the front node from the second list, and push it onto the front of the first. // a = {1, 2, 3}; b = {1, 2, 3} =&gt; a={1, 1, 2, 3}, b={2, 3} 123456void moveNode(node_ **dest, node_ **source)&#123; node_ *newNode = *source ; *source = newNode-&gt;next ; newNode-&gt;next = *dest ; *dest = newNode ; &#125; alternatingSplit() given a list, split its nodes into two shorter lists. if we number the elements 0, 1, 2, …, then all the even elements go to the first sublist, and all the odd elements go to tthe second 12345678910111213void alternatingSplit(node_ *source, node_ **ahead, node_ **bhead)&#123; node_ *a = NULL ; node_ *b = NULL ; node_ *cur = *source ; while(cur)&#123; moveNode(&amp;a, &amp;cur); if(cur)&#123; moveNode(&amp;b, &amp;cur); &#125; &#125; *ahead = a ; *bhead = b;&#125; shuffleMerge() given two list, merge their nodes together to make one list, takign nodes alternatively between the two lists 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748node_* shufflMerge(node_ *a, node_ *b)&#123; node_ *res = NULL ; int i=0; while(a || b)&#123; if(i % 2 == 0 &amp;&amp; a)&#123; moveNode(&amp;res, &amp;a); &#125;else if(b)&#123; moveNode(&amp;res, &amp;b); &#125; &#125;&#125; //but this gives the tail to front ordernode_* shufflMerge2(node_ *a, node_ *b)&#123; node_ dummy(0); node *tail = &amp;dummy ; while(1)&#123; if(a==NULL)&#123; tail-&gt;next = b; break ; &#125;else if(b == NULL)&#123; tail-&gt;next = a; break; &#125;else&#123; tail-&gt;next = a ; tail = a ; a = a-&gt;next ; tail-&gt;next = b; tail = b; b = b-&gt;next; &#125; &#125; return dummy.next ;&#125;// recursive ?node_* shufflMerge3(node_ *a, node_ *b)&#123; node_ *res ; node_ *recur ; if(a==NULL) return b ; else if(b=NULL) return a ; else&#123; recur = shuffleMerge3(a-&gt;next, b-&gt;next); res = a ; a-&gt;next = b ; b-&gt;next = recur ; return res; &#125;&#125; sortedMerge() given two sorted in incresing order listes, merge into one in increasing order 1234567891011121314151617181920212223242526272829303132333435363738node_ *sortedMerge(node_ *a, node_ *b)&#123; node_ dummy(0); node_ *tail = &amp;dummy ; dummy.next = NULL ; while(1)&#123; if(a==NULL)&#123; tail-&gt;next = b ; break; &#125;else if(b==NULL)&#123; tail-&gt;next = a; break ; &#125; if(a-&gt;val &lt;= b-&gt;val)&#123; moveNode(&amp;(tail-&gt;next), &amp;a); &#125;else&#123; moveNode(&amp;(tail-&gt;next), &amp;b); &#125; tail = tail-&gt;next; &#125; return dummy.next ;&#125;// how this works?node_ *sortedMerge2(node_ *a, node_ *b)&#123; node_ *result = NULL ; if(a==NULL) return b; if(b==NULL) return a; if(a-&gt;val &lt;= b-&gt;val)&#123; result = a; result-&gt;next = sortedMerge2(a-&gt;next, b); &#125;else&#123; result = b; result-&gt;next = sortedMerge2(a, b-&gt;next); &#125; return result;&#125; mergeSort() 12345678910111213void mergeSor(node_ ** headRef)&#123; node_ *head = *headRef ; node_ *a, *b; if( (head==NULL) || (head-&gt;next == NULL))&#123; return ; &#125; frontBacksplit(head, &amp;a, &amp;b); mergeSort(&amp;a); mergeSort(&amp;b); *headRef = sortedMerge(a,b):&#125; reverse() 123456789101112131415void reverse(node_ **head)&#123; node_ *res = NULL; node_ *cur = *head ; node_ *next ; while(cur)&#123; next = cur-&gt;next; cur-&gt;next = res ; res = cur ; cur = next ; &#125; *head = res;&#125; recursive reverse() // concerned 1234567891011121314void recursiveReverse(node_ **head)&#123; ndoe_ *first, *rest ; if(*head == NULL) return ; first = *head ; rest = first-&gt;next ; if(rest == NULL) return ; recursiveReverse(&amp;rest); first-&gt;next-&gt;next = first ; first-&gt;next = NULL ; *head = rest ;&#125; referencelinked list problems linked list basics from standford cs]]></content>
      <tags>
        <tag>leetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hypervisor and gpu virtualization]]></title>
    <url>%2F2020%2F03%2F14%2Fhypervisor-and-gpu-virtualization%2F</url>
    <content type="text"><![CDATA[backgroundto build a private data center for ads training and SIL. a few concepts there: linux remote desktopLinux GUI is based on x-protocol, which is kind of CPU based, during deploying gpu-based app in docker swarm, I have studied xserver, ssh -X channels give the way for visualization desktop and simple apps(e.g. glxgears) among xserver and xclient. while for really gpu-densive app, e.g. game rendering, machine learning, ssh -X can’t really use gpu resource well, which needs further support from device management mechanism from docker or k8s. virutal machine/ cloud desktopVM is based on hypervisor, known as a virtual machine monitor(VMM), a type of virtualization software that supports the creation and management of VMs, hypervisor translate requests between the physical and virtual resources, making virtualization possible. A hypervisor allows one host computer to support multiple guest VMs by virtually sharing its resources, like memory and processing. Generally, there are two types of hypervisors. Type 1 hypervisors, called “bare metal,” run directly on the host’s hardware. Type 2 hypervisors, called “hosted,” run as a software layer on an operating system, like other computer programs, the most common e.g. vmware, citrix. When a hypervisor is installed directly on the hardware of a physical machine, between the hardware and the operating system (OS), it is called a bare metal hypervisor, which separates the OS from the underlying hardware, the software no longer relies on or is limited to specific hardware devices or drivers. VDIa type of vm application is virtual desktop inftrastructure(vdi), VDI hosts desktop environments on a centralized server and deploys them to end-users on request. this process also known as server virtualization VDI is not necessarily requires a physical gpu, without a gpu, we can still run vmware, but the performance is poor. but for many other VM usage, beyond VDI, we still need ways to access GPU devices in VM. GPU techs in VMvwmare has an introduction about gpu tech in vm: software 3D. using software to mimic GPU calculating. all 3d calculating is done by CPU vsga(vitual shared graphics acceleration), each virtual desktop has a vsga driver, which has a ESXi module, inside which has a GPU driver, which can call the physical gpu, but this shared mode is through Hypervisor vdga(virtual dedicated graphics accleration), or pass through mode, the physical GPU is assigned to a special virtual desktop for one user only. vgpu, split physical GPU to a few virtual-GPU, each virtual desk can have one vGPU. VM vs dockerdocker is a light-weight virtualization way, which don’t need hypervisor, and the app in docker is access the host physical devices directly, making docker looks like a process, rather than a virtual OS; and scientfic computing apps run in VM is actually using the virtual CPU from hypervisor, which bypass the cpu optimzation used for math calcualting. so usually a physical machine can start hundres or thousands docker, but can only run a few VM. referlinux remote desktop understand vdi gpu tech nvidia vgpu tech in vSphere GPU hypervisors on OpenStack containers vs hypervisors]]></content>
  </entry>
  <entry>
    <title><![CDATA[pure c/c++ pointer]]></title>
    <url>%2F2020%2F03%2F13%2Fpure-c-c-pointer%2F</url>
    <content type="text"><![CDATA[pointer assignment12345678910111213141516171819202122232425262728293031323334int main()&#123; int a = 10 ; int *pa = &amp;a ; cout &lt;&lt; "pa add " &lt;&lt; &amp;pa &lt;&lt; "pa val " &lt;&lt; *pa &lt;&lt; endl ; int b = 22; int *pb = &amp;b ; cout &lt;&lt; "pb add " &lt;&lt; &amp;pb &lt;&lt; "pb val " &lt;&lt; *pb &lt;&lt; endl ; int d = 100 ; int *pd = &amp;d ; cout &lt;&lt; "pd add " &lt;&lt; &amp;pd &lt;&lt; "pd val " &lt;&lt; *pd &lt;&lt; endl ; int *pc = pb ; //pointer assignment cout &lt;&lt; "pc add " &lt;&lt; &amp;pc &lt;&lt; "pc val " &lt;&lt; *pc &lt;&lt; endl ; pb = pd ; cout &lt;&lt; "pb add " &lt;&lt; &amp;pb &lt;&lt; "pb val " &lt;&lt; *pb &lt;&lt; endl ; cout &lt;&lt; "pd add " &lt;&lt; &amp;pd &lt;&lt; "pd val " &lt;&lt; *pd &lt;&lt; endl ; pd = pa ; cout &lt;&lt; "pd add " &lt;&lt; &amp;pd &lt;&lt; "pd val " &lt;&lt; *pd &lt;&lt; endl ; pa = pc ; cout &lt;&lt; "pa add " &lt;&lt; &amp;pa &lt;&lt; "pa val " &lt;&lt; *pa &lt;&lt; endl ; cout &lt;&lt; "pc add " &lt;&lt; &amp;pc &lt;&lt; "pc val " &lt;&lt; *pc &lt;&lt; endl ; return 0;&#125; /* pb add 0x7fff5e744b30pb val 22pd add 0x7fff5e744b28pd val 100pc add 0x7fff5e744b20pc val 22pb add 0x7fff5e744b30pb val 100pd add 0x7fff5e744b28pd val 100pd add 0x7fff5e744b28pd val 10pa add 0x7fff5e744b40pa val 22pc add 0x7fff5e744b20pc val 22*/ pointer assigment used a lot in linked list problems, the sample above is a pointer solution for linked list reverse. consider pointer as a container with an address to another object. pointer assignment, e.g. pointerA = pointerB only changes the content in the container. but the address of the container itself doesn’t change. and with *(dereference) the pointer, we can see the content is changed. further, taken pc, pb, pd as another example.12pc = pb ;pb = pd; the first line will make container pc to store what is stored in container pb, in another word, the first line will make pc point to the address, which is stored in pb. and the second line will then put what’s stored in container pd to container pb. after this two updates, pc points to the original content in pb; pb and pd points to the same content. obviously, what’s inside pc now, has nothing to do with pointer pb. pointer++ forwardthe basic scenario is as following, will p2 move forward as well ? 123int *p1 = &amp;int_var ;int *p2 = p1; p1++ ; we can see from the following test.c, p2 won’t move forward as p1++. 1234567891011121314151617181920212223242526int main()&#123; int a[4] = &#123;1, 2, 3, 4 &#125; ; cout &lt;&lt; " a addr " &lt;&lt; &amp;a &lt;&lt; " a val " &lt;&lt; *a &lt;&lt; endl ; int *p = a ; int *q = p ; cout &lt;&lt; " p addr " &lt;&lt; &amp;p &lt;&lt; " p val " &lt;&lt; *p &lt;&lt; endl ; cout &lt;&lt; " q addr " &lt;&lt; &amp;q &lt;&lt; " q val " &lt;&lt; *q &lt;&lt; endl ; for(int i=0; i&lt;3; i++)&#123; q++; &#125; cout &lt;&lt; " a addr " &lt;&lt; &amp;a &lt;&lt; " a val " &lt;&lt; *a &lt;&lt; endl ; cout &lt;&lt; " p addr " &lt;&lt; &amp;p &lt;&lt; " p val " &lt;&lt; *p &lt;&lt; endl ; cout &lt;&lt; " q addr " &lt;&lt; &amp;q &lt;&lt; " q val " &lt;&lt; *q &lt;&lt; endl ; return 0;&#125; /* a addr 0x7fff5e968bb0 a val 1 p addr 0x7fff5e968b40 p val 1 q addr 0x7fff5e968b38 q val 1 a addr 0x7fff5e968bb0 a val 1 p addr 0x7fff5e968b40 p val 1 q addr 0x7fff5e968b38 q val 4*/ pointer++ can be reviewed as a same type pointer in current pointer’s neighbor, then do a pointer assigment: 12345678910111213141516171819int* tmp_p = 0x7fff5e744b30 ; int* p1 = 0x7fff5e744b28 ; p1 = tmp_p ;``` ### int++```c++int main()&#123; vector&lt;int&gt; nums ; for(int i=0; i&lt;5; i++)&#123; nums.push_back(i); &#125; int p=0; std::cout &lt;&lt; "nums[p++] " &lt;&lt; nums[p++] &lt;&lt; " p val " &lt;&lt; p &lt;&lt; endl ; return 0;&#125; /* nums[p++]: 0 p val: 1 */]]></content>
      <tags>
        <tag>pure C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nodejs introduction]]></title>
    <url>%2F2020%2F03%2F12%2Fnodejs-introduction%2F</url>
    <content type="text"><![CDATA[callback / asnyc123456var fs = require("fs") ;fs.readFile("input.txt", function(err, data)&#123; if(err) return console.error(err); console.log(data.toString());&#125;) the async fun take the callback fun as its last parameter. event loop1234567891011var events = require('events')var emitter = new events.EventEmitter();var connectH = function connected()&#123; console.log("connected") emitter.emit('data_received'); // trig 'data_received'&#125;emitter.on('connection', connectH);emitter.on('data_received', function()&#123; console.log('data received');&#125;)emitter.emit('connection'); // trig 'connection' event emitterwhen async IO is done, will send an event to the Event queue. e.g. when fs.readStream() open a file will trig an event. e.t.c 123addListener(event, listener)on(event, listener) #listeningemit(evnet, [arg1], ...) #trig file system1234567891011121314var fs = require("fs")fs.open("input.log", "r+", function(err, fd)&#123;&#125;);fs.state("input.log", function(err, stats_info)&#123;&#125;);fs.readFile("input.log", function(err, data)&#123; if(err)&#123; return console.error(err); &#125; console.log(data.toString());&#125;);fs.writeFile("output.log", function(err)&#123; if(err)&#123; return console.error(err);&#125; console.log("write successfully")&#125;);fs.read(fd,buffer, [args..]) #read binary stream bufferas js language has only txt bytes data, to deal with binary data, introduce Buffer 12345Buffer.alloc(size)Buffer.from(buffer||array||string)Buffer.write() #write to bufferbuffer.toString() #read from bufferbuffer.toJSON() stream123456var fs=require("fs");var readerStream = fs.createReadstream("input.file")readerStream.on('data', function(chunk, err)&#123;&#125;)var writeStream = fs.createWriteStream("output.file")writeStream.on('finished', function()&#123;&#125;)readerStream.pipe(writeStream); #pipe from a reader stream to a writer stream module systemto enable different nodejs files can use each other, there is a module system, the module can be a nodejs file, or JSON, or compiled C/C++ code. nodejs has exports and require used to export modules’ APIs to external usage, or access external APIs. 12module.exports = function()&#123;&#125;exports.method = function()&#123;&#125; the first way export the object itself, the second way only export the certain method. Global Object12console.log()console.error() common modules path 123var path = require("path");path.join("/user/", "test1"); path.dirname(p_); http server 1234567891011121314var http = require("http");http.createServer(function(request, response)&#123; var url_path = request.url ; server(url_path, function(err, data)&#123; if(err)&#123; console.log(err); response.writeHead(404, "xx"); &#125;else&#123; response.writeHeead(200, "yy"); response.write(data.toString()); &#125; response.end(); &#125;);&#125;).listen(8080); http client 123456789101112var http = require('http')url = "http://localhost:8080/index.html"var req = http.request(url, callback);var callback = function(response)&#123; var body = '' ; response.on('data', function(data)&#123; body += data; &#125;); response.on('end', function()&#123; console.log(body); &#125;);&#125;; ExpressExpress has requst and response object to handle request and reponse. express.static can handle static resources, e.g. image, css e.t.c 12345678910111213141516171819202122232425262728293031323334var express = require("repress");var app = express();app.use('/public', express.static('public'));app.get('/index', function(req, res)&#123;&#125;)app.get('/user', function(req, res)&#123; var response = &#123; "name": req.query.name, "id": req.query.id &#125;; res.send(JSON.stringify(response));&#125;);app.post('/user', function(req, res)&#123; var response = &#123; "name" : req.body.name ; "id" : req.body.id &#125;; res.send(JSON.stringify(response));&#125;); app.post('file_upload', function(req, res)&#123; var des_file = __dirname + "/" + req.files[0].originalname ; fs.readFile(req.files[0].path, function(err, data)&#123; fs.writeFile(des_file, data, function(err)&#123; if(err)&#123;console.error(err);&#125; else&#123; var response = &#123; message: "file uploaded successfully" , filename: req.files[0].originalname &#125;; &#125; res.send(JSON.stringify(response)); &#125;); &#125;);&#125;)var server = app.listen(8080, function()&#123;&#125;) res is what send from server to client, for both /get, /post methods. req object represents the HTTP request and has properties for the request query string, parameters, body, HTTP headers e.t.c req.body contains key-value pairs of data submitted in the request body. by default, it’s undefined, and is populated when using body-parsing middleware. e.g. body-parser req.cookies when using cookie-parser middleware, this property is an object that contains cookies send by the request req.path contains the path part of the request url req.query an object containing a property for each query string parameter in the route req.route the current mathced route, a string data access object(DAO)Dao pattern is used to separate low level data accessing API or operations form high level business services. usually there are three parts: DAO interface, which defines the standard operations to be performed on a model object DAO class, the class that implement DAO interfaces, this class is responsible to get data from database, or other storage mechanism model object, a simple POJO containing get/set methods to store data retrieved using DAO class o/r mapping (orm) is used a lot to map database itme to a special class, and it’s easy to use, but a little drawback of orm is it assume the database is normalized well. DAO is a middleware to do directly SQL mapping, who mapes SQL query language to the output class. separting models, logic and daos routes.js, where to put routes, usually referenced as controllers models.js, where to put functions talk to database, usually referenced as dao layer views.js these three components can put under app; all static data usually put under public folder; the Express package.json and index.js are at the same level as app. refernodejs at runoob.com nodejs &amp; mysql bearcat-dao introduction koa chokidar]]></content>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[high concurrent ws log server]]></title>
    <url>%2F2020%2F03%2F11%2Fhigh-concurrent-ws-log-server%2F</url>
    <content type="text"><![CDATA[backgroudpreviously, we designed a logger module from ws to db/, this blog is one real implementation of the python solution. high concurrent ws server onlinesingle server with multi clients: a simple C++ the server is started first, and wait for the incoming client calls, periodically report its status: how many clients keep connected. meanwhile, once an incoming call is detected and accepted, the server will create a separate thread to handle this client. therefore, the server creates as many separate sessions as there are incoming clients. how handle multiple clients? once an incoming client call is received and accepted, the main server thread will create a new thread, and pass the client connection to this thread what if client threads need access some global variables? a semaphore instance ish helpful. how ws server handles multiple incomming connections socket object is often thought to represent a connection, but not entirely true, since they can be active or passive. a socket object in passive/listen mode is created by listen() for incoming connection requets. by definition, such a socket is not a connection, it just listen for conenction requets. accept() doesn’t change the state of the passive socket created by listen() previously, but it returns an active/connected socket, which represents a real conenction. after accept() has returned the connected socket object, it can be called again on the passive socket, and again and again. or known as accept loop But call accept() takes time, can’t it miss incoming conenction requests? it won’t, there is a queue of pending connection requests, it is handled automatically by TCP/IP stack of the OS. meaning, while accept() can only deal with incoming connection request one-by-one, no incoming request will be missed even when they are incoming at a high rate. python env setupwebsockets module requires python3.6, the python version in bare OS is 3.5, which gives: 12345678910 File "/usr/lib/python3/dist-packages/websockets/compatibility.py", line 8 asyncio_ensure_future = asyncio.async # Python &lt; 3.5 ^SyntaxError: invalid syntaxß``` basically, [asyncio.async](https://stackoverflow.com/questions/51292523/why-does-asyncio-ensure-future-asyncio-async-raise-a-syntaxerror-invalid-synt) and another depend module [discord.py](https://github.com/Rapptz/discord.py/issues/1396), require python &lt; 3.5v, then gives another error:```pythonTypeError: unsupported operand type(s) for -=: 'Retry' and 'int' the Retry error fixed by adding the following lines to ~/.pip/pip.conf 12345678910111213141516[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple``` in a bare system with pre-installed python3.5, I did the following steps: ```pythonsudo apt install software-properties-commonsudo add-apt-repository ppa:deadsnakes/ppasudo apt-get install python3.7 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 2sudo apt-get install python3-websocketssudo apt-get install python3-websocketsudo apt-get install python3-sqlalchemysudo pip3 install threadpoolsudo apt-get remove --purge python3-websocket #based on python3.5sudo apt-get install python3-websocket #based on python3.7 which gives another error: 123456/var/lib/dpkg/info/python3-websocket.postinst: 6: /var/lib/dpkg/info/python3-websocket.postinst: py3compile: not founddpkg: error processing package python3-websocket (--configure): subprocess installed post-installation script returned error exit status 127Errors were encountered while processing: python3-websocketE: Sub-process /usr/bin/dpkg returned an error code (1) which then be fixed by go to /var/lib/dpkg/info/ and delete all python3-websocket.* files: 123sudo rm /var/lib/dpkg/info/[package_name].*sudo dpkg --configure -asudo apt-get update everything looks good, but still report: 1ModuleNotFoundError: No module named &apos;websockets&apos; Gave up setting up with the bare python, then create a new conda env, and ran the following settings inside, clean and simple: 12345pip install websocketspip install websocket-client #rather websocketpip install threadpoolpip install sqlalchemypip install psycopg2 during remote test, if ws server down unexpected, need kill the ws pid: 12345678910111213141516171819202122232425262728293031323334353637383940sudo netstat -tlnp #find the special running port and its pid kill $pid``` or `kill $(lsof -t -i:$port)`## websockets &amp; websocket-clientthe long-lived connection sample from [ws-client](https://github.com/websocket-client/websocket-client) is good base, as here need to test a high concurrent clients, we add `threadpool`:```python def on_open(ws, num): def run(*args): ##args for i in range(3): time.sleep(1) message = "your_message" ws.send(json.dumps(message)) time.sleep(1) ws.close() print("thread terminating...") thread.start_new_thread(run, ()) def on_start(num): websocket.enableTrace(True) ws = websocket.WebSocketApp("ws://localhost:8888/", on_message = on_message, on_error = on_error, on_close = on_close) ws.on_open = on_open(ws, num) ws.run_forever() def threadpool_test(): start_time = time.time() pool = ThreadPool(100) test = list() for itr in range(100): test.append(itr) requests = makeRequests(on_start, test) [pool.putRequest(req) for req in requests] pool.wait() print('%d second'% (time.time() - start_time)) in ws-client src, we see: on_open: callable object which is called at opening websocket. this function has one argument. The argument is this class object. but all customized callback func can add more arguments, which is helpful. on_message: callable object which is called when received data. on_message has 2 arguments. The 1st argument is this class object. the 2nd argument is utf-8 string which we get from the server. we can implement a simple sqlalchemy orm db-writer, and add to the ws-server: async def process(self, websocket, path): raw_ = await websocket.recv() jdata = json.loads(raw_) orm_obj = orm_(jdata) try: self.dbwriter_.write(orm_obj) print(jdata, "write to db successfully") except Exception as e: dbwriter_.rollback() print(e) greeting = "hello from server" await websocket.send(greeting) print(f"&gt; {greeting}") def run(self): if self.host and self.port : start_server = websockets.serve(self.process, self.host, self.port) else: start_server = websockets.serve(self.process, "localhost", 8867) asyncio.get_event_loop().run_until_complete(start_server) asyncio.get_event_loop().run_forever() in summaryin reality, each ws-client is integrated to one upper application, which generate messages/log, and send to ws-server, inside which write to db, due to asyncio, the performance is good so far. in future, we maybe need some buffer at ws-server. refera simple multi-client ws server create a simple python sw server using Tornado python: websockets python: json python: threadpool]]></content>
      <tags>
        <tag>websocket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[design a logger from ws to db]]></title>
    <url>%2F2020%2F03%2F09%2Fdesign-a-logger-from-ws-to-db%2F</url>
    <content type="text"><![CDATA[backgroundfor now, we had design a file server from db to s3 and mf reader to feed data to the ads test module, which run in SIMD mode, e.g. k8s. for a closed-loop SIL, another component is how to record test message/outputs to database. I am thinking firstly event driven webframework python, or plusar, which is a popular python event-driven project. as lgsvl pythonAPI is really a good choice to transfer message among server and clients. so the basic idea is to integrate websocket/tcp channel inside aeb model websocket vs tcpmost ADAS models developed in OEM are matlab based, and the most common communication protocol in Matlab is UDP, a little review about tcp vs udp, I personally don’t like udp, no reason. so narrow to TCP, the further question is: should it be a raw TCP or Websocket), websocket enables a stream of messages instead of a stream of bytes. WebSockets is built on normal TCP sockets and uses frame headers that contains the size of each frame and indicate which frames are part of a message. The WebSocket API re-assembles the TCP chunks of data into frames which are assembled into messages before invoking the message event handler once per message. WebSocket is basically an application protocol (with reference to the ISO/OSI network stack), message-oriented, which makes use of TCP as transport layer. btw, TCP is bidirectional because it can send data in both directions, and it is full-duplex because it can do that simultaneously, without requiring line turnarounds, at the API level for ADAS test outputs, it’s plain txt, I’d prefer websocket. so make the decision. matlab websocket clientthere is an github project: matlab websocket, which is enough to implement a simple websocket in Matlab. discussion at stackoverflow basically, here we bind a weboscket client to the adas model, in matlab. which is a good choice, so when runnign adas model in massively, the whole system looks like multi ws clients concurrently communicate with one ws server, which is well separated from adas model, and can freely implmented in either nodejs, python or c# e.t.c., totally friendly to other third-party data analysis tools in down-stream. multi-write db concurrentlyonce we get the message from ws clients, we need transfer all of them to a database, e.g. pySQL. but the first thing need to understand is whether SQL can concurrent inserts. if there are multiple INSERT statements, they are queued and performed in sequence, concurrently with the SELECT statements. SQL doesn’t support parallel data inserts into the same table, with InnoDB or MyISAM storage engine. this also answered another question, why not directly adas model write to db, since the parallel runinng cases increase, writing DB will be the bottleneck. so has the websocket middle level, is a good choice. nodejs solutionnodejs is really powerful to implement the solution above. basically, a ws server and sql writer: mysqljs python solutionsimilar as nodejs, python has matured websocket module and ORM moduel to write db. referliaoxuefeng tcp zhihu mysql lock nodejs connect mysql]]></content>
  </entry>
  <entry>
    <title><![CDATA[asam mdf reader]]></title>
    <url>%2F2020%2F03%2F07%2Fasam-mdf-reader%2F</url>
    <content type="text"><![CDATA[previously reviewed a few open source mdf readers as well as get basic concept about mdf. here is a working process with asammdf, which is step by step to understand this lib. the basic problem here is to read a special signal data, not know its channel and group id. first try1234567891011121314151617reader = MDF4(file_name)data_dict = reader.info()print("the total len of dict: ", len(data_dict))print(data_dict.keys())block_ = data_dict['group 117']channel_metadata = reader.get_channel_metadata('t', 117)channel117_comment = reader.get_channel_comment('t', 117)channel117_name = reader.get_channel_name(117, 0)pattern="channel 123"for key in block_ : r1 = re.search(pattern, str(block_[key]), re.M|re.I) if r1: print(key)data_ = reader.get("channel 123") #doesn't work, as it reports can't find Channeldata_ = reader.get("fus_lidar_camera", "group 123") #doesn't work, as it reports can't find the Groupdata_ = reader.get("fus_lidar_camera") #works so first, “channel 123” is actually not the name. and with the right name can reading data, but no guarantee which group this channel is from, and further can’t know is there additional groups has record this type of data. second tryas mentioned mdf4 groups have attributes, how about to access these attributes. so far, especially helpful attributes are: groups:list, list of data group dicts channels_db:dict, used for fast channel access by name. for each name key the value is a list of (group index, channel index) tuples 123456789channels = reader.groups[2] #with the same order as found in mdf, even with index 0, 1, ...print("type of channels: ", type(channels)) #dictif "fus.ObjData_Qm_TC.FusObj.i17.alpPiYawAngle" in channels: print(channels["fus.ObjData_Qm_TC.FusObj.i17.alpPiYawAngle"].id) #noneprint(" chanels type: " , type(channels["channels"])) #list print(" chanels len: " , len(channels["channels"])) #577print("chanel_group type: " , type(channels["channel_group"])) # ChannelGroup objectprint("chanel_group len: " , len(channels["channel_group"])) #17print(" data_group type: " , type(channels["data_group"])) #DataGroup objectprint(" data_group len: " , len(channels["data_group"])) #10 which is far more less as expected, but give an good exploring about the reader’s attributes. third try12345678910111213141516171819reader = MDF4(file_name)cdb = reader.channels_db#print("size of c_db ", len(cdb))#print("type of cdb ", type(cdb))#for more sigles interested, please append to this listsig_list = ['sensor_1', 'sensor_2', 'sensor_3']for sig in sig_list: if sig in cdb: print(cdb[sig]) for item in cdb[sig]: (gid, cid) = item alp = reader.get(sig, gid, cid) print("type of current sig ", type(alp)) print("size of current sig ", len(alp)) #alp is the raw sig data, here is only test print else: print(sig, " is not in cdb" which gives me the right way to access the interested signals’ raw data. combing with s3 streaming body read, as mentioned in previous mdf-reader, which finally gives the powerful pipeline from reading mdf4 files from s3 storage to downside appilication referpyton reg python substring]]></content>
  </entry>
  <entry>
    <title><![CDATA[design a nodejs web server from db to s3]]></title>
    <url>%2F2020%2F03%2F05%2Fdesign-a-nodejs-web-server-from-db-to-s3%2F</url>
    <content type="text"><![CDATA[backgroundduring ADS road test, there are Tb~Pb mount of data generated, e.g. sensor raw data(rosbag, mf4), images, point cloud e.t.c. previous few blogs are focusing on data storage. for a more robost and user-friendly data/file server, also need consider database and UI. from the end-user/engineers viewpoint, a few basic functions are required: query certain features and view the data/files filtered download a single/a batch of interested files (for further usage or analysis) upload large mount of files quickly to storage (mostly by admin users) a FTP server to support s3traditionally, for many large size files to download, FTP is common used. the prons and corns comparing fttp and ftp for transfering files: HTTP is more responsive for request-response of small files, but FTP may be better for large files if tuned properly. but nowadays most prefer HTTP. doing search a little more, there are a lot discussions to connect amazon s3 to ftp server: transfer files from s3 storage to ftp server FTP server using s3 as storage using S3 as storage for attachments in a web-mail system FTP/SFTP access to amazon s3 bucket and there are popular ftp clients which support s3, e.g. winSCP, cyberduck, of course, aws has it own sftp client, as well as aws s3 browser windows client), more client tools check here however, ftp can’t do metadata query. for some cases, e.g. resimulation of all stored scenarios, which makes no difference for each scenario, we can grab one by one and send it to resmiluator; but for many other cases, we need a certain pattern of data, rather than reading the whole storage, then a sql filter is much efficient and helpful. so a simple FTP is not enough in these cases. s3 objects/files to dbstarting from a common bs framework, e.g. react-nodejs, and nodejs can talk to db as well. ** nodejs query buckets/object header info from s3 server, and update these metadata into db. there is a great disscussion about storing images in db - yea or nay: when manage many TB of images/mdf files, storing file paths in db is the best solution: db storage is more expensive than file system storge you can super-acc file system access: e.g. os sendfile() system call to asynchronously send a file directly from fs to network interface, sql can’t web server need no special coding to access images in fs db win out where transactional integrity between image/file and its metadata are important, since it’s more complex to manage integrity between db metdata to fs data; and it’s difficult to guarantee data has been flushed to disk in the fs so for this file server, the metadata include file-path-in-s3, and other user interested items. 12345file_id feature file_path 1 1 http://s3.aws.amazon.com/my-bucket/item1/img1a.jpg 2 2 http://s3.aws.amazon.com/my-bucket/item1/img1b.jpg 3 3 http://s3.aws.amazon.com/my-bucket/item2/img2a.jpg 4 4 http://s3.aws.amazon.com/my-bucket/item2/img2b.jpg ** during browser user query/list request, nodejs talk to db, which is a normal bs case. ** when the browser user want to download a certain file, then nodejs parse the file metadata and talk to s3 nodejs to s3nodejs fs.readFile()taking an example from official nodejs fs doc: 123456789101112const fs = require('fs')fs.readFileSync(pathname, function(err, data)&#123; if(err)&#123; res.statusCode = 500 ; res.end(`Err getting file: $[err&#125;`) &#125;else&#123; res.end(data); &#125;&#125;); const fileUrl = new URL('file://tmp/mdf')fs.readFileSync(fileUrl); if not file directly, maybe fs.Readstream class is another good choice to read s3 streaming object. fs.readFile() and fs.readFileSync() both read full content of the file in memory before returning the data. which means, the big files are going to have a major impact on your memory consumption adn speed of execution of the program. another choice is fs-readfile-promise. express res.downloadres object represent the HTTP response that an Express app sends when it gets an HTTP request. expressjs res.download 12345678res.downlaod('/ads/basic.mf4', 'as_basic.mf4', function(err)&#123; if(err)&#123; log(`download file error: $&#123;err&#125;`) &#125;else&#123; log(`download file successfully`) &#125;&#125;) aws sdk for nodejstaking an example from aws sdk for js 12345678910var aws = require('aws-sdk')var s3 = new aws.S3()s3.createBucket(&#123;Bucket: your_bucket_name, function()&#123; var params=&#123;Bucket: your_bucket_name, Key: your_key, Body: mf4.streaming&#125;; s3.putObject(params, function(err, data)&#123; if(err) console.log(err) else console.log("upload data to s3 succesffully") &#125;);&#125;); check aws sdk for nodejs api for more details. in summaryeither a FTP server or a nodejs server, it depends on the upper usage cases. a single large-size(&gt;100mb) file(e.g. mf4, rosbag) download, nodejs with db is ok, as db helps to filter out the file first, and a few miniutes download is needed many of little-size(~1mb) files(e.g. image, json) downlaod, nodejs is strong without doubt. many of large-size files download/upload, a friendly UI is not necessary, comparing to the performance, then FTP may be the solution.]]></content>
      <tags>
        <tag>aws</tag>
        <tag>web server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[driving scenarios on-going]]></title>
    <url>%2F2020%2F03%2F04%2Fdriving-scenarios-on-going%2F</url>
    <content type="text"><![CDATA[Euro NCAP driving scenariosAdaptive Cruise Control(ACC)ACC is tested in an extended version of AEB. these systems are designed to automatically adapt the speed when approaching a slower moving vehicle or noe that is braking. notification, not all systems perform well with stationary obstacles. a slow moving vehicle test a stationary obstacle(e.g. vehicle) test autonomous emergency braking(AEB)AEB pedestrain an adult pedestrain running from the driver’s side of the vehicle an adult walk from the passenger’s side a child runs from between parked cars on the passenger’s side of the car pedestrain walking in the same direction as the vehicle algnedwith the centre of vehicle pedestrain walking in the same direction as the vehicle offset to one side AEB cyclist cyclist is crossing the vehicle’s path cyclist is travelling in the same direction Euro NCAP gives greatest reward if a collision is completely avoided; in some case, if AEB is unable to stop the vehicle completely, still give some points. lane keeping assist(LKA) lane centering, a good ADS should continue to support the driver during the manoeuvre and will not resist the driver or deactivate. cut-in test, a npc from adjacent lane merges into the current lane cut-off test, a npc leaves the current lane abruptly to avoid a stopped vehicle ahead swerve around a small obstacle test speed assistance test, to use map data and/or data from sensors to identify the local speed limit NHTSA driving scenariosa previous blog design scenarios in ADS simulation is based on NHTSA. forward collision prevention Forward collision warning AEB pedestrain AEB Adaptive lighting backing up &amp; parking rear automatic braking backup camera rear cross traffic alert lane &amp; side assist lane departure warning lane keeping assist blind spot detection lane centering assist maintaing safe distance traffic jam assist highway pilot ACC Matlab Driving Scenario samplesAEB (for vulnerable road users) AEB_Bicyclist_Longitudinal_25width, at collision time, the bicycle is 25% of the way across the width of the ego vehicle. AEB_CCRb_2_initialGap_12m, a car-to-car rear braking(CCRb) scenario where the ego rear hit a braking agent. the deceleration is 2 m/s^2 with initial gap 12m AEB_CCRm_50overlap, a car-to-car rear moving(CCRm) scenario, where the ego rear hits a moving vehicle. at collisio ntime, the ego overlaps with 50% of the width of the moving vehicle AEB_CCRs_-75overlap, a car-to-car stationary(CCRs) sceanrio, where the ego rear hits a stationary vehicle. at collision time, the ego overlaps with 75% of the width of the stationary vehicle. when the ego is to the left of the other vehicle, the percent overlap is negative AEB_Pedestrain_Farside_50width, the ego collides with a pedestrain who is traveling from the left side(far side, assuming vehicle drive on right side of the road). at collision time, the pedestrain is 50% of the way across the width of the ego AEB_PedestrainChild_Nearside_50width, the ego collides with a pedestrain who is traveling from the right side(near side), at collision time, the pedestrain is 50% of the way across the width of the ego Emergency Lane Keeping(ELK) ELK_FasterOvertakingVeh_Intent_Vlat_0.5, ego intentionally changes lanes but collides with a faster overtaking vehicle, the ego travels at a lateral velocity of 0.5m/s ELK_OncomingVeh_Vlat_0.3, ego unintentionally changes lanes and collides with an oncoming vehicle, with ego’s lateral velocity of 0.3m/s ELK_OvertakingVeh_Unintent_Vlat_0.3, ego unintentionally changes lanes, overtake a vehicle in the other lane and collides. the ego travles at a lateral velocity at 0.3m/s ELK_RoadEdge_NoBndry_Vlat_0.2, ego unintentionally changes lanes and ends up to hte road edge, with lateral velocity at 0.2m/s LKA LKA_DashedLine_Solid_Left_Vlat_0.5 LKA_DashedLine_Unmarked_Right_Vlat_0.5 LKA_RoadEdge_NoBndry_Vlat_0.5 LKA_RoadEdge_NoMarkings_Vlat_0.5 LKA_SolidLine_Dashed_Left_Vlat_0.5 LKA_SolidLine_Unmarked_Right_Vlat_0.5 open source libs Microsoft: autonomous driving cookbook TrafficNet: an open scenario lib Toyota: vehicle simulation unity toolkit mathworks: github sensetime: driving stero dataset google: open simulator interface cmu: ml4ad scenarios in L2 vs L3+scenarios in l2 is more like point in the whole driving manuevor, at a certain scenario/point, how the ADAS system response. while L3+ is a continously scenario, along each point during driving is part of the scenario, and inside which all kinds of l2 case scenario can happen. it does make sense to integrate l2 scenario planning response to l3+, which makes l3+ system more strict. refer2018 Automated driving test mathworks: Euro NCAP driving scenarios in driving scenario designer Euro NCAP roadmap 2025 nhtsa vehicle safety legacy doc NHTSA driver assitance tech Study and adaptation of the autonomous driving simulator CARLA for ATLASCAR2 implementing RSS model on NHTSA pre-crash scenarios]]></content>
  </entry>
  <entry>
    <title><![CDATA[boto3 streamingBody to BytesIO]]></title>
    <url>%2F2020%2F03%2F03%2Fboto3-streamingBody-to-BytesIO%2F</url>
    <content type="text"><![CDATA[boto3 streamingBody to BytesIOboto3 class into A Session is about a particular configuration. a custom session: 123session = boto3.session.Session()ads = session.client('ads')ads_s3 = session.resource('s3') Resources is an object-oriented interface to AWS. Every resource instance has a number of attributes and methods. These can conceptually be split up into identifiers, attributes, actions, references, sub-resources, and collections. 1obj = ads_s3.Object(bucket_name="boto3", key="test.mdf") Client includes common APIs: 123456789101112131415161718copy_object()delete_object()create_bucket()delete_bucket()delete_objects()download_file()download_fileobj()get_bucket_location()get_bucket_policy()get_object()head_bucket()head_object()list_buckets()list_objects()put_bucket_policy()put_object()upload_file()upload_fileobj() Service Resource have bucket and object subresources, as well as related actions. Bucket, is an abstract resource representing a S3 bucket. 1b = ads_s3.Bucket('name') Object, is an abstract resource representing a S3 object. 1obj = ads_s3.Object('bucket_name', 'key') read s3 object &amp; pipeline to mdfreaderthere are a few try-and-outs. first is to streaming s3 object as BufferedReader, which give a file-like object, and can read(), but BufferedReader looks more like a IO streaming than a file, which can’t seek. botocore.response.StreamingBody as BufferedReaderthe following discussion is really really helpful:boto3 issue #426: how to use botocore.response.StreamingBody as stdin PIPE at the code of the StreamingBody and it seems to me that is is really a wrapper of a class inheriting from io.IOBase) but only the read method from the raw stream is exposed, so not really a file-like object. it would make a lot of sense to expose the io.IOBase interface in the StreamingBody as we could wrapped S3 objects into a io.BufferedReader or a io.TextIOWrapper.read() get a binary string . the actual file-like object is found in the ._raw_stream attribute of the StreamingBody class 1234import iobuff_reader = io.BufferedReader(body._raw_stream)print(buff_reader)# &lt;_io.BufferedReader&gt; wheras this buff_reader is not seekable, which makes mdfreader failure, due to its file operate needs seek() method. steam a non-seekable file-like object stdio stream to seekable file-like objectso I am thinking to transfer the BufferedReader to a seekable file-like object. first, need to understand why it is not seekable. BufferedRandom is seekable, whereas BufferedReader and BufferedWriter are not. Buffered streams design: BufferedRandom is only suitable when the file is open for reading and writing. The ‘rb’ and ‘wb’ modes should return BufferedReader and BufferedWriter, respectively. is it possbile to first read() the content of BufferedReader to some memory, than transfer it to BufferedRandom? which gives me the try to BufferedReader.read(), which basicaly read all the binaries and store it in-memoryA, then good news: in-memory binary streams are also aviable as Bytesio objects: f = io.BytesIO(b"some in-memory binary data") what if assign BytesIO to this in-memoryA. which really gives me a seekable object: fid_ = io.BufferedReader(mf4_['Body']._raw_stream) ; read_in_memory = fid_.read() bio_ = io.BytesIO(read_in_memory); then BytesIO object pointer is much more file-like, to do read() and seek(). referboto3 doc boto3 s3 api samples mdf4wrapper iftream to FILE what is the concept behind file pointer or stream pointer using io.BufferedReader on a stream obtained with open working with binary data in python read binary file and loop over each byte smart_open project PEP 3116 – New I/O]]></content>
      <tags>
        <tag>aws</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph & boto3 introduction]]></title>
    <url>%2F2020%2F02%2F28%2Fceph-boto3-intro%2F</url>
    <content type="text"><![CDATA[ceph introA Ceph Storage Cluster requires at least one Ceph Monitor, Ceph Manager, and Ceph OSD (Object Storage Daemon) ceph storage clusterThe Ceph File System, Ceph Object Storage and Ceph Block Devices read data from and write data to the Ceph Storage Cluster. cluster operations data placement pools, which are logical groups for storing objects Placement Groups(PG), PGs are fragments of a logical object pool CRUSH maps, provide the physical topology of the cluster to the CRUSH algorithm to determine where the data for an object and its replicas should be stored, and how to do so across failure domains for added data safety among other things. Balancer, a feature that will automatically optimize the distribution of PGs across devices to achieve a balanced data distribution Librados APIs workflowapis can interact with: Ceph monitor as well as OSD. get libs configure a cluster handling the client app must invoke librados and connected to a Ceph Monitor librados retrieves the cluster map when the client app wants to read or write data, it creates an I/O context and bind to a pool with the I/O context, the client provides the object name to librados, for locating the data. then the client application can read or write data Thus, the first steps in using the cluster from your app are to 1) create a cluster handle that your app will use to connect to the storage cluster, and then 2) use that handle to connect. To connect to the cluster, the app must supply a monitor address, a username and an authentication key (cephx is enabled by defaul an easy way, in Ceph configuration file: 123[global]mon host = 192.168.0.1keyring = /etc/ceph/ceph.client.admin.keyring 1234567891011121314151617import radostry: cluster = rados.Rados(conffile='')except TypeError as e: print 'Argument validation error: ', e raise eprint "Created cluster handle."try: cluster.connect()except Exception as e: print "connection error: ", e raise efinally: print "Connected to the cluster." python api, has default admin as id, ceph as cluster name, and ceph.conf as confffile value creating an I/O contextRADOS enables you to interact both synchronously and asynchronously. Once your app has an I/O Context, read/write operations only require you to know the object/xattr name. 12345678910111213141516171819202122232425262728293031323334353637print "\n\nI/O Context and Object Operations"print "================================="print "\nCreating a context for the 'data' pool"if not cluster.pool_exists('data'): raise RuntimeError('No data pool exists')ioctx = cluster.open_ioctx('data')print "\nWriting object 'hw' with contents 'Hello World!' to pool 'data'."ioctx.write("hw", "Hello World!")print "Writing XATTR 'lang' with value 'en_US' to object 'hw'"ioctx.set_xattr("hw", "lang", "en_US")print "\nWriting object 'bm' with contents 'Bonjour tout le monde!' to pool 'data'."ioctx.write("bm", "Bonjour tout le monde!")print "Writing XATTR 'lang' with value 'fr_FR' to object 'bm'"ioctx.set_xattr("bm", "lang", "fr_FR")print "\nContents of object 'hw'\n------------------------"print ioctx.read("hw")print "\n\nGetting XATTR 'lang' from object 'hw'"print ioctx.get_xattr("hw", "lang")print "\nContents of object 'bm'\n------------------------"print ioctx.read("bm")print "Getting XATTR 'lang' from object 'bm'"print ioctx.get_xattr("bm", "lang")print "\nRemoving object 'hw'"ioctx.remove_object("hw")print "Removing object 'bm'"ioctx.remove_object("bm") closing sessions12345print "\nClosing the connection."ioctx.close()print "Shutting down the handle."cluster.shutdown() librados in Pythondata level operations configure a cluster handle To connect to the Ceph Storage Cluster, your application needs to know where to find the Ceph Monitor. Provide this information to your application by specifying the path to your Ceph configuration file, which contains the location of the initial Ceph monitors. 12import rados, syscluster = rados.Rados(conffile='ceph.conf') connect to the cluster 123456789cluster.connect()print "\nCluster ID: " + cluster.get_fsid()print "\n\nCluster Statistics"print "=================="cluster_stats = cluster.get_cluster_stats()for key, value in cluster_stats.iteritems(): print key, value manage pools 12345cluster.create_pool('test')pools = cluster.list_pools()for pool in pools: print poolcluster.delete_pool('test') I/O context to read from or write to Ceph Storage cluster, requires ioctx. 123ioctx = cluster.open_ioctx($ioctx_name)#ioctx = cluster.open_ioctx2($pool_id) ioctx_name is the name of the pool, pool_id is the ID of the pool read, write, remove objects 123ioctx.write_full("hw", "hello")ioctx.read("hw")ioctx.remove_object("hw") with extended attris 123456789101112131415ioctx.set_xattr("hw", "lang" "en_US")ioctx.get_xattr("hw", "lang")``` * list objs ```python obj_itr = ioctx.list_objects()while True: try: rados_obj = obj_itr.next() print(rados_obj.read()) RADOS S3 apiCeph supports RESTful API that is compatible with basic data access model of Amazon S3 api. 12345PUT /&#123;bucket&#125;/&#123;object&#125; HTTP/1.1DELETE /&#123;bucket&#125;/&#123;object&#125; HTTP/1.1GET /&#123;bucket&#125;/&#123;object&#125; HTTP/1.1HEAD /&#123;bucket&#125;/&#123;object&#125; HTTP/1.1 //get object info GET /&#123;bucket&#125;/&#123;object&#125;?acl HTTP/1.1 Amazon S3Simple Storage Serivce(s3) is used as file/object storage system to store and share files across Internet. it can store any type of objects, with simple key-value. elastic computing cluster(ec2) is Amazon’s computing service. there are three class in S3: servivce, bucket, object. there are two ways to access S3, through SDK(boto), or raw Restful API(GET, PUT). the following is SDK way. create a bucketPut(), Get(), return all objects in the bucket. Bucket is a storage location to hold files(objects). 123456789101112131415161718def create_bucket(bucket_name, region=None): try: if region is None: s3_client = boto3.client('s3') s3.client.create_bucket(Bucket=bucket_name) else: s3_client = boto3.client('s3', region_name=region) location = &#123;'LocationConstraint': region&#125; s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration=location) except ClientError as e: logging.error(e) return False return True response = s3_client.list_buckets()for bucket in response['Buckets']: print bucket upload filesbasically to upload a file to an S3 bucket. 12345678910111213def upload_file(file_name, bucket, object_name=None): if object_name is None: object_name = file_name # Upload the file s3_client = boto3.client('s3') try: response = s3_client.upload_file(file_name, bucket, object_name) except ClientError as e: logging.error(e) return False return True upload_file() handles large files by splitting them into smaller chunks and uploading each chunk in parallel. which is same logic as ceph to hide the lower-level detail of data splitting and transfering. upload_fileobj() accepts a readable file-like object, which should be openedin bin mode, not text mode: 123s3 = boto3.client('s3')with open("FILE_NAME", "rb") as f: s3.upload_fileobj(f, "BUCKET_NAME", "OBJECT_NAME") all Client, Bucket and Object have these two methods. download filesdownload_file() a pair of upload files method, which accepts the names of bucket and object to download, and the filename to save the downloaded file to. 12s3 = boto3.client('s3')s3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME') same as upload file, there is a read-only open method: download_fileobj(). bucket policy123456s3 = boto3.client('s3')result = s3.get_bucket_policy(Bucket='BUCKET_NAME')bucket_policy = json.dumps(bucket_policy)s3.put_bucket_policy(Bucket=bucket_name, Policy=bucket_policy)s3.delete_bucket_policy(Bucket='BUCKET_NAME')control_list = s3.get_bucket_acl(Bucket='BUCKEET_NAME') get objects12obj_list = s3.list_objects(Bucket='bucket_name')obj_cont = s3.get_object(Bucket='bucket_name', Key='file_key') error: botocore.errorfactory.NoSuchKey: An error occurred (NoSuchKey) when calling the GetObject operation: Unknown read objects through url (restful api)through url path to get a file/object: https://&lt;bucket-name&gt;.s3.amazonaws.com/&lt;key&gt; referceph official doc botocore official doc boto3 github amazon S3 example GUI: create an S3 bucket and store objects in AWS boto API access S3 object]]></content>
      <tags>
        <tag>aws</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[from HPC to cloud (2) - distributed storage intro]]></title>
    <url>%2F2020%2F02%2F27%2Ffrom-HPC-to-cloud-2%2F</url>
    <content type="text"><![CDATA[常见分布式存储背景 块存储 vs 文件存储 文件存储读写慢，利于共享；块存储读写快，不利于共享。所有磁盘阵列都是基于Block块的模式，而所有的NAS产品都是文件级存储。物理块与文件系统之间的映射关系：扇区→物理块→逻辑块→文件系统。所以，文件级备份，需要一层一层往下查找，最后才能完成整个文件复制。 对象存储 vs 块存储 富含元数据的对象存储使大规模分析成为企业的有吸引力的命题。对象存储也使得提供和管理跨越地理位置的存储变得相对经济。同时，块存储的属性使其成为高性能业务应用程序、事务数据库和虚拟机的理想选择，这些应用程序需要低延迟、细粒度的数据访问和一致的性能。块存储(云盘)只能被一个主机挂载。 文件存储 vs 对象存储 对象存储(ceph, Linux block device)和文件系统(hdfs, gfs, restful Web API)在接口上的本质区别是对象存储不支持fread(), fwrite()类似的随机位置读写操作，即一个文件PUT到对象存储里以后，如果要读取，只能GET整个文件，如果要修改一个对象，只能重新PUT一个新的到对象存储里，覆盖之前的对象或者形成一个新的版本。对象存储的接口是REST风格的，通常是基于HTTP协议的RESTful Web API，通过HTTP请求中的PUT和GET等操作进行文件的上传即写入和下载即读取，通过DELETE操作删除文件。文件存储读写api如，file.open(); 对象存储读写api如 s3.open()。文件存储，多次读写；对象存储，一次写，多次读。对象存储的容量是无上限的；文件存储的容量达到一定上限，性能会极差。 对象存储，需要对原数据做解析吗？ 存储类型 块存储Direct Attach Storage(DAS), 每台主机服务器有独立的存储设备，各存储设备之间无法互通，需要跨主机存取。 Storage Area Network(SAN), 高速网络连接的专业存储服务器，与主机群通过高速i/o连结。 文件存储Network Attached Storage(NAS), 连接在网络上并提供存取服务。采用NFS或CIFS命令集访问数据，以文件为传输协议，通过TCP/IP实现网络化存储，可扩展性好、价格便宜、用户易管理 对象存储核心是将数据通路（数据读或写）和控制通路（元数据）分离，并且基于对象存储设备（Object-based Storage Device，OSD）构建存储系统。对象存储设备具有一定的智能，它有自己的CPU、内存、网络和磁盘系统. 一般配置metadata server(MDS)。 存储用例 对象存储非结构化(文档、图像、视频等)、备份存档、大数据分析（无实时性） 块存储比较适合数据库业务、虚拟机 文件存储企业共享、hpc大数据分析、流媒体处理，web服务。 容器云容器云架构12345控制台门户(管理员入口）saas(）paas(容器，微服务、中间件、cicd)资源调度管理平台iaas(计算、存储、网络），虚拟化提供资源池， 物理机直接架构k8s 容器云与私有云搭建的硬件资源分开。 虚拟层上支持k8s 容器部署在虚拟机上的原因：既需要容器，也需要虚拟机的环境。一般互联网公司倾向分开容器的资源 和 虚拟机的资源。不管是容器云，还是虚拟机，都会有一个web管理平台。考虑弹性升缩，vm更灵活。而如果容器直接部署在x86裸机上，如果没有容器的管理层，docker容器挂了，就无法恢复。 应用部署容器云 vs 私有云 仿真计算，倾向容器环境 CI/CD开发工具、中间件，倾向容器环境 web server，倾向容器环境 sql，倾向物理机或虚拟机 参考KingStack UCloud EasyStack]]></content>
  </entry>
  <entry>
    <title><![CDATA[from HPC to cloud computing]]></title>
    <url>%2F2020%2F02%2F25%2Ffrom-HPC-to-cloud-computing%2F</url>
    <content type="text"><![CDATA[HPCAnsys HPC, OpenFoam in amazon EC2, rescale, these are some samples of current CAE applications in HPC cloud. most CAE simulations, in nut, are solving large sparse matrix, either optmization or differential, which depends on famous linear equations solvers/libs, e.g. Petsc, libMesh, Intel Math Kernel e.t.c, inside which are implemented with message passing interface(MPI), share memory, or similar techs. why MPI is a have-to in these applications? because the problem interested itself is so huge, which is so much computing heavily than windows office, WeChat. as engineers want the result more preciesly, the problem interested dimension will increase more. e.g. a normal static stress analysis of vehicle engine is about 6 million elements, each of which will take 6 ~ 12 vertexes, and each vertex has 6 DoF, which gives about a kinematic matrix with size about 200 million x 200 million, no single PC can store this much data, and not even to do the matrix computing then. so all these problems have to be running on super computers, or high-performance computer(HPC), which have more memory or CPU cores. beyond large memory and CPU cores, also need fast-speed network, as each CPU can do calculation really fast, 2 ~ 4 GHz, so if the system can’t feed data that fast, the CPUs are hungry, which is the third feature of HPC: Gb high-speed internal network. as the NUMA/multi-core CPU architecture is popular now, to achieve the best performance from these chips is also a hot topic. cloudif taken HPC as centerlized memory and CPU cores(the Cathedral), then cloud is about distributed memory and CPU cores(the Bazaar). cloud is more Internet, to connected weak-power nodes anywhere, but totally has great power. the applications in cloud must be easy to decomposed in small pieces, so each weak-power node can handle a little bit. when coming to cloud computing, I’d think about Hadoop, OpenStack, and docker. hadoop is about to analysis massive data, which is heavily used in e-commercial, social network, games. docker is the way to package small application and each of the small image can run smoothly in one node, which currently is call micro-serivce, which is exactly as the name said, MICRO. OpenStack take care virtualization layer from physical memory, cpu resources, which used about in public cloud, or virtual office env. A or Bcloud is more about DevOps, as each piece of work is not difference from the single PC, but how to deploy and manage a single piece of work in a large cloud is the key, so develop and operations together. compare to HPC, both develop and operation needs professions. cloud computing has extended to edge computing, which is more close to normal daily life, and more bussiness-valued; while HPC guys are more like scientist, who do weather forcast, rocket science, molecular dynamics e.t.c. finally, the tech itself, cloud or HPC, has nothing to do with the bussiness world. refercloudscaling: grid, hpc, cloud… what’s the difference linkedin: supver computer vs cloud]]></content>
  </entry>
  <entry>
    <title><![CDATA[mdf4 reader]]></title>
    <url>%2F2020%2F02%2F24%2Fmdf4-reader%2F</url>
    <content type="text"><![CDATA[read mdf4 in general the most important “branch” in the tree is the list of data groups (DG block). record used to store the plain measurement data, the records can either be contianed in one single “data” (DT) block, or distributed over several DT blocks using a “data list” block . Each DG block has points to the data block, as well as necessary information to understand and decode the data. the sub-tree of DG is channel group(CG) blocks and channel(CN) blocks. record layouteach record contains all signal values which have the same time stamp, in the same channel. for different channel groups(CG), each record must start with a “record id” the record layout of ID 1, will be described by one channel group, the record layout of ID 2 by another channel group. Both channel groups must be children of the DT block’s parent data group. channelsEach channel describes a recorded signal and how its values are encoded in the records for the channel’s parent CG block. vector APIswhich is licensed by vector GetFileManager() OpenFile() GetChannelByName() GetChannelGroup() CreaterDataPointer() GetRecordCount() SetReadMode() SeekPos() GetPos() GetTimeSec() ReadPhysValueDouble(pChannle, &amp;value, &amp;isvalid) ReadRawValueDouble(pChannel, &amp;rawValue) ReadPhysValueDouble(pChannel, &amp;phyValue) MultiReadPhysValueDouble(pChannel, valueBuffer, size, &amp;read) turbolabReadFileMF4 class: Open() Seek(pos) Read(Size, Buffer) Close() CMdf4Calc class : Cmdf4Calc(pChan, pblock) Cmdf4Calc(pChan, mf4file) cmf4DataGroup class: GetRecord() GetRawValueFromRecord() GetValueFromRecord() mdf4FileImport class : ImportFile() ReleaseFile() I am not reading this C++ code. asammdfMDF class init(name=None, ): name can be either string or BytesIO filter(channels, ) get_group(index, channels=None, ) iter_channels(,) iter_get(channel_name=None,group_id=None, group_index=None, ) iter_groups() to_dataframe(), generate pandas DataFrame whereis(channel_name) MDF4 classincludes data_group, channel_group, channels, data_block, data_location, record_size e.t.c get(channel_name=None, group=None, ) get_can_signal() get_channel_name(group, index) get_channel_unit(channel_name=Nont, group=None, ) get_master(index, data=None, ), return the master channel samples for given group info() the following are sub data blocks of MDF4: Channel class ChannelGroup class DataGroup class DataList class DataBlock class HeaderBlock class 12mdf = MDF4("basic.mf4")db_info = mdf.info() all group, channel data is packages as python dict. the most exciting part, as asam-mdf can support BytesIO, which gives the way to read mdf files stores in Amazon S3: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 mf4_ = obj_api.get_object("bucket_name", "sample.mf4") fid_ = io.BufferedReader(mf4_['Body']._raw_stream) ; read_in_memory = fid_.read() bio_ = io.BytesIO(read_in_memory); if bio_.seekable(): mdf_ = MDF(bio_) mdf_.info()``` ### installation of asam-mdf* [install pandas](https://www.pypandas.cn/en/docs/installation.html#installing-using-your-linux-distribution’s-package-manager)`python3 -m pip install pandas` (not working)`sudo apt-get install python3-pandas` * [install canmatrix](https://canmatrix.readthedocs.io/en/latest/installation.html)`sudo python3 setup.py install` * install pathlib2 `sudo -H pip3 install pathlib2 `if `WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.`using: `sudo -H python3 -m pip install（包名）`## [mdfreader](https://github.com/ratal/mdfreader)which is open source LSP licensed. and the APIs are clear about getChannels, and read data from each channel, which was my first choice. but later found out it's difficult to support streamIO input, which was the best format I can reached from Amazon S3 pipeline.##### MdfInfo* read_info()* list_channels()##### Mdf* read()* filter_channel_names()* get_channel_data()* get_channel_unit()* keep_channels()```pythonimport mdfreaderfilename="basic.mf4"file_info=mdfreader.MdfInfo()channels = file_info.list_channels(filename)print(channels)file_pointer = mdfreader.Mdf(filename)keys_ = file_pointer.keys()for k_ in keys_: chan1 = file_pointer.get_channel_data(k_) print(chan1) python necessarybuffered iobinary I/O, usually used in following: 123f = open("mdf_file", "rb")type(f)&lt;type '_io.BufferedReader'&gt; open(file, mode=’r’, buffering=-1, encoding=…) ifmode=&#39;b&#39;, the returned content is bytes object, can’t encoding. if buffereing is not setted, for binary file with buffering with a fixed length. when with mode=&#39;w/r/t, it returns io.TextIOwrapper, if mode=rb, it returns io.BufferedReader, else if mode=wb, it returns io.BufferedWriter, else mode=wrb, it returns io.BufferedRandom. check here dict built-in funs12345678910cmp(dict1, dict2) ;len(dict)str(dict)dict.clear()dict.get(key)dict.has_key(key)dict.items()dict.keys()dict.values()pop(key) a bit about readersa few years ago I was in CAE field, dealing with different kind of CAE format, e.g .nas. during that time, there is a similar need to have a file transfer from vendor specified format to python format e.t.c. so now it comes again, which is funny to know adapters in CAE and autonomous driving design. referasammdf API doc asam mdf4 doc]]></content>
  </entry>
  <entry>
    <title><![CDATA[gpu rendering in k8s cloud]]></title>
    <url>%2F2020%2F02%2F21%2Fgpu-rendering-in-k8s-cloud%2F</url>
    <content type="text"><![CDATA[backgroundto deploy simulation in docker swarm, there is a hurting that GPU rendering in docker requires a physical monitor(as $DISPLAY env variable). which is due to GeForece(Quadro 2000) GPUs, for Tesla GPUs there is no depends on monitor. really intertesting to know. there were two thoughts how to deploy simulation rendering in cloud, either a gpu rendering cluster, or a headless mode in container cluster. unity renderingunity rendering pipeline is as following ： CPU calculate what need to draw and how to draw CPU send commands to GPU GPU plot direct renderingfor remote direct rendering for GLX/openGL, Direct rendering will be done on the X client (the remote machine), then rendering results will be transferred to X server for display. Indirect rendering will transmit GL commands to the X server, where those commands will be rendered using server’s hardware. 12direct rendering: YesOpenGL vendor string: NVIDIA Direct Rendering Infrasturcture(DRI) means the 3D graphics operations are hardware acclerated, Indirect rendering is used to sya the graphics operations are all done in software. DRI enable to communicate directly with gpus, instead of sending graphics data through X server, resulting in 10x faster than going through X server for performance, mostly the games/video use direct rendering. headless modeto disable rendering, either with dummy display. there is a blog how to run opengl in xdummy in AWS cloud; eitherwith Unity headless mode, When running in batch mode, do not initialize graphics device at all. This makes it possible to run your automated workflows on machines that don’t even have a GPU. to make a game server running on linux, and use the argument “-batchmode” to make it run in headless mode. Linux -nographics support -batchmode is normally the headless flag that works without gpu and audio hw acceleration GPU containera gpu containeris needs to map NVIDIA driver from host to the container, which is now done by nvidia-docker; the other issue is gpu-based app usually has its own (runtime) libs, which needs to install in the containers. e.g. CUDA, deep learning libs, OpengGL/vulkan libs. k8s device plugink8s has default device plugin model to support gpu, fpga e.t.c devices, but it’s in progress, nvidia has a branch to support gpu as device plugin in k8s. nvidia docker mechanismwhen application call cuda/vulkan API/libs, it further goes to cuda/vulkan runtime, which further ask operations on GPU devices. cloud DevOpssimulation in cloudso far, there are plenty of cloud simulation online, e.g. ali, tencent, huawei/octopus, Cognata. so how these products work? all of them has containerized. as our test said, only Tesla GPU is a good one for containers, especially for rendering needs; GeForce GPU requires DISPLAY or physical monitor to be used for rendering. cloud vendors for carmakersas cloud simulation is heavily infrastructure based, OEMs and cloud suppliers are coming together: FAW -&gt; Ali cloud GAC -&gt; Tencent Cloud SAC -&gt; Ali cloud / fin-shine Geely -&gt; Ali cloud Changan -&gt; Ali cloud GWM -&gt; Ali cloud Xpeng, Nio (?) PonyAI/WeRide (?) obviously, Ali cloud has a major benefit among carmakers, which actually give Ali cloud new industry to deep in. traditionally, carmakers doesn’t have strong DevOps team, but now there is a change. we can see the changing, as connected vehicles service, sharing cars, MaaS, autonomous vehicles are more and more requring a strong DevOps team. but not the leaders in carmakers realize yet. fin-shineSAC cloud, which is a great example for other carmakers to study. basically they are building the common cloud service independently. container service &lt;– k8s/docker storage &lt;– hdfs/ceph big data &lt;– hadoop/sql middleware applications &lt;– AI, simulation it’s an IP for SAC, but on the other hand, to maintain a stable and user-friendly cloud infrastructure is not easy at all. referdocker blender render cluster nvidia vgpu using GPUs with K8s K8S on nvidia gpus openGL direct hardware rendering on Linux runinng carla without display tips for runing headless unity from a script Unity Server instance without 3d graphics a blog: nvidia device plugin for k8s k8s GPU manage &amp; device plugin mechanism]]></content>
  </entry>
  <entry>
    <title><![CDATA[review-apollo (2).md]]></title>
    <url>%2F2020%2F02%2F17%2Freview-apollo-2%2F</url>
    <content type="text"><![CDATA[review apollo (2)EMPlannerin review apollo(1), we had went through perception, prediction and part of planning module. the ADS planning problem can be summaried with a few input conditions, we can get a few drivable driving path, now EMPlanner, here need to find out the best one. input conditions: perception and prediction obstacles and its future(in 5s) trajectory current road status, e.g. special area, pedestrain cross e.t.c. both path and speed need to be planned or optimized. the core of EMPlanner is Dynamic programming(DP), which is an classic algorithm to search the global optimized solution. the DP garantee each sub-group best solution accumulated to the final global best solution. PathOptimizerthe DP used in path optimizer basic processes: starting from current location, in every time, move in x-direction about next n meters(to next location), and sample in y-direction at the next location, calcuate the cost from current location to each of the sampling point at the next location, and save cost value for each path for this sub path segment. (this is the different from DP to greedy algorithm), finally we retrieve all the cost values tables for each sub-path, and get the final best solution for the whole path. a few question about DP here: how long in x-direction need to plan ? KMinSampleDistance = 40.0 what size of the interval in x-direction is good ? 1step_length = math::Clamp(init_point.v() * SamplePointLookForwardTime, config_.step_length_min(), config_.step_length_max()); how many sample points in y-direction is needed ? 1num_sample_per_level = config_.sample_points_num_each_level(); how to sample in y-direction and x-direction ? which is case by case, depends on the ego’s maneuver as well as the lanes info. basically, we had RouteSegments/ReferenceLine set, which includes the filtered referencelines for ego vehicle. for each referenceLine, we can calcuate the cost, then figure out the min cost referenceline as planning module’s output. 1234567891011121314151617181920212223242526EMPlanner::Plan(&amp;start_point, *frame)&#123; foreach reference_line_info in frame-&gt;reference_line_info(): PlanOnReferenceLine();&#125;EMPlanner::PlanOnReferenceLine(start_point, frame, reference_line_info)&#123; foreach optmizer in tasks_ : ret = optmizer-&gt;Execute(frame, reference_line_info);&#125;PathOptimizer::Execute(*frame, *reference_line_info)&#123; ret = Process(speedData, reference_line, start_point, path_data);&#125;DpPolyPathOptimizer::Process(SpeedData, ReferenceLine, init_point, path_data)&#123; dp_road_graph.FindPathTunnel();&#125;DPRoadGraph::FindPathTunnel(init_point, &amp;obstacles, *path_data)&#123; CalculateFrenetPoint(init_point, &amp;init_frenet_frame_point_); GenerateMinCostPath(obstacles, &amp;min_cost_path); foreach point in min_cost_path: frenet_path.append(transfer(point)); path_data-&gt;SetFrenetPath(frenet_path);&#125; point2point cost calcualatione.g. from the left interval sample points(in y-direction) to current interval sample points(in y-direction). during this cost calcuation, consider pathCost, staticObstacleCost and DynamicObstacleCost. 12345TrajectoryCost::Calculate()&#123; total_cost += CalculatePathCost(); total_cost += CalculateStaticObstacleCost(); total_cost += CalculateDynamicObstacleCost();&#125; UpdateNode() 12345678910DPRoadGraph::GenerateMinCostPath()&#123; for(level=1; level &lt; path_waypoints.size(); ++level)&#123; level_points = path_waypoints[level]; //all sample points in y-direction at current level for(i=0; i&lt;level_points.size(); ++i)&#123; cur_point = level_points[i]; UpdateNode(prev_dp_points, level, &amp;trajectory_cost, &amp;cur_point); &#125; &#125;&#125; PathCost from each point at prevous level to current point at current level, consider the length, 1st order(speed), 2sec order(acc) should be smooth, can give a 5th-order polyfit, same as what’d done in referenceLine smoother. for each interval/curve polyfit(by curve.Evaluate()), we can re-sampling it more density, so at each new sampling point, calculate its (position, speed, acc)l, dl, ddl 123456789101112131415161718192021222324252627282930TrajectoryCost::CalculatePathCost(curve, start_s, end_s, curr_level)&#123; foreach sampling_point in (end_s - start_s): l = curve.Evalaute(0, sampling_point) // cost from l path_cost += l * l * config_.path_l_cost() // cost from dl dl = curve.Evaluate(1, sampling_point) path_cost += dl * dl * config_.path_dl_cost() // cost from ddl ddl = curve.Evaluate(2, sampling_point) path_cost += ddl * ddl * config_.path_ddl_cost() &#125;``` * StaticObstacleCostStaticObstacleCost() used the same idea from PathCost, basically, to resampling the interval, and calculate a cost on each of the sampling point. ```c++TrajectoryCost::CalculateStaticObstalceCost(curve, start_s, end_s)&#123; foreach curr_s in (end_s - start_s): curr_l = curve.Evaluate(0, curr_s, start_s); foreach obs_boundary :: static_obstacle_boundaries: obstacle_cost += GetCostFromObs(curr_s, curr_l, obs_boundary);&#125;TrajectoryCost::GetCostFrmObs()&#123; delta_l = fabs(adc_l - obs_center); obstacle_cost.safety_cost += config_.obstacle_collision_cost() * Sigmoid(config_.obstacle_collision_distance() - delta_l) ; DynamicObstacleCost 1234567891011TrajectoryCost::CalculateDynamicObstacleCost(curve, start_s, end_s)&#123; foreach timestamp: foreach obstacle_trajectory in dynamic_obstacle_boxes_: obstacle_cost += GetCostBetweenObsBoxes(ego_box, obstacle_trajectory.at(timestamp));&#125;TrajectoryCost::GetCostBetweenObsBoxes(ego_box, &amp;obs_box)&#123; distance = obstacle_box.DistanceTo(ego_box); if(distance &gt; config_.obstacle_ignore_distance()) return obstacle_cost ; obstacle_cost.safety_cost += ( config_.obstacle_collision_cost() + 20) * Sigmoid(config_.obstacle_collision_distance() - distance); static obstacles cost calculating is like one space-dimension, only consider cost from the fixed/static obstacle to the fixed sampling points. for dynamic obstacles case, as the obstacle is moving, so we need a time-dimension, to calculate the cost at each time interval, from the current obstacle location to the fixed sampling points. for dynamic obstacle cost, there is a risk cost (20), which need to take care. min cost pathwe got the cost matrix from the start_s level to the target_s/final level. but the final level in y-direction has 7 sampling points, all of which are drivable. so backward propagation, to get the best path (or a NODE in DAG) 1234DPRoadGraph::GenerateMinCostPath()&#123; for(cur_dp_node : graph_nodes.back())&#123; fake_head.UpdateCost() &#125; SpeedOptimizerthe previous PathOptimier give the best (s, l) location of ego should arrived at each interval section along the referenceline. so what’s the best speed to reach these interval section ? to do speed optimzier, two parts: constraint speed conditions, e.g. speed limit, upper/lower boundary of drivable area along the refereneline due to obstacle trajectory speed planning BoundaryMapper()for each obstalce’s trajectory, sampling in time-dimension, if its trajectory has overlapped with ego’s trajectory, mark the boundary box. 123456789101112131415161718StBoundaryMapper::CreateStBoundary()&#123; foreach trajectory_point in trajectory: obs_box = obstacle.GetBoundingBox(trajectory_point); for(path_s=0.0, path&lt;s &lt; discretized_path.Length(); path_s += step_length)&#123; curr_adc_path_point = discretized_path.EvaluateUsingLinearApproximation(); if( CheckOverlap(curr_adc_path_point, obs_box, st_boundary_config_.boundary_buffer()) ) &#123; &#125; &#125;&#125;// only when obs trajectory overlapped with ego's trajectoryStBoundaryMapper::MapWithDecision(path_obstacle, decision)&#123; boundary = StBoundary::GenerateStBoundary(lower_points, upper_points); path_obstacle-&gt;SetStBoundary(boundary); // assign different boundary type &#125; SpeedLimitDecider speed limit centripetal acc limit centripetal force limit DpStGraph::Searchso far we have info from boundaryMapper, which tells lower_s and upper_s of safe driving area along reference line, excluding the overlaping area of obstacle’s trajectories; as well as speed limit. so now in the driving safe area, which speed ego should take at each point is also done by DP, speed can be represent in time-space two dimension, in a T-S table. so speed DP can transfer to find the min cost [T,S] althrough the whole planning drive path. 123456789101112131415161718192021222324252627282930313233DpStGraph::Search(speed_data)&#123; InitCostTable().ok(); CalculateTotalCost().ok(); RetriveSpeedProfile(speed_data).ok();&#125;``` ### QpSplineSpeedOptimizer DPSpeedOptimizer output the value pair of (time, location)(t, s) at each interval, then can get the velocity, acc, angular velocity by differencial among the neighbor intervals. QPSpline is a better way to get a polyfit 2nd-order smoothed speed profile along the referenceLine, which is higher-precision than DP.### SpeedOptimizer and PathOptimizer fusionPathOptimizer outputs driving path in (accumulated_distance, section_direction distance)(s, l), SpeedOptimizer outputs driving path in (time, accumulated_distance)(t, s). now to get the best/optmized path along the referenceLine, need combine these two outputs.```c++for(cur_rel_time=0.0; cur_rel_time &lt; speed_data_.TotalTime(); cur_rel_time += TimeSec)&#123; speed_data_.EvaluateByTime(cur_rel_time, &amp;speed_point); if(speed_point.s() &gt; path_data_.discretized_path().Length()) break; path_data.GetPathPointWithPathS(speed_point.s(), &amp;path_point); path_point.set_s(path_point.s() + start_s); trajectory_point.mutable_path_point()-&gt;CopyFrom(path_point); trajectory_point.set_v(speed_point.v()); trajectory_point.set_a(speed_point.a()); trajectory_point.set_relative_time(speed_point.t() + relative_time); ptr_discretized_trajectory-&gt;AppendTrajectoryPoint(trajectory_point); &#125; the above fusion is on one ReferenceLine, for all referenceLine, we can reach out the min cost referenceline as following: Frame::FindDriveReferenceLineInfo(){ foreach reference_line_info in reference_line_info_ : if(reference_line_info.IsDrivable() &amp;&amp; reference_lien_info.Cost &lt; min_cost){ drive_reference_line_info_ = &amp;reference_line_info; min_cost = reference_line_info.Cost(); } return drive_reference_line_info_ ; } Planning in summaryPlanning module has three components, ReferenceLineProvider, Frame and EMPlanner. Frame has ReferencelineInfo, which has everything about once-time planning, including the min-cost ReferenceLineInfo from EMPlanner. EMPlanner do DP and QP for PathPlan and SpeedPlan, seperately, which give a best driving path for each RefereneLine. in the whole, we can reach out the global best(min-cost) referenceLine from EMPlanner, and store back in Frame. ControlControl module readin the planning ReferenceLine (Trajectory, VehicleStatus) and Localization, output control command. ControlComponent::Proc(){ chassis_msg = chassis_reader_-&gt;GetLatestObserved(); OnChassis(chassis_msg); trajectory_msg = trajectory_reader_-&gt; GetLatestObserved(); localization_msg = localization_reader_-&gt; GetLatestObserved(); status = ProduceControlCommand(&amp;control_command); control_cmd_writter_-&gt;Write(); } simulationsimulation is a system level verification/test tool, there are a few topics: simulate the worldhow to make a virtual world in simulation, quickly, easily, close to real and massively deploy-friendly. currently most commericial tools, e.g. PreScan, CarMaker, are supporting well real map(.osm), and on-line/distributed/cloud service, which is good way. a few pioneers are trying to make the virtual world by 3D build through image/point cloud scanning. which is maybe the closest way to the physical world, but need large computing resource. integrate ADS systemhow to integrate ADS system to simulation, easily. most simulation software/platform is independent from ADS core. for in-house prototype team, they both develop ADS and simulate in Matlab. which is pretty common in most OEM teams. in product phase, it’s common to package ADS core as ros node, and talk to the simulation software by ROS. friendly APIsystem level simulation has plenty of scenarios, requiring the simulation platform has friendly way to produce massively scenarios. including env, ego/npc behaviors, special cases e.t.c most common APIs are Python and Matlab. massively deploymentit’s back to the test purpose, simulation needs to cover as much as possible scenarios, which requires IT infrastructure to support massively deployment. e.g. cloud deployment `]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[review apollo (1)]]></title>
    <url>%2F2020%2F02%2F15%2Freview-apollo-1%2F</url>
    <content type="text"><![CDATA[dig into Apollo/Daohu527 and YannZ/Apollo-Notes are some good summary of Apollo. localizationrtk!image 12345678910111213141516171819RTKLocalizationComponent::InitIO()&#123; corrected_imu_listener_ = node_ -&gt;CreateReader&lt;localization::CorrectImu&gt;(); gps_status_listener_ = node_ -&gt; CreateReader&lt;driver::gnss::InsStat&gt;();&#125;RTKLocalizationComponent::Proc(&amp; gps_msg)&#123; localization_-&gt;GpsCallback(gps_msg); if(localization_-&gt;IsServiceStarted()) &#123; LocalizationEstimate localization; localization_-&gt;GetLocalization(&amp;localization); localization_-&gt;GetLocalizationStatus(&amp;localization_status); PublishPoseBroadcastTopic(localization); &#125; &#125; ndtnormal distribution transform(NDT) is to match our sensors view points compare to what we see on an existing map. due to the view points from sensor may be a little off from the map, or the world might change a little between when built the map and when we record the view points. so NDT will try to match view points to a grid of probability functions created from the map, instead of the map itself. msfperceptionmodule overview1234567891011121314151617181920212223242526272829303132333435363738394041424344void Perception::RegistAllOnboardClass() &#123; RegisterFactoryLidarProcessSubnode(); RegisterFactoryRadarProcessSubnode(); RegisterFactoryFusionSubnode(); traffic_light::RegisterFactoryTLPreprocessorSubnode(); traffic_light::RegisterFactoryTLProcSubnode();&#125;``` the data flow in perception has two ways, either ROS message send/subscribe, or shared data. Apollo design a `global map` data structure:```c++GlobalFactoryMap&lt;string, map&lt;string, ObjectFactory*&gt; &gt;``` the first string element can be `SubNode` or `ShareData`.e.g. * lidar share data can stored as: `GlobalFactorMap[sharedata][LidarObjectData]`* radar subnode data can stored as: `GlobalFactorMap[Subnode][RadarObjectData]`#### DAG processafter ros subnode and shared data registered(not instance), [perception module create a DAG](https://github.com/YannZyl/Apollo-Note/blob/master/docs/perception/perception_software_arch.md)(directional acyclic graph) process, which includes subNode, Edge, and ShareData. `Edge` defines start node and end node of the data flow. e.g. LidarProcessSubnode -&gt; FusionSubnode.* subnode init ```c++DAGStreaming::InitSubnodes()&#123; for(auto pair: subnode_config_map) &#123; Sunode* inst = SubnodeRegisterer::GetInstanceByName(subnode_config.name()); inst-&gt;Init() ; &#125; &#125; edge init 1234DAGStreaming::Init()&#123; event_manager_.Init(dag_config_path.edge_config());&#125; it’s easy to understand EventManager is used to register edge, as the data flow either by ros node or by share data is event driven. inside EventManager there are two variables: event_queue_map &lt;eventID, message_queue&gt; event_meta_map &lt;eventID, message_info&gt; shareData init 1234DAGStreaming::Init()&#123; InitSharedData(dag_config.data_config());&#125; each subnode and shareData is a separate ROS/data thread, which started after DAG process initialization finished, the run() process is the event loop: 1234567891011121314151617181920DAGStreaming::Schedule()&#123; for(auto&amp; pair: subnode_map_) &#123; pair.second-&gt;Start(); &#125; for(auto&amp; pair : subnode_map_) &#123; pair.second-&gt;Join(); &#125;&#125;Subnode::Run()&#123; while(!stop) &#123; status = ProcEvents(); &#125;&#125; Lidar process hdmap ROI filter. basically compare the lidar cloud points to hdmap area, and filter out the cloud points outside of ROI. CNN image segmentation, basically classfiy the point clouds as obstacles based on CNN. obstacle minBox obstalce tracking lidar fusion Lidar &amp; Radar fusionthe data fusion ros node proces in the following way: step1: collect sensor’s data 1234567891011121314151617181920212223242526272829303132333435363738394041424344FusionSubnode::Process()&#123; BuildSensorObjs(events, &amp;sensor_objs); fusion_-&gt;Fusion(sensor_objs, &amp;objects+);&#125;FusionSubnode::BuildSensorObjs(&amp;events, std::vector&lt;SensorObjects&gt; * multi_sensor_objs)&#123; foreach event in events: &#123; if( event.event_id == lidar ) &#123; sensor_objects.sensor_type = VELODYNE ; &#125; else if ( event.event_id == radar)&#123;&#125; else if (event.event_id == camera) &#123;&#125; else&#123;return false;&#125; &#125; multi_sensor_objs-&gt;push_back(*sensor_objects); &#125; ``` step2: process sensors data```c++ProbabiliticFusion::Fuse(multi_sensor_objs, *fused_objs)&#123; sensor_mutex.lock(); foreach sensor_objs in multi_sensor_objs: &#123; sensor_manager_-&gt;AddSensorMeasurements(sensor_objs); &#125; sensor_manager_-&gt;GetLatestFrames(fusion_time, &amp;frames);&#125; sensor_mutex.unlock();sensor_mutex.lock();foreach frame in frames: FuseFrame(frame)CollectFusedObjects(fusion_time, fused_objects);sensor_mutex.unlock(); the data structure for sensor_type, timestamp and perceptioned obj is: std::map&lt;string sensorType, map&lt;int64, SensorObjects&gt; &gt; sensors_; sensors_[sensor_id][timestamp] return the sensor’s object at a special timestamp for Lidar and Radar fusion, we get a frame list, each frame has the object list from both Lidar and Radar. basically, so far it is just matching Lidar objects to Radar objects, assiged to timestamp. Lidar objects and Radar objects are indepenent. predictionin Apollo, prediction module anticipatd the future motion trajectories of the perceived obstacles. prediction subscribe to localization, planning and perception obstacle messages. when a localization update is received, the prediction module update its internal status, the actual prediction is triggered when perception sends out its perception obstacle messages, as following code : 123MessageProcess:OnLocalization(*ptr_localization_msg);MessageProcess:OnPlanning(*ptr_trajectory_msg)MessageProcess:OnPerception(*ptr_perception_msg, &amp;prediction_obstacles) take a detail review of OnPerception as following: 12345678910MessageProcess:OnPerception(perception_obstacles, prediction_obstacles)&#123; ptr_obstalces_container = ContainerManager::Instance()-&gt;GetContainer&lt;ObstaclesContainer&gt;(); ptr_trajectory_container = ContainerManager::Instance()-&gt;GetContainer&lt;ADCTrajectoryContainer&gt;(); EvaluatorManager::Instance()-&gt;Run(ptr_obstacles_container); PredictorManager::Instance()-&gt;Run(perception_obstacles, ptr_trajectory_container, ptr_obstacles_container) ContainerManagerused to instancialize an instance of the special container, which is used to store the special type of message. there are three types of contianer: ObstaclesContainer used to store obstalce info and its lane info. obstacle info coming from perception; its lane info coming from hd map and LaneGraph, LaneSequence and LaneSegment. check the explanation of ObstacleContainer LaneGraph is namely the lane network. e.g. the pre/post lane, neighbor lane, or where the lane disappear. LaneSegment is the element/piece of discretization of the lane network, which has a start point and end point. LaneSequence is a set of all possible next lane choices from current lane, which is only based on the lane network and current obstacle velocity, rather than from planning. PoseContainer used to store vehicle world coord and the coord transfer matrix, velocity info e.t.c ADCTrajectoryContainer EvaluatorManagerwhat evaluator does, is to compute the probability of each laneSequence, the obstacle is either vehicle, or pedestrain or bicycle. to compute probability is with multi-layer predictor(MLP) model: 123456MLPEvaluator::Evaluate(obstacle_ptr)&#123; foreach lane_sequence_ptr in lane_graph-&gt;lane_sequence_set: ExtractFeatureValues(obstacle_ptr, lane_sequence_ptr, &amp;feaure_values) probability = ComputeProbability(feature_values); &#125; ObstacleFeature there are 22 dimensions, which are greate resources to understand a good prediction model in ADS. these features includes, dimension, velocity, acc, direction of the obstacle, and relative position, angle to boundary lane e.t.c. LaneFeature since the laneSequence is already discritized as LaneSegment, for each lane Segment, Apollo calculate 4 features: the angle from lanePoint position to its direction vector; obstacle’s projected distance to laneSequence; the direction of lanePoint; the direction angle from lanePoint’s direction to obstacle’s direction. for each LaneSequnce only calculate its first 40 features, namely the first 10 LaneSegment. from 22 Obstacle features and 40 lane features, compute the probability of each LaneSequence. PredictorManagerthis is where we answer the predict question, where in the next 5 sec the obstacle located. after EvaluatorManager(), we get the probability of each LaneSequence. then choose the few LaneSequences with high probability, and predict a possible obstacle motion for each high-probability LaneSequence as well as with ADCTrajectory info from planning module. 123456789101112131415161718192021222324252627PredictorManager::Run(perception_obstalces)&#123; foreach obstacle in perception_obstacles: predictor-&gt;Predict(obstacle) foreach trajectory in predictor-&gt;trajectories(): prediction_obstacle.add_trajectory(trajectory) prediction_obstacles.add_prediction_obstalce(prediction_obstalce)&#125;``` basically for each obstacle, we do [prediction its next few seconds position](https://github.com/YannZyl/Apollo-Note/blob/master/docs/prediction/predictor_manager.md). there are a few predictors: MoveSequencePredictor, LaneSequencePredictor, FreeMovePredictor.```c++MoveSequencePredictor::Predict(obstacle)&#123; feature = obstalce-&gt;latest_feature(); num_lane_sequence = feature.lane_graph().lane_sequence_size() FilterLaneSequence(feature, lane_id, &amp;enable_lane_sequence) foreach sequence in feature.lane_sequence_set &#123; DrawMoveSequenceTrajectoryPoints(*obstacle, sequence, &amp;points) trajectory = GenerateTrajectory(points) trajectory.set_probability(sequence.probability()); trajectories_.push_back(trajectory) &#125;&#125; why need prediction modulewhether including motion prediction or not is based on the computing ability of ADS system. for low power system, the planning module update really high frequency, then there is no prediction, or prediction only need be considered in less than a few mic-sec; but for complex ADS system with low frequency update of planning, the ability to forsee the next few secs is very helpful. Planning!image VehicleStateProvider12345678class VehicleStateProvider &#123; common::VehicleState vehicle_state_ ; localization::LocalizationEstimate original_localization_ ; Update(); EstimateFuturePosition();&#125; P&amp;C Mapupdate routing responseduring PnC map, we need to query waypoints(in s, l coord), e.g. current vehicle position, target position, in which routing laneSegment. get laneSegment info from routing response 12345678PncMap::UpdateRoutingResponse(routing)&#123;foreach road_segment in routing.road() : foreach passage in road_segment.passage(): foreach lane in passage.segment(): &#123; all_lane_id.insert(lane.id()); &#125;&#125; query waypoints 123RouteSegments::WithinLaneSegment(&amp;lane_segment, &amp;waypoint)&#123; return lane_segment.lane-&gt;id().id() == waypoint.id() &amp;&amp; waypoint.s() &gt;= lane_segment.start_s &amp;&amp; waypoint.s() &lt;= lane_segment.end_s&#125; with UpdateRoutingResponse(), basically trafer waypoint in s,l coord to a more readable representation: [route_index, roadSegid, PassageID, laneSegmentID] generate RouteSegmentsbased on current vehicle status and routing response, get all driving-avaiable path set, each RouteSegment is one driving-avaiable path in a short period. The length of each RouteSegment depends on the both backward(30m by default) lookup length and forward lookup length, which depends on ego vehicle velocity, a time threashold(8s by default), min_distance(150m by default), max_distance(250m by default). check here UpdateVehicleState() it’s not about update vehicle velocity e.t.c, but find out vehicle current location adc_waypoint_ in the routing path and the next lane index next_routing_waypoint_index_. 123451. based on current vehicle velocity(x,y) and heading direction, lookup hdmap to find out all possible lanes, where current vehicle is on2. check if any lane from the possible lanes set, belong to any lanes from routingResponse.road(), the output is the filtered lanes set, on which the vehicle is and which belongs to routingResponse. 3. calculate vehicle projection distance to each lane in the filtered lanes set, the lane with min projection distance is the target/goal lane GetNeighborPassages() basically, check the next connected and avaiable channel. for situations, e.g. next cross lane is left turn, or lane disappear 123456789101112131415161718192021222324GetNeighborPassages(road, passage_index)&#123; if (routing::FORWARD) &#123; // keep forward, return current passage &#125; if (source_passage.can_exit()) &#123; // current passage disappear &#125; if (IsWaypointOnSegment(next_routing_waypoint)) &#123; // next routing waypoint is in current lane &#125; if(routing::LEFT or routing::RIGHT)&#123; neighbor_lanes = get_all_Left/Right_neighbor_forward_lane_id() foreach passage in road: foreach segment in passage: if(neighbor_lanes.count(segment.id())&#123; result.emplace_back(id); break; &#125; return result; &#125; GetRouteSegments() once we get the neighbor channels/passenges, the last step is to check if these passenges are drivable, only when current lane where ego vehicle located is the same lane or exactly next one lane when the vehicle projected on the passenge, and make sure the same direction. all other situations are not drivable. additionaly add forward and backward segments will give current RouteSegments. tips, passage is the channel where vehicle drive, or lane in physical road. but we use only LaneSegment, there is no keyword Lane. from RouteSegment to road sample pointsRouteSegment is similar as LaneSegments from hdMAP, including laneID, start_s, end_s; with additional mapPathPoint info, e.g. heading direction, and other traffic area property, which is used to lookup HD map. rather than LaneSegments is about 2m, RouteSegment length can be much longer, including a few LaneSegments. if each LaneSegment provides a sample point, and packaging these smaple points as MapPathPoint, then RouteSegments can be represented as a list of MapPathPoint. and in reality, both start_point and end_point of each LaneSegemnt also added as a sample point, but need take off overlap points. 1PnCMap::AppendLaneToPoints() each two mapPathPoint again can group as a LaneSegment2d, and for the LaneSegment2d in same lane, can joining as one LaneSegment: 123456Path::InitLaneSegments()&#123; for(int i=0; i+1&lt;num_points_; ++i)&#123; FindLaneSegment(path_points_[i], path_points_[i+1], &amp;lane_segment); lane_segments_.push_back(lane_segment); LaneSegment.Join(&amp;lane_segments);&#125; each small piece of LaneSegment2d helps to calculate the heading direction. 123456789101112131415161718192021Path::InitPoints()&#123; for(int i=0; i&lt;num_points_; ++i)&#123; heading = path_points_[i+1] - path_points[i]; heading.Normalize(); unit_directions_.push_back(heading); &#125;&#125;``` tips, LaneSegment2d is the small piece about 2m, LaneSegment is a larger segment, and has accumulated_start_s, and accumulated_end_s info.so far, we have LaneSegment and MapPathPoints set to represent the RouteSegment. the MapPointPoint is about 2m each, Apollo(why?) density it about 8 times to get a set of sample path points, which is about 0.25m each.```c++Path::InitWidth()&#123; kSampleDistance = 0.25 ; num_sample_points_ = length_ / kSampleDistance + 1 ; for(int i=0; i&lt;num_sample_points_; ++i)&#123; mapPathPoint_ = GetSmoothPoint(s); s += kSampleDistance ; &#125;&#125; finally, RouteSegment has traffic zone info, e.g. cross road, parking area e.t.c ReferenceLineProviderReferenceLineProvider has two funcs, check here: smoothing sample path_points to get a list of anchor points, which is the exactly vehicle driving waypoints. from path_points to anchor_points is resampling, with a sparse(5m) sampling distance, as well as additionally consider one-side driving correction factor. taking an example about one-side driving correction factor,: when driving on left curve, human driver keeps a little bit left, rather than keeping in the centerline. piecewise smoothing from anchor points the basic idea is split anchor points in n subgroups, and polyfit each subgroup; for the subgroup connection part need to make sure the zero-order, first order and second order differential smooth. which in final return to an optimization problem with constraints. Framethe previous PnCMap and ReferenceLine is in an ideal driving env, Frame class considers obstacles behavior, and traffic signs. check here obstacle lagPredictionpredictionObstacle info is filtered by the following two cases, before use as the obstacle trajectory for planning. for latest prediction obstacles only if perception_confidence is large than confidence_threshold(0.5 by default) and the distance from the prediction obstacle to ego vehicle is less than distance_threshold(30m by default), then this obstacle is considered for history prediction obstacles only if the prediction_obstacles has more than 3 obstacles. as well as each obstacle appear more than min_appear_num(3 by default), and for the same obstacle, the timestamp distance from its previous prediction to latest prediction is not longger than max_disappear_num(5 by default), then this obstcle need take considerion. relative position from ego vehicle to obstacleas we get the obstacles’ LagPrediction trajectory, as well as ReferenceLine for ego vehicle. now we combine this two information, to understand when the ego is safe and how far ego can drive forward. the output is the referenceline with each obstacle overlapped info. including the time low_t when overlap begins, and the time high_t when overlap ends; and the start location low_s-start and end location high_s-start of the overlap. rule based behavior to handle overlap areathere are 11 rule based behavior. backside_vehicle change_lane crosswalk destination front_vehicle keep_clear pull_over reference_line_end rerouting signal_light stop_sign Planning::RunOnce() { foreach ref_line_info in frame_-&gt;reference_line_info()){ traffic_decider.Init(); traffic_decider.Execute(&amp;ref_line_info); } }]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ads ros and simulator in one docker]]></title>
    <url>%2F2020%2F01%2F15%2Fads-ros-and-simulator-in-one-docker%2F</url>
    <content type="text"><![CDATA[backroundpreviously, we had test to run ads ros in one docker container, and lgsvl simulator in another docker container. once they are hosted in the same host machine, the ADS ros nodes and lgsvl can communicate well. based on this work, now it’s the time to integrate ADS ros nodes into the lgsvl docker. the basic idea of how to combine multi docker images into one, is use multistage-build, which I call “image level integration”. image level integrationbasically we have both ads_ros image and lgsvlsimulator image already, and there are a few components from ads_ros can be imported to lgsvlsimulator container: 123456FROM ads_ros:latest AS ADSFROM lgsvlsimulator:latestRUN mkdir /catkin_wsCOPY --from=ADS /root/catkin_ws /catkin_wsCOPY --from=ADS /opt/ros /opt/ros CMD [&quot;/bin/bash&quot;] the problem of image level integration, it actually miss some system level components: /etc/apt/sources.list.d/ros-latest.list, which then can’t update ros modules; other components, e.g. which are installed during building ads_ros image by apt-get install, which are go the system lib path, which of course can distinct out, and copy to lgsvlsimulator, but which is no doubt tedious and easy to miss some components. component level integrationas ads_ros is really indepent to lgsvlsimulator, so another way is use lgsvlsimulator as base image, then add/build ros component and ads_ros compnents inside. FROM ros:kinetic AS ROS # http://wiki.ros.org/kinetic/Installation/Ubuntu FROM lgsvlsimulator:latest RUN mkdir -p /catkin_ws/src COPY --from=ROS /opt/ros /opt/ros COPY --from=ROS /etc/apt/sources.list.d/ros1-latest.list /etc/apt/sources.list.d/ros1-latest.list # ADD ros key RUN apt-key adv --keyserver &apos;hkp://keyserver.ubuntu.com:80&apos; --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654 ## -------- install ads ros packages in lgsvlsimulator container ------------ ## ENV CATKIN_WS=/catkin_ws ENV ROS_DISTRO=kinetic ## https://docs.ros.org/api/catkin/html/user_guide/installation.html RUN APT_INSTALL=&quot;apt-get install -y --no-install-recommends&quot; &amp;&amp; \ apt-get update &amp;&amp; \ DEBIAN_FRONTEND=noninteractive $APT_INSTALL \ build-essential \ apt-utils \ ca-certificates \ psmisc \ cmake \ python-catkin-pkg \ ros-${ROS_DISTRO}-catkin \ ros-${ROS_DISTRO}-tf \ ros-${ROS_DISTRO}-turtlesim \ ros-${ROS_DISTRO}-rosbridge-suite \ iputils-ping \ net-tools # RUN source ~/.bashrc # copy ads ros into ws COPY /ads_ros/src $CATKIN_WS/src ### build msgs RUN /bin/bash -c &apos;. /opt/ros/${ROS_DISTRO}/setup.bash; cd ${CATKIN_WS}; catkin_make --pkg pcl_msgs autoware_msgs nmea_msgs &apos; ### build ros nodes RUN /bin/bash -c &apos;. /opt/ros/${ROS_DISTRO}/setup.bash; cd ${CATKIN_WS}; catkin_make &apos; # copy ros scripts COPY /ads_ros/script_docker $CATKIN_WS/script ###--------finished ads ros package -------------- ### CMD [&quot;/bin/bash&quot;] runtime issuewith the dockerfile above, we can build the docker image which include both lgsvl and ads ros. one runtime issue is due to lgsvl scenario is run with python3, while our ads ros, especially ros_bridge_launch is based on python2. so need some trick to add python2 at $PATH before python3 when launch ros_bridge, then exchange back.]]></content>
      <tags>
        <tag>lgsvl</tag>
        <tag>ros</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ads ros and lgsvl talk in dockers]]></title>
    <url>%2F2020%2F01%2F14%2Fads-ros-and-lgsvl-talk-in-dockers%2F</url>
    <content type="text"><![CDATA[backgoundpreviously, we had integrated ads ros nodes into one launch file. now we try to put ads nodes into docker, and talk to lgsvl simulator in another docker. run dockerimage12docker build -t ads_ros . docker run -it ads_ros /bin/bash unable to run cakin_make in dockerfile1234567891011121314151617181920212223242526272829303132333435363738394041424344RUN /bin/bash -c &apos;. /opt/ros/kinetic/setup.bash; cd &lt;into the desired folder e.g. ~/catkin_ws/src&gt;; catkin_make&apos;``` #### dockerfile for ads ros``` FROM ros:kinetic # create local catkin workspace ENV CATKIN_WS=/root/catkin_wsENV ROS_DISTRO=kineticRUN mkdir -p $CATKIN_WS/src## install catkin_make ## https://docs.ros.org/api/catkin/html/user_guide/installation.htmlRUN APT_INSTALL=&quot;apt-get install -y --no-install-recommends&quot; &amp;&amp; \ apt-get update &amp;&amp; \ DEBIAN_FRONTEND=noninteractive $APT_INSTALL \ build-essential \ apt-utils \ ca-certificates \ psmisc \ cmake \ vim \ python-catkin-pkg \ ros-$&#123;ROS_DISTRO&#125;-catkin \ ros-$&#123;ROS_DISTRO&#125;-tf \ ros-$&#123;ROS_DISTRO&#125;-turtlesim \ ros-$&#123;CATKIN_WS&#125;-rosbridge-suite iputils-ping \ net-tools ### add third-party headers# RUN source ~/.bashrc # copy ads ros into wsCOPY /src $CATKIN_WS/src### build msgs RUN /bin/bash -c &apos;. /opt/ros/$&#123;ROS_DISTRO&#125;/setup.bash; cd $&#123;CATKIN_WS&#125;; catkin_make --pkg pcl_msgs pb_msgs autoware_msgs mobileye_msgs ibeo_msgs nmea_msgs &apos; ### build ros nodes RUN /bin/bash -c &apos;. /opt/ros/$&#123;ROS_DISTRO&#125;/setup.bash; cd $&#123;CATKIN_WS&#125;; catkin_make &apos;# copy ros scriptsCOPY /script $CATKIN_WS/scripts # run ros shellWORKDIR $&#123;CATKIN_WS&#125;/scripts ros envsset ROS_IP on all involved containers to their IP, and set ROS_MASTER_URI to the IP of the roscore container. That would avoid the DNS problem. understand ros environment variables $ROS_ROOT : set the location whre the ROS core packages are installed $ROS_MASTER_URI : a required setting that tells nodes where they can locate the master $ROS_IP or $ROS_HOSTNAME : sets the declared network address of a ROS node get docker container’s IPdocker inspect -f &quot;{{ .NetworkSettings.Networks..IPAddress }}&quot; &lt;container_name||container_id&gt; tips, network_name: e.g. host, bridge, ingress e.t.c. with docker host net, then the container doesn’t have its own IP address allocated, but the application is available on the host’s IP address with customized port. run roscore in docker and talk to rosnodes at host start roscore from docker container as following : sudo docker run -it –net host ads_ros /bin/bash roscore start other ros nodes at host 1234567891011121314151617181920212223rosnode list ## &gt;&gt;&gt; /rosout source $ROS_PACKAGE/setup.shrosrun rtk_sensor rtk_sensor ### run successfully ``` how to understand ? once the docker container start with `host network`, the `roscore` run inside docker, is same as run in the host machine !! #### ads ros in docker talk to lgsvl in another docker with HOST network * once we start ads ros docker as following: ```shellsudo docker run -it --net host ads_ros /bin/bashroscore ``` * start lgsvl in docker: ```shell#! /usr/bin/env bashxhost + sudo nvidia-docker run -it -p 8080:8080 -e DISPLAY=unix$DISPLAY --net host -v /tmp/.X11-unix:/tmp/.X11-unix lgsvlsimulator /bin/bash then access webUI in host machine, and add host: 10.20.181.132 in Clusters page, and add 10.20.181.132:9090 for Selected Vehicles. as lgsvl is also in host network. so these two docker can communicate through ros well !! ads ros container talk to lgsvl container with ROS_IPsince lgsvl will run in docker swarm env, we can’t depend on host network, which requires ROS_IP env. the following test is in one host machine. in host terminal 123456789101112131415161718192021222324252627282930313233343536export ROS_MASTER_URI=http://192.168.0.10:11311export ROS_HOSTNAME=192.168.0.10export ROS_IP=192.168.0.10 roscore ``` * in ads ros docker ```shellsudo docker run -it \ --env ROS_MASTER_URI=http://10.20.181.132:11311 \ --env ROS_IP=10.20.181.132 \ ads_ros /bin/bashrosnod list ``` however, when start `roscore` in docker, it reports:```shellUnable to contact my own server at [http://10.20.181.132:33818/].This usually means that the network is not configured properly.A common cause is that the machine cannot ping itself. Please checkfor errors by running: ping 10.20.181.132``` if checking the IP address inside the docker container by `ifconfig`, which reports `172.17.0.3`, which then make sense that the container can't talk to `10.20.181.132`, which means we can't assign a special IP address for a docker container. so reset in the docker container as:```shellexport ROS_MASTER_URI=http://172.17.0.3:11311export ROS_HOSTNAME=172.17.0.3 actually, the host terminal can talk to the ads ros container directly, with no need to set $ROS_HOSTNAME &amp; $ROS_MASTER_URI specially; as well as another docker container in this host machine, e.g. lgsvl. a little bit knowledge about docker network. so each docker container does have an virtual IP, e.g. 172.17.0.1. while if run the docker image with host network, there is no special container IP, but the container directly share the IP of the host machine. as multi docker containers run in the same host machine, even without host network, they are in the same network range, so they can communicate to each other. additionaly for ros_master, which may requires to add $ROS_HOSTNAME &amp; $ROS_MASTER_URI. start lgsvl in another docker 123456789#! /usr/bin/env bashxhost + sudo nvidia-docker run -it \ -p 8080:8080 \ -e DISPLAY=unix$DISPLAY \ -v /tmp/.X11-unix:/tmp/.X11-unix \ --env ROS_MASTER_URI=http://172.17.0.3:11311 \ --env ROS_HOSTNAME=172.17.0.3 lgsvlsimulator /bin/bash in summaryso far, we have host ads ros in one docker, and lgsvl in another docker, and they are in the same machine, and they can talk to each other. the next thing is to put ads ros and lgsvl in one image. referrosdep docker 1.10 container’s IP in LAN listening to ROS messages in docker containers exposing ROS containers to host machine why you need IP address of Docker container catkin_make not found in dockerfile]]></content>
      <tags>
        <tag>lgsvl</tag>
        <tag>ros</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS ADS integrated to python talk to lgsvl]]></title>
    <url>%2F2020%2F01%2F10%2FROS-ADS-integrated-to-python-talk-to-lgsvl%2F</url>
    <content type="text"><![CDATA[backgroundpreviously, we had done ADS ros package directly talk with lgsvl. the step further is to trigger ADS ros nodes from scenario python script a few requirements: for ADS ros should be integrated with each scenario python script, which means, when the python script finished, the ADS ros should exit the ads ros package shell script is independ of python script execute shell in Python to call a shell command directly with os.system(cmd) or 1subprocess.call(&quot;ls -lt&quot;, shell=True) to run shell script with a python subprocess 1proc=subprocess.Popen([&quot;&quot;], stdout=subprocess.PIPE) subprocess.Popen() return the process object a few helpful commands to debug rosrun: 1234567ps -aux | grep &quot;roscore&quot; ps -aux | grep &quot;rosmaster&quot;killall -9 roscore killall -9 rosmaster ros1 to python3there is an real issue, due to the ADS ros package is mostly implemented by ROS1, maintained by the algorithm team. but lgsvl scenario is running with python3. ROS1 matches Python2. so there comes the solution, either to upgrade the ADS ros pakcage to ROS2, or find a way to run ROS1 in python3 env. conda base envwe had decided to adapt ROS1 in python3 env. as the host machine has conda env, first need to disable conda base. 1conda config --set auto_activate_base false as lgsvl scenario is runned with conda python3 env, inside which used subprocess to run ADS ros shell, in which creates a few new gnome-terminals, which are non log-in terminal, and the trick things here: even though disabled auto activate base, and the terminal has no header (base), but when check the python path, it still points to the conda/bin/python, which will fail rosbridge_launch.server, which is a pure ros1 and python2 module. so need check if conda --version in the ads ros shell script is in current terminal, if does, run conda deactivate, which gives error: 1CommandNotFoundError: Your shell has not been properly configured to use &apos;conda deactivate&apos;. it looks there is some mess up, with init conda.sh is in ~/.bashrc, but during the new terminal ceated, it doens’t confgiure all right. which can be fixed : 123456conda --version if [ $? == 0 ]then source ~/anaconda3/etc/profile.d/conda.sh conda deactivatefi in this way, we run lgsvl scenario in python3, as well as can new with python2 terminals to run ads ros nodes from this python3 terminal ros scripts path is not python pathas we try to separate python scripts from ads ros nodes, so need some global env variable for the ros scripts path. kill all ros nodes gracefullywe need restart/shutdown the ads ros nodes package at each time when python scenario script start/stop, which has two steps: to shutdown ros nodes, e.g. rosnode kill to close all the gnome-terminals to run the ros nodes the second step is very tricky, need some understand. A terminal is a file. Like /dev/tty. Files do not have a process id. The process that “owns” the terminal is usually called the controlling process, or more correctly, the process group leader. gnome-terminal runs a single pid, it creates a child process for each and every window and/or tab. and can retrieve these child processes by the command: $ pgrep -P &lt;pid_of_gnome-terminal&gt; Many terminals seem to mask themselves as xterm-compatible, which is reported by echo $TERM or echo $COLORTERM. 123$! is the PID of the last backgrounded process.kill -0 $PID checks whether it&apos;s still running.$$ is the PID of the current shell. #! /bin/bash gnome_pid=`pgrep gnome-terminal` subpids=`pgrep -P ${gnome_pid}` if [ ! $1 ]; then echo &quot;missing the gnome-terminal pid, exit&quot; exit -1 fi #shutdown the ros nodes rosnode kill -a &gt; /dev/null 2&gt;&amp;1 killall -9 rosmaster &gt; /dev/null 2&gt;&amp;1 #shutdown the terminals where ros nodes hosted for pid in $subpids do if [[ $pid != $1 ]] ; then echo $pid kill $pid fi done tips: the implement has a litle problem when integrate with lgsvl when the simulation time is short, e.g. 1sec, then ros_clear.sh can’t catch the opened gnome-terminals from ros_start.sh. find the process name using PID$ ps aux | grep PID $ ps -p PID -o format $ curpid=`ps -p $$ -o ppid=` the output of ps aux –&gt; in the foreground process groups –&gt; is a session leader summaryso far, we integrated ads ros packages into python, which then talk to lgsvl. referpythn subprocess from Jianshu understand linux process group pidof command which_term get the info of a PID get the pid of running terminal]]></content>
      <tags>
        <tag>lgsvl</tag>
        <tag>ros</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ADS stack in ros talk to lgsvl]]></title>
    <url>%2F2020%2F01%2F04%2FADS-stack-in-ros-talk-to-lgsvl%2F</url>
    <content type="text"><![CDATA[learning is about repeat 100 times !this maybe the third time to go through ROS, which still looks fresh, but does give a whole picture about how ROS works in ADS dev. what is ros topic and message the topic is the channel where nodes are subscribed for to read messages or where the nodes publish those messages; the message is the data itself, previously defined. 1234567891011rostopic echo [topic]rostopic list rosmsg show [message]rosrun [package_name] [node_name]rospack find [package_name]rosnode info [node_name] what is a publisher/subscriber a publisher (node) publishes messages to a particular topic. publish() is asynchronous, and only does work if there are subscribers connected on that topic. publish() itself is very fast, and it does as little work as possible: 12serialize the message to a buffer push the buffer onto a queue for later processing 1ros::Publisher advertise(const std::string&amp; topic, uint32_t queue_size, bool latch = false); the queue size is defined in publihser/outgoing message queue. if publishing faster than roscpp can send the message over the wire, roscpp will start dropping OLD messages. 1ros::Subscriber subscribe(const std::string&amp; topic, uint32_t queue_size, &lt;callback, which may involve multiple arguments&gt;, const ros::TransportHints&amp; transport_hints = ros::TransportHints()); the queue_size is the incoming message/subscriber queue size, roscpp will use for your callback. if messages are arriving too fast and you are unable to keep up, roscpp will start throwing away OLD messages. in summary: * publish() is asynchronous. * When you publish, messages are pushed into a queue (A) for later processing. This queue is immediately pushed into the outgoing/publisher queue (B) . PS: If no one subscribes to the topic, the end is here. * When a subscriber subscribes to that topic. Messages will be sent/pushed from the corresponding outgoing/publisher queue (B) to the incoming/subscriber queue (C).--&gt; this is done by internal thread * When you spin/ callback, the messages handled are from the incoming/subscriber queue (C). messages msg files are simple text files for specifying the data structure of a message. These files are stored in the msg subdirectory of a package. the publisher and subscriber must send and receive the same type/topic of message 1&#123;&quot;op&quot;: &quot;publish&quot;, &quot;topic&quot;: &quot;/talker&quot;, &quot;msg&quot;: &#123;&quot;data&quot; : &quot;_my_message&quot; &#125;&#125; lgsvl defines the rosBridge class, which has a Reader() and Writer(), corresponding to Subsciber and Publisher, respectfually. rosBridge in lgsvlrosbridge is an adapter for non-ros apps to talk with ros. it’s common to package ADS software as ros node during dev stage, and rosbridge is the common way to integrate simulator with ADS software. the base implementation of rosBridge in lgsvl is as following: RosBridge.cs12ConcurrentQueue&lt;Action&gt; QueuedActions ;Dictionary&lt;string, Tuple&lt;Func&lt;JSONNode, object&gt;, List&lt;Action&lt;object&gt;&gt;&gt;&gt; Readers ; the topic is packaged as: 1234&#123; &quot;op&quot;: &quot;subscribe or publish or call_service or service_response or set_level&quot;, &quot;topic&quot;: &#123;&#125;, &quot;type&quot;: &#123;&#125;&#125; AddReader(topic, callback) a few types/topices supported: Deteced3DObjectArray, Deteced2DObjectArray, VehicleControlData, Autoware.VehicleCmd, TwistStamped, Apollo.control_command which is the list of ros messages that lgsvl can parsing. 1234567if(!Readers.ContainsKey(topic))&#123; Readers.Add(topic, Tuple.Create&lt;Func&lt;JSONNode, object&gt;, List&lt;Action&lt;object&gt;&gt;&gt;(msg=&gt;converter(msg), new List&lt;Action&lt;object&gt;&gt;()) ) ;&#125; Readers[topic].Item2.Add(msg=&gt;callback(T)msg)); AddReader is the subscriber nodes in lgsvl. in Sensor group, there are three sensors do AddReader: GroundTruth3DVisualizer VehicleControlSensor GroudTruth2DVisualizer which means, lgsvl server can read these three typies of messsages. if there is no rendering/visual needs, only vehicle conroller (acc, brake) message is required for lgsvl. AddWriter(topic) the types/topic supported are: ImageData, PointCloudData, Detected3DObjectData, Detected2DObjectData, SignalDataArray, DetectedRadarObjectData, CanBusData, GpsData, ImuData, CorrectedImuData, GpsOdometryData, ClockData AddWriter() is a writer adapter, which returns a special type/topic writer/publisher. AddWriter is the publisher nodes in lgsvl, in Sensor group, the following sensors can publish message out: LidarSensor SignalSensor GpsInsSensor GpsOdometrySensor DepthCameraSensor ImuSensor SemanticCameraSensor GroudTruth2DSensor CanBusSensor RadarSensor GroudTruth3DSensor ClockSensor GpsSensor ColorCameraSensor AddService(topic, callback) OnMessage(sender, args) 1234567891011if (args.op==&quot;publish&quot;)&#123; topic = json[&quot;topic&quot;] Readers.TryGetValue(topic, out readerPair) var parse = readerPair.Item1 ; var readers = readerPair.Item2; var msg = parse(json[&quot;msg&quot;]); foreach (var reader in readers) &#123; QueuedActions.Enqueue(()=&gt;reader(msg)); // &#125; ros subscriber uses a callback mechanism, when the message is coming, all readers who subscribe this topic will read in this message. RosWriter.csthe message output is in the format as: 12345&#123; &quot;op&quot; : &quot;publish&quot; , &quot;topic&quot; : Topic, &quot;msg&quot;: message &#125; websocket in ros bridgethe implementation of ros bridge in lgsvl is by WebSocket, which maintained a continously communication pipeline, for external ros nodes publishers. ros ads talk to lgsvl rosbridgefrom the previous section, lgsvl talk to external ros nodes (which is the ADS stack) through rosbridge, and which needs external inputs, e.g. vehicle control command, 2d/3d ground truth visualizer. and the ADS stack ros nodes can subscribes Gps, canbus, Lidar, Radar, GroudTruth, SemanticCamera, DeepCamera topics through lgsvl rosbrige. so the pipeline are simple as following: lgsvl rosbridge --&gt; {gps, radar, camera, lidar, groudTruth message} --&gt; ADS stack nodes ADS stack nodes --&gt; {vehicle control command message} --&gt; lgsvl rosbridge usually, the ADS stack nodes are a few related ros nodes, including RTK/GPS sensor, Lidar/Camera/Radar sensors, hdmap node, data fusion node e.t.c. run ROS ADS stackthe following nodes are used commonly in ADS dev: gps_node subscribe: /gps, /odom e.t.c publish: /sensor/data topics camera_node subscribe: /camera/raw_data publish: /objects_list, /lane_object e.t.c radar_node subscribe: /radar/raw_data publish: /obstacle/velocity , /obstacle/distance hdmap_node subcribe: /rtk/data, /ins/data. publish: /lane/info, /speed_limit e.t.c. fusion_node subscribe topics: /canbus, rtk/data, /ifv_lane, /radar/blind, /radar/corner, /esr, /velodey, /lane/info e.t.c publish topics: /object_list/, /road/info, /vehicle/status and visual related messages e.t.c. planning_node subscribe topics: /object_list, /road/info, /vehicle/status from fusion_node publish topics: /vehicle/cmd_ctl ADS ros launchthe previous section has a basic idea about what kind of ROS nodes usually needs to run the ADS stack. In road test or simulaiton test, we prefer a simple way to launch all ADS related nodes in one launch. which usually give a simple launch file to start all nodes, or a shell script to start each ros nodes sequencially. system verificationros is a common solution to package ADS stack and integrate with the simulation env during SIL sysem verification, as well as road test data collection. since ROS is the cheap solution for ADS dev and test, compared to CAEape/Vector, Dspace solution. on the other hand, due to the hardware computing limitation, drivers depends, and espeically the ros is mostly run in Linux kind of non-realtime OS, which is not good for domain controller(HAD) test. so during the real test envrionment, we need to consider these true issues. another topic is about how to quick build up a user-friend verification pipeline for, which keeps as little modification as possible, but can run in both simulation verification and physical verification. referros overview understand rosbridge: simple ROS UI roslibjs]]></content>
      <tags>
        <tag>lgsvl</tag>
        <tag>ros</tag>
        <tag>self-driving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mobile vehicle interface]]></title>
    <url>%2F2019%2F12%2F29%2Fmobile-vehicle-interface%2F</url>
    <content type="text"><![CDATA[backgroundhuman-machine interface(HMI), in-vehicle device(IVD). human-vehicle interface(HVI), e.g. voice control, touch screen, face detection, e.t.c, which does well for young and passion generation, who have great voice, sharp fingers, and good-look face, and all these improving user experienced tech are based on similar AI. what about the elders ? which should be, in the next 10 - 20 years, the most tough family and society issues in China, and as well as for Japan right now. the basicaly option here is HVI is not enough for the elders groups, or at least is not the only choice for elders, who don’t have great voice, good-look face, or sharp fingers, and even no patient to talk/look/touch with the weak AI system. a very good alternative or additional supportting solution should be mobility vehicle interface(MVI) mobility vehicle interfacewhat kind of mobility devicemobility is a general name, for all kinds of mobile devices, especially for Iphone, Android smart phone and personal care robot. application scenariosfor an elder, who has daily needs for traffic transfer. e.g. go to hospital, to supermarket, to a special restuarant or a park for dinner or sit down with some old friends. the elders are slow in movement and talk, and the current AI based human-vehicle-interface(HVI) definitely make the elders feel pressure. a better or alternative solution is let the mobility device to talk to the vehicle, through an interface mobility vehicle interfacethe mobility vehicle interface(MVI) can be based on existing vehicle OS and mobility OS. there are plenty existing in-vehicle OS, e.g. CarPlayer, Baidu OS, GENIV, QNX e.t.c; and on mobility OS side, the most common are iOS and Andriod. most in-vehicle OS can run the same apps on mobility OS, e.g. google navigation map, instant messages, music, emergency call service e.t.c. so the first solution is app2app. basically the same app ran in the personal mobility device(PMD) talk to the same app run in vehicle OS. PMD has more time with the owner, so has more personality than vehicle, especially as vehicle in future is more like a public service, rather than a personal asset. that’s why mobility vehicle interface(MVI) is a good option, especially for elders, who may not enjoy talk to AI. beyond the easy to implement at this moment, app2app solution has a few limitations: the security is mainly provided by the app supplier, which is not a unit solution, as different app suppliers have different security mechanism. as apps hosted in this system is growing, the adapters or interfaces to make the bridge grows too, which decrease the user experince and increase the system cost. so a better solution is a new mobility-vehicle interface protocol, which is the only bridge between personal mobility and vehicles. and no matter what kind of apps and how many apps hosts in both system, won’t be a burden for the sytem anymore. moblity vehicle interface protocol]]></content>
  </entry>
  <entry>
    <title><![CDATA[can MaaS survive in China]]></title>
    <url>%2F2019%2F12%2F29%2Fcan-MaaS-survive-in-China%2F</url>
    <content type="text"><![CDATA[the difference of US and China: citizenship vs relationshipthe first class cities, e.g. Beijing, Shanghai, Shenzhen, are not different from Chicago, New York, in normal people’s lifestyle: they share the same luxurious brands, Starbucks, Texas beaf Steak, city public services, and the same international popular elements in eletronic consumers, clothing, vehicles, and even the office env. However, down to the third class, or forth class cities in China and US, there are a huge difference. the bottom difference is citizenship（公民意识） vs relationship（关系文化). in US and most developed countries, citizenship is a common sense, no matter in small towns or big cities. in China, the residents in big cities are similar to residents in developed countries; but the normal people in third cities value more about relationship, rather than citizenship, so basically the rules how to live a high-qualitied/successful life in these cities is not universal, which means if a resident from big cities jumping to these small cities, his experince about what is a good career/life choice has to be changed, and further which has a great influence about consuming habit and the acceptance of emerging market. the Chinese goverment is pushing urbanization in most uncitizenlized areas, hopefully this process can be achieved in a few generations, which can be affected by both goverment policy and the external forces, e.g. trade war. any way, there is no short way. Chinese subside marketsin China, one kind of the most profit business is e-trade, e.g. Alibaba, JD, Pinduoduo, e.t.c. they are sinking to the third/forth cities in China in recent years, which is a special phenomenon in China, the reason as I see, is due to the division between citizenship in top class cities and relationship in most small cities in China. for most developed countries, e.g. US, the market is so flat that once one product/service is matured in big cities, there is no additional cost to expand to small towns in national wide. But here in China, the market, the society structure, the resident’s consuming habit are not flat due to the division as mentioned previously. so they need different bussniess strategy for product/service in big cities and most small towns. for the emerging market, the investors and service/product providers need input from top consulting teams, e.g. PWC, Deloitte, BCG, but the research paper from these teams try to ignore the value gap in Chinese large cities and small cities. Of course I can understand the consulting strategy, as emerging market is looking for new services in near future, and it should looks promising. taking mobility as a service (MaaS) as an example, from sharing cars to MaaS is likely happened in urban areas in next 10 years, and expand to most areas in west European and NA countries, but in the most small towns of China, it may never happen. MaaS is a promising service if the society and resident’s value are similar (or plat). for developing countries, e.g. China, India, these emerging market wouldn’t be a great success in national wide. start-ups in MaaS mobiag mobilleo invers maymobility vaimoo in Brazil populus.ai staflsystems polysync.io geotab public resources in moblity as a service(MaaS)Mass alliance International parking &amp; mobility institute shared mobility services in Texas DI_Forces of change-the future of mobility PWC_how shared mobility and automation will reolution Princeton_strategies to Advanced automated and connected vehicles: a primer for state and local decision makers Accenture_mobility as a service whitepaper Bosch_HMI Toyota_Mobility ecosystem Volkswagen_E-mobility module Siemens_Intelligent Traffic Systems MaaS in UK the tech liberation front autonomous vehicle technology]]></content>
  </entry>
  <entry>
    <title><![CDATA[configure hadoop in 2-nodes cluster]]></title>
    <url>%2F2019%2F12%2F28%2Fconfigure-hadoop-in-2-nodes-cluster%2F</url>
    <content type="text"><![CDATA[backgroundit’s by accident that I have to jump into data center, where 4 kinds of data need deal with: sensor verification, with huge amount of special raw sensor data AI perception training, with huge amout of fusioned sensor data synthetic scenarios data, which used to resimualtion inter-middle status log data for Planning and Control(P&amp;C) big data tool is a have-to go through for L3+ ADS team, which has already developed in top start-ups, e.g. WeRide and Pony.AI, as well as top OEMs from NA, Europen. Big data, as I understand is at least as same important to business, as to customers. compared to AI, which is more on customer’s experience. and 2B is a trending for Internet+ diserving into traditional industry. anyway, it’s a good try to get some ideas about big data ecosystem. and here is the first step: hadoop prepare jdk and hadoop in single nodeJava sounds like a Windows langage, there are a few apps requied Java in Ubuntu, e.g. osm browser e.t.c., but I can’t tell the difference between jdk and jre, or openjdk vs Oracle. jdk is a dev toolkit, which includes jre and beyond. so when it’s always better to set JAVA_HOME to jdk folder. jdk in ubuntuthere are many different version of jdk, e.g. 8, 9, 11, 13 e.t.c. here is used jdk-11, which can be download from Oracle website, there are two zip files, the src and the other. the pre-compiled zip is enough to Hadoop in Ubuntu. 1234tar xzvf jdk-11.zip cp -r jdk-11 /usr/local/jdk-11 cd /usr/localln -s jdk-11 jdk append JAVA_HOME=/usr/local/jdk &amp;&amp; PATH=$PATH:$JAVA_HOME/bin to ~/.bashrc, and can run test java -version. what need to be careful here, as the current login user may be not fitted for multi-nodes cluster env, so it’s better to create the hadoop group and hduser, and use hduse as the login user in following steps. create hadoop user1234sudo addgroup hadoopsudo adduser --ingroup hadoop hduser sudo - hduser #login as hduser the other thing about hduser, is not in sudo group, which can be added by: curren login user is hduser: 12345678910111213141516171819202122groups # hadoopsu - # but password doesn't correct#login from the default user terminalsudo -i usermod -aG sudo hduser#backto hduser terminalgroups hduser # : hadoop sudo exit su - hduser #re-login as hduser ``` #### install and configure hadoop hadoop installation at Ubuntu is similar to Java, which has src.zip and pre-build.zip, where I directly download the `pre-build.zip`.another thing need take care is the version of hadoop. since `hadoop 2.x` has no `--daemon` option, which will leads error when master node is with `hadoop 3.x`.```shelltar xzvf hadoop-3.2.1.zip cp -r hadoop-3.2.1 /usr/local/hadoop-3.2.1cd /usr/localln -s hadoop-3.2.1 hadoop add HADOOP_HOME=/usr/local/hadoop and PATH=$PATH:$HADOOP_HOME/bin to ~/.bashrc. test with hadoop version hadoop configure is find here there is another issue with JAVA_HOME not found, which I modify the JAVA_HOME variable in $HADOOP_HOME/etc/hadoop/hadoop_env.sh passwordless access among nodes generate SSH key pair 1234on maste node:ssh-keygen -t rsa -b 4096 -C "master"on worker node:ssh-keygen -t rsa -b 4096 -C "worker" the following two steps need do on both machines, so that the local machine can ssh access both to itself and to the remote. enable SSH access to local machine ssh-copy-id hduser@192.168.0.10 copy public key to the remote node ssh-copy-id hduser@192.168.0.13 tips, if changed the default id_rsa name to sth else, doesn’t work. after the changes above, will generates a known_hosts at local machine, and an authorized_keys, which is the public key of the client ssh, at remote machine. test hadoop on master node 12345hduser@ubuntu:/usr/local/hadoop/sbin$ jps128816 SecondaryNameNode128563 DataNode129156 Jps128367 NameNode on worker node: 123hduser@worker:/usr/local/hadoop/logs$ jps985 Jps831 DataNode and test with mapreduce]]></content>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deploy lgsvl in docker swarm 4]]></title>
    <url>%2F2019%2F12%2F25%2Fdeploy-lgsvl-in-docker-swarm-4%2F</url>
    <content type="text"><![CDATA[backgroundpreviously, tried to deploy lgsvl in docker swarm, which is failde due to the conflict of host network to run lgsvl and the routing mesh of swarm, as I thought. http listen on *why used –network=host, is actually not a have-to, the alternative option is to use &quot;*&quot; as Configure.webHost, instead of localhost nor a special IP address, which lead o HttpListener error: 1The requested address is not vaid in this context. then, we can docker run lgsvl without host network limitations. but still, if run by docker service create, it reports failure: Error initiliazing Gtk+. Gtk/UI in Unitywhen starting lgsvl, it pops the resolution window, which is a plugin of Unity Editor, and implemented with gtk, as explained in last section, which leads to the failure to run lgsvl as service in docker swarm. the simple solution is to disable resolution selection in Unity Editor. 1Build Settings --&gt; Player Settings --&gt; Disable Resolution then the popup window is bypassed. ignore publish portI tried to ignore network host and run directly with routing mesh, but it still doesn’t work. then I remember at the previous blog, when run vkcube or glxgears in docker swarm, it actually does use --network host, so it looks the failure of running lgsvl in docker swarm, is not due to network host, but is due to Gtk/gui. as we can bypass the resolution UI, then directly running as following, works as expected: 1sudo docker service create --name lgsvl --generic-resource &quot;gpu=1&quot; --replicas 2 --env DISPLAY --mount src=&quot;X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; --network host lgsvl add assets into containeranother update is to bind assets from host into lgsvl image, which is stored as sqlite data.db, which is a necessary, as we bypassed the authentication, and the cluster has no access to external Internet. where is nextin recent two month, had digged into docker swarm to run lgsvl. so far, the main pipeline looks work now, and there are still a lot little fix there. to run AV simulation in cloud, is a necessary way to test and verify L3+ AV algorithms/products. previous ADAS test is more on each individual feature itself, e.g. ACC, AEB .e.t.c, all of which are easy to define a benchmark test case, and engineers can easily define the test scenarios systemetically. But for L3+, the env status space is infinite in theory, there is no benchmark test cases any more, and at best we can do is statiscally test cases, which requires a huge number of test cases, which is where virutal simulation test in cloud make sense. from tech viewpoint, the next thing is how to drive the L3+ dev by these simulation tools. and another intesting is the data infrastructure setup]]></content>
      <tags>
        <tag>lgsvl</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web service bypass in lgsvl]]></title>
    <url>%2F2019%2F12%2F15%2Fweb-service-bypass-in-lgsvl%2F</url>
    <content type="text"><![CDATA[backgroundpreviously had talked lg new version code review, where introduced the new server-browser arch, which is focused on the lgsvl server side implementation, which was based on Nancy and sqliteDB; also a simple introduction about reactjs the gap how client send a http request to the server is done by axios. also another issue is how to access asset resource across domain. namely, running the lgsvl service at one host(192.168.0.10), and http request send from another remote host(192.168.0.13). Axiosthe following is an example on how Axios works. 12345678910111213141516171819202122constructor() &#123; this.state = &#123; user: null &#125; &#125;componentDidMount() &#123; axios.get(&apos;https://dog.ceo/api/breeds/image/random&apos;) .then(response =&gt; &#123; console.log(response.data); if(response.status == 200) setState(user, reponse.data) &#125;) .catch(error =&gt; &#123; console.log(error); &#125;);&#125; render() &#123; return ( ) &#125; from React componnet to DOM will call componentDidMount(), inside which axios send a GET request to https://dog.ceo/api/breeds/image/random for a random dog photo. and can also store the response as this component’s state. enactenact is a React project manager, the common usage: 123enact create . # generate project at current dir npm run serve npm run clean enact prject has a configure file, package.json, while can specify the proxy, which is localhost by default. if want to bind to a special IP address, this is the right place to modify. 1234&quot;enact&quot;: &#123; &quot;theme&quot;: &quot;moonstone&quot;, &quot;proxy&quot;: &quot;http://192.168.0.10:5050&quot;&#125;, inside lgsvl/webUI, we need do this proxy configure, to support the across-domain access. Nancy authenticationthis.RequiresAuthentication(), which ensures that an authenticated user is available or it will return HttpStatusCode.Unauthorized. The CurrentUser must not be null and the UserName must not be empty for the user to be considered authenticated. By calling this RequiresAuthentication() method, all requests to this Module must be authenticated. if not authenticated, then the requests will be redirected to http://account.lgsimulator.com. You need to include the types in the Nancy.Security namespace in order for these extension methods to be available from inside your module. this.RequiresAuthentication() is equal to return (this.Context.CurrentUser == null) ? new HtmlResponse(HttpStatusCode.Unauthorized) : null; all modules in lgsvl web server are authenticated by Nancy:RequiresAuthentication(), for test purpose only, we can bypass this function, and pass the account directly: 1234// this.RequiresAuthentication();// return service.List(filter, offset, count, this.Context.CurrentUser.Identity.Name)string currentUsername = &quot;test@abc.com&quot;;return service.List(filter, offset, count, currentUsername) in this way, no matter what’s the account in React client, server always realize the http request is from the user test@abc.com. sqlite dbin Linux, sqlite data.db is stored at ~/.config/unity3d/&lt;company name&gt;/&lt;product name&gt;/data.db in Windows, data.db is stored at C:/users/username/AppData/LocalLow/&lt;company name&gt;/&lt;product name&gt;/data.db it’s interesting when register at lgsvlsimualtor.com, and it actually send the account info back to local db, which give the chance to bypass. debug webUIthe chrome and firefox has react-devtools plugins, which helps, but webUI doesn’t use it directly. to debug webUI it’s even simpler to go to dev mode in browser, and checking the few sections is enough refer bring your data to the front]]></content>
      <tags>
        <tag>lgsvl</tag>
        <tag>react</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deploy lgsvl in docker swarm 3]]></title>
    <url>%2F2019%2F12%2F15%2Fdeploy-lgsvl-in-docker-swarm-3%2F</url>
    <content type="text"><![CDATA[backgroundpreviously tried to run Vulkan in virtual display, which failed as I understand virtual display configure didn’t fit well with Vulkan. so this solution is direct display to allow each node has plugged monitor(which is called PC cluster). for future in cloud support, current solution won’t work. and earlier, also tried to deploy lgsvl in docker swarm, which so far can work with Vulkan as well, after a little bit understand X11 a few demo test can run as followning: deploy glxgears/OpenGL in PC cluster123export DISPLAY=:0 xhost + sudo docker service create --name glx --generic-resource &quot;gpu=1&quot; --constraint &apos;node.role==manager&apos; --env DISPLAY --mount src=&quot;X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; --mount src=&quot;tmp&quot;,dst=&quot;/root/.Xauthority&quot; --network host 192.168.0.10:5000/glxgears deploy vkcube/Vulkan in PC cluster123export DISPLAY=:0 xhost + sudo docker service create --name glx --generic-resource &quot;gpu=1&quot; --constraint &apos;node.role==manager&apos; --env DISPLAY --mount src=&quot;X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; --mount src=&quot;tmp&quot;,dst=&quot;/root/.Xauthority&quot; --network host 192.168.0.10:5000/vkcube deploy service with “node.role==worker”123export DISPLAY=:0xhost + sudo docker service create --name glx --generic-resource &quot;gpu=1&quot; --constraint &apos;node.role==worker&apos; --env DISPLAY --mount src=&quot;tmp&quot;,dst=&quot;/root/.Xauthority&quot; --network host 192.168.0.10:5000/glxgears deploy service in whole swarm123xhost + export DISPLAY=:0 sudo docker service create --name glx --generic-resource &quot;gpu=1&quot; --replicas 2 --env DISPLAY --mount src=&quot;tmp&quot;,dst=&quot;/root/.Xauthority&quot; --network host 192.168.0.10:5000/vkcube which deploy vkcube service in both manager and worker node: overall progress: 2 out of 2 tasks 1/2: running [==================================================&gt;] 2/2: running [==================================================&gt;] verify: Service converged understand .Xauthorityas docker service can run with --mount arguments, which give the first try to copy .Xauthority to manager node, but .X11-unix is not copyable, which is not a normal file, but a socket. in docker service create, when create a OpenGL/vulkan service in one remote worker node, and using $DISPLAY=:0, which means the display happens at the remote worker node. so in this way, the remote worker node is played the Xserver role; and since the vulkan service is actually run in the remote worker node, so the remote worker node is Xclient ? assuming the lower implement of docker swarm serviceis based on ssh, then when the manager node start the service, it will build the ssh tunnel to the remote worker node, and with the $DISPLAY variable as null; even if the docker swarm can start the ssh tunnel with -X, which by default, will use the manager node’s $DISPLAY=localhost:10.0 Xauthority cookie is used to grant access to Xserver, so first make sure which machine is the Xserver, then the Xauthority should be included in that Xserer host machine. a few testes: 1234567ssh in worker: echo $DISPLAY --&gt; localhost:10.0xeyes --&gt; display in master monitor ssh in worker: xauth list --&gt; &#123;worker/unix:0 MIT-MAGIC-COOKIE-1 19282b0a651789ed27950801ef6f1441; worker/unix:10 MIT-MAGIC-COOKIE-1 a6cbe81637207bf0c168b3ad20a9267a &#125;in master: xauth list --&gt; &#123; ubuntu/unix:1 MIT-MAGIC-COOKIE-1 ee227cb9465ac073a072b9d263b4954e; ubuntu/unix:0 MIT-MAGIC-COOKIE-1 75893fb66941792235adba22362c4a6f; ubuntu/unix:10 MIT-MAGIC-COOKIE-1 785f20eb0ade772ceffb24eadeede645 &#125; so which cookie is is for this $DISPLAY ? it shouldb be the one on ubuntu/unix:10; 12ssh in worker: export DISPLAY=:0xeyes --&gt; display in worker monitor then it use the cookie: worker/unix:0. deploy lgsvl service in swarm123xhost + export DISPLAY=:0sudo docker service create --name lgsvl --generic-resource &quot;gpu=1&quot; --replicas 2 --env DISPLAY --mount src=&quot;tmp&quot;,dst=&quot;/root/.Xauthority&quot; --network host --publish published=8080,target=8080 192.168.0.10:5000/lgsvl which gives: overall progress: 0 out of 2 tasks 1/2: container cannot be disconnected from host network or connected to host network 2/2: container cannot be disconnected from host network or connected to host network basically, the service is deployed in ingress network by default, but as well, the service is configured with host network. so it conflict. swarm network the routing mesh is the default internal balancer in swarm network; the other choice is to deploy serivce directly on the node, namely bypassing routing mesh, which ask the service run in global mode and with pubished port setting as mode=host, which should be the same as --network host in replicas mode. the limitation of bypassing routing mesh, is only one task on one node, and access the published port can only require the service from this special node, which doesn’ make sense in cloud env. 12345678910docker service create \ --mode global \ --publish mode=host,target=80,published=8080 \ --generic-resource &quot;gpu=1&quot; \ --env DISPLAY \ --mount src=&quot;tmp&quot;,dst=&quot;/root/.Xauthority&quot; \ --mount src=&quot;X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; \ --network host \ --name lgsvl \ lgsvl:latest tips: –mount src=”X11-unix”,dst=”/tmp/.X11-unix” is kind of cached. so once docker image has ran in worker node, then it doesn’t need to pass this parameter again, but once worker node restart, it need this parameter again in summary about swarm netowrk, the routing mesh should be the right solution for cloud deployment. so how to bypass --network host ? the reason of host network is the lgsvl server and webUI can works in same host; if not, there is kind of cross-domain security failures, which actually is another topic, namely, how to host lgsvl and webUI/React in different hosts. need some study of webUI in next post.]]></content>
      <tags>
        <tag>lgsvl</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[where are you in next 4 years (1)]]></title>
    <url>%2F2019%2F12%2F08%2Fwhere-are-you-in-next-4-years-1%2F</url>
    <content type="text"><![CDATA[it’s more than one year since I started this series “where are you in the next 5 years”, I wound love to transfer to “the next 4 years”, and thanks for the opportunity to get back in China, so there is a high chance to involve in the market heavily in a short time. at the begining of the year, travelled around the whole nation, stay in Shanghai, Beijing, Shenzhen, Guangzhong. and that was a great chance to get familiar with the startups in autonomus vehicle, as this trip really gave me some input, and till now I had another half year in one of the top OEMs in China. combined with this two sources, which gave me the kind of the whole picture of ADS market happening in China. I would love to write this blog more in bussiness thought, rather than engineering way. L4 startupsADS leap time is about 2016 to the first half year of 2018. there are a bunch of startups and also most OEMs have build their ADS teams. the startups, e.g. Pony.ai, WeRide, AutoX, roadstar(the new split), ToSimple, Momenta. which are still very active recently. Today I have taken PlusAI’s tech open day, I have to say, most of these startups have very similar tech roadmap. I personally, think that’s a really sad thing. a few teches they all have: simulation pipeline data pipeline(collect, label, training) AI based perception, motion planning friendly HMI WeRide and Pony.ai are in Robtaxi service; ToSimple and PlusAI are in highway logistics; Momenta is in harbor transformation. Alibaba, jingdong, Meituan e.t.c are in small personal package delivery shuttles, similar as Nuro. OEMs focus in the passenger vehicles. DiDi focus in taxi services as well, similar as Uber, Waymo. all of them can be called as ADS service suppliers. however, most of them use the exactly same sensor packages, including Lidar, Camera, Radar, GPS e.t.c. the software stacks during prodcut dev as mentioned above are mostly similar; there maybe a few special features during the services in deployment, e.g. Robtaxi may have a Uber-like call-taxi app e.t.c, Rather than that, nothing really is amazing about ADS itself. and mostly this is not a tech problem, it’s must be defined or find out by the social guys, who are from the real needs. in the engineering work environment, it’s easily to misunderstand the role of engineering. engineering is the bumper, only when the house need fixed, there is a need for bumper. However, in an engineering-centered env, it’s so easy to tell no difference between I have the bumper and I have the needs. My experieince till now, I am learning how to use the bumper well, but few thinks why need to learn to use the bumper. on the other hand, what kind of tech is really helpful or profitable? by chance, to talk with Unity China team, who are enhancing Unity3D engine with cloud support, unity simualtion, which is the feature I am looking for a while. if the tech pipeline is the waterflow, the Unity team is the one standing at the upper flow, who can implement new features in engine. just like Nvidia, Google e.t.c, these are the guys who really make a difference with their tech. and it’s profitable of course.]]></content>
  </entry>
  <entry>
    <title><![CDATA[where are you in next 4 years (2)]]></title>
    <url>%2F2019%2F12%2F08%2Fwhere-are-you-in-next-4-years-2%2F</url>
    <content type="text"><![CDATA[backgroudjoined the zhongguancun self-driving car workshow today, different level than PlusAI’s tech show at last weekend. there are a few goverment speakers, e.g. zhongguancun tech officer. and sensor suppliers, e.g. SueStar, zhongke hui yan, holo matic, self-driving solution suppliers, e.g. pony.ai; xiantong tech e.t.c, and media press and investors, interesting thing the investors doesn’t look promising. as mentioned last time, the ADS solution suppliers and most sensor startups join the champion, in about 2 years. and goverment are invovled with policy-friendy support. just the captial market or investors don’t buy it at this moment. XianTongXianTong, focused in city road cleaning, which is a special focus, rather than passenger vehicles, pacakge trucks, or small delivery robotics. They have some data in the cleanning serivces in large cities, e.g. beijing, shanghai, hangzhou, and Europen. current cleaner laybor’s harsh and dangerous working env current cleaner laybor’s limitation to working hours, benefits requirements they mentioned the city cleaning market is about 300 billions in China, which looks promising, but how much percent of the cleaning vechiles in this market is not talked. it’s maybe about 20% ~ 60%, as there are a lot human resources, city green plant needs e.t.c, which eats a lot of money, and the current cleaner vehicle products that support ADS maybe has an even smaller part in the whole vehicles used in city cleaning services. so the whole city clearning service market sounds promising, but down to the clearning vehicles, and especailly without a matured and in-market cleaner vehicle product, it’s really difficult to digest and dig golden from the market. I have a feeling, most startups has the similar gap, they do vision big, e.g. to assist the city, the companies, the bussiness, the end customers run/live more efficiently/enjoyable/profitable. but the reality is not that friendly for them, as they spent investor’s money, which expect to get profitable return in a short time. which may push the startups to draw big pictures far beyond their ability, or even far beyond the whole industry’s ability. as well as they draw big pictures, they are very limited to deep into the market, to understand the customers, to design the products with original creativity. creativity or applicationfor investors, these a special industry-application based startups, I think, at most may get investing profit at 1 ~ 4 times. maybe it’s a good idea to understand the successful invest cases happened in last 5 years. And I am afraid that’s also a self-centered option, that most CV happened in high-tech, Internet-based startups. cause the current self-driving market, especially the startups in China, which focus in ADS full-stack solutions, sensors, services providers, are not a game-change player. in history, the first successful company to product PCs, smart phones, Internet searching serivce, social network servie, taxing service, booking (restrount) service, food delivery service, they are game changers. and somehow they are most talked in public, in investers, and most understandable by most knowledge consumers, e.g. engineers. but are they the whole picture of the national economy? what about the local seafood resturant, the local auto repair shops; or the company who product pig’s food, who product customers’ bikes; or the sales company who sell steel to Afria. The economy is more plentiful and complex, than a mind-straight knowledge consumer can image. For a long time, I didn’t realise the local food restarant or a local gym, but they do higher money payback, and definitely higher social status, than a fixed income engineer. so don’t try to find out the secret of each component of the whole economy, and then try to find out the most optimized way to live a life. there is no, or every way to live a life, it is the most optimized way. so the CEOs of these startups, are not crazy to image themselves as the game changers, like Steve Jobs, so they know their energy. surviving firstsince as they know their limitation, so they are not nuts, so they are just enough energy to find a way to survive, even not in a good shape. that’s the other part, as an naive young man, always forgot the fact that surviving is not beautiful most times. the nature has tell the lesson: the young brids only kill his/her brothers/sisters, then he/she can survive to grow into adult. for the deers in Afria plant, each minite is either run to survive or being eaten. companies or human as a single, has the same situation, even the goverment try to make it a little less harsh, but most time to survive is difficult. market sharingas a young man, 1000 billion market sounds like a piece of cake, when comes to the small company, who work so hard to get a million level sale, sounds like a piece of shit. and that’s another naive mind about the reality. like I can’ see the money return from a local seafood resturout, and when I found out it does get more than a million every year, the whole me is shocked. so there is no big or small piece of cake, as it come to survive. most CEOs, they are not nuzy, and they know clearly in their heart, that their company need to survivie and make a profitable income, and that’s enough, to change the world is not most people’s responsibility, but survive it is. however, these CEOs in publich, they talk their company or product as the game-changers, that’s what the investors’ want they to say. so don’t think the market is too small any more, as well as it can support a family to live a life. dream is not the option to most people, that’s the final summary. but survive is a have-to. life is evergreenlife is not only about surviving. if else, human society is an animal world. thinking reasonal always give the doomed impression, and the life in blue; once more perceptual in mind, the day is sunny and joyful. “the theory is grey, but the tree of life is evergreen” develop team in OEMas mentioned, currently there are plenty of system simulation verification special suppliers, e.g. 51 vr, baidu, tencent, alibaba e.t.c, and definitely there softwares are more matured than our team. I am afriad jsut at this moment, the leader teams in OEM don’t realize that to build a simualtion tool espcially to support L3 is mission-impossible. if else, the requirements for simulation, should come from suppliers, rather than build OEM’s own simualtion develop team. I still remember the first day to join this team, sounds like we are really creating something amazing, and nobody else there have more advantages than us. then gradually I realize we can’t customize the Unity engine, we can’t support matured cloud support, we can’t even implement the data pipeline for road test log and analysis. most current team members work stay in requirements level, and in a shadow mode. and actually most of these needs, does have a few external companies/teams have better solution. there does a lot existing issues, from software architecture to ADS algorithms restructure, but these work is mostly not done by OEM develop team. the second-level developing team, can’t make the top-class ADS product. as the new company will split out, this team is less possible to survive in the market, or the leaders have to set up a new developing team. if AI is the direction, that’s another huge blank for this team. I think either go to the ADS software architecture or to AI is a better choice now.]]></content>
  </entry>
  <entry>
    <title><![CDATA[running Vulkan in virtual display]]></title>
    <url>%2F2019%2F12%2F04%2Frunning-Vulkan-in-virtual-display%2F</url>
    <content type="text"><![CDATA[install xserver-xorg-video-dummyapt-cache search xserver-xorg-video-dummy apt-get update sudo apt-get install xserver-xorg-video-dummy which depends on xorg-video-abi-20 and xserver-xorg-core, so need to install xserver-xorg-core first. after update xorg.conf as run Xserver using xserver-xorg-video-dummy driver, and reboot the machine, which leads both keyboard and mouse doesn’t reponse any more. understand xorg.confusually, xorg.conf is not in system any more, so most common use, the xorg will configure the system device by default. if additional device need to configure, can run in root X -configure, which will generate xorg.conf.new file at /root. there are two xorg.conf, one generated by running X -configure, which located at /root/xorg.conf.new ; the other is generated by nvidia-xconfigure, which can be found at /etc/X11/xorg.conf. the following list is from xorg.conf doc ServerLayout section it is at the highest level, they bind together the input and output devices that will be used in a session. input devices are described in InputDevice sections, output devices usualy consist of multiple independent components(GPU, monitor), which are defined in Screen section. each Screen section binds togethere a graphics board(GPU) and a monitor. the GPU are described in Device sections and monitors are described in Monitor sections FILES section used to specify some path names required by the server. e.g. ModulePath, FontPath .. SERVERFLAGS section used to specify global Xorg server options. all should be Options &quot;AutoAddDevices&quot;, enabled by default. MODULE section used to specify which Xorg server (extension) modules shoul be loaded. INPUTDEVICE section Recent X servers employ HAL or udev backends for input device enumeration and input hotplugging. It is usually not necessary to provide InputDevice sections in the xorg.conf if hotplugging is in use (i.e. AutoAddDevices is enabled). If hotplugging is enabled, InputDevice sections using the mouse, kbd and vmmouse driver will be ignored. Identifier and Driver are required in all InputDevice sections. Identifier used to specify the unique name for this input device; Driver used to specify the name of the driver. An InputDevice section is considered active if it is referenced by an active ServerLayout section, if it is referenced by the −keyboard or −pointer command line options, or if it is selected implicitly as the core pointer or keyboard device in the absence of such explicit references. The most commonly used input drivers are evdev(4) on Linux systems, and kbd(4) and mousedrv(4) on other platforms. a few driver-independent Options in InputDevice: CorePointer and CoreKeyboard are the inverse of option Floating, which, when enabled, the input device does not report evens through any master device or control a cursor. the device is only available to clients using X input Extension API. Device section there must be at least one, for the video card(GPU) being used. Identifier and Driver are required in all Device sections. Monitor Sectionthere must be at least one, for the monitor being used. the default configuration will be created when one isn’t specified. Identifier is the only mandatory. Screen SectionThere must be at least one, for the “screen” being used, represents the binding of a graphics device (Device section) and a monitor (Monitor section). A Screen section is considered “active” if it is referenced by an active ServerLayout section or by the −screen command line option. The Identifier and Device entries are mandatory. debug keyboard/mouse not response after X upgrade login to Ubuntu safe mode, by F12 –&gt; Esc (to display GRUB2 menu), then enable network –&gt; root shell run X -configure one line say: List of video drivers: dummy, nvidia, modesetting. uninstall xserver-xorg-video-dummyI thought the dummy video driver is the key reason, so uninstall it, then rerun the lines above, check /var/log/Xorg.0.log: 1234567891011121314151617181920212223[ 386.768] List of video drivers:[ 386.768] nvidia[ 386.768] modesetting[ 386.860] (++) Using config file: &quot;/root/xorg.conf.new&quot;[ 386.860] (==) Using system config directory &quot;/usr/share/X11/xorg.conf.d&quot;[ 386.860] (==) ServerLayout &quot;X.org Configured&quot;[ 386.860] (**) |--&gt;Screen &quot;Screen0&quot; (0)[ 386.860] (**) | |--&gt;Monitor &quot;Monitor0&quot;[ 386.861] (**) | |--&gt;Device &quot;Card0&quot;[ 386.861] (**) | |--&gt;GPUDevice &quot;Card0&quot;[ 386.861] (**) |--&gt;Input Device &quot;Mouse0&quot;[ 386.861] (**) |--&gt;Input Device &quot;Keyboard0&quot;[ 386.861] (==) Automatically adding devices[ 386.861] (==) Automatically enabling devices[ 386.861] (==) Automatically adding GPU devices[ 386.861] (**) ModulePath set to &quot;/usr/lib/xorg/modules&quot;[ 386.861] (WW) Hotplugging is on, devices using drivers &apos;kbd&apos;, &apos;mouse&apos; or &apos;vmmouse&apos; will be disabled.[ 386.861] (WW) Disabling Mouse0[ 386.861] (WW) Disabling Keyboard0Xorg detected mouyourse at device /dev/input/mice.Please check your config if the mouse is still notoperational, as by default Xorg tries to autodetectthe protocol. there is a warning: (WW) Hotplugging is on, devices using drivers &#39;kbd&#39;, &#39;mouse&#39; or &#39;vmmouse&#39; will be disabled disable Hotpluggingfirst generate by X -configure at /root/xorg.conf.new, and copy it to /etc/X11/xorg.conf. then add the additional section in /etc/X11/xorg.conf, , which will disable Hotplugging: 1234Section &quot;ServerFlags&quot;Option &quot;AllowEmptyInput&quot; &quot;True&quot;Option &quot;AutoAddDevices&quot; &quot;False&quot;EndSection however, it reports: 12345678(EE) Failed to load module &quot;evdev&quot; (module does not exist, 0)(EE) NVIDIA(0): Failed to initialize the GLX module; please check in your X(EE) NVIDIA(0): log file that the GLX module has been loaded in your X(EE) NVIDIA(0): server, and that the module is the NVIDIA GLX module. If(EE) NVIDIA(0): you continue to encounter problems, Please try(EE) NVIDIA(0): reinstalling the NVIDIA driver.(EE) Failed to load module &quot;mouse&quot; (module does not exist, 0)(EE) No input driver matching `mouse&apos; switch to nvidia xorg.confwhich reports: 1234(EE) Failed to load module &quot;mouse&quot; (module does not exist, 0)(EE) No input driver matching `mouse&apos;(EE) Failed to load module &quot;evdev&quot; (module does not exist, 0)(EE) No input driver matching `evdev&apos; it fix the Nvidia issue, but still can’t fix the input device and driver issue. switch to evdev driveras mentioned previously, evdev driver is the default driver for Linux, and will be loaded by Xserver by default. so try to both Keyboard and Mouse driver to evdev, which reports: 1234(EE) No input driver matching `kbd&apos;(EE) Failed to load module &quot;kbd&quot; (module does not exist, 0)(EE) No input driver matching `mouse&apos;(EE) Failed to load module &quot;mouse&quot; (module does not exist, 0) looks it’s the problem of driver, even the default driver is missed. I try to copy master node’s /usr/lib/xorg/modules/input/ to worker node, then it reports : 12(EE) module ABI major version (24) doesn&apos;t match the server&apos;s version (22)(EE) Failed to load module &quot;evdev&quot; (module requirement mismatch, 0) which can be fixed by adding Option IgnoreABI . delete customized keyboard and mouseif enable Hotplugging, the X will auto detect the device, I’d try: 1234567891011121314151617181920212223242526272829303132333435Section &quot;ServerLayout&quot; Identifier &quot;Layout0&quot; Screen 0 &quot;Screen0&quot; 0 0 EndSectionSection &quot;Monitor&quot; Identifier &quot;Monitor0&quot; VendorName &quot;Unknown&quot; ModelName &quot;Unknown&quot; HorizSync 28.0 - 33.0 VertRefresh 43.0 - 72.0 Option &quot;DPMS&quot;EndSectionSection &quot;Device&quot; Identifier &quot;Device0&quot; Driver &quot;nvidia&quot; VendorName &quot;NVIDIA Corporation&quot;EndSectionSection &quot;Screen&quot; Identifier &quot;Screen0&quot; Device &quot;Device0&quot; Monitor &quot;Monitor0&quot; DefaultDepth 24 SubSection &quot;Display&quot; Depth 24 EndSubSectionEndSectionSection &quot;ServerFlags&quot; Option &quot;AllowEmptyInput&quot; &quot;True&quot; Option &quot;IgnoreABI&quot; &quot;True&quot;EndSection which reports: 123(II) No input driver specified, ignoring this device.(II) This device may have been added with another device file.(II) config/udev: Adding input device Lenovo Precision USB Mouse (/dev/input/mouse0 there is no ERROR any more, but looks the default Input driver (evdev?) can’t be found out … reinstall xorgMouse and keyboard can be driven by evdev or mouse/keyboard driver respectively. Xorg will load only endev automatically, To use mouse and/or keyboard driver instead of evdev they must be loaded in xorg.conf. There is no need to generate xorg.conf unless you want to fine tune your setup or need to customize keyboard layout or mouse/touchpad functionality. firstly configure new network interface for worker node: configure DHCP network connection setting at /etc/network/interface: 12auto enp0s25 iface enp0s25 inet dhcp ifconfig enp0s25 downifconfig enp0s25 up then reinstall xorg: sudo apt-get update sudo apt-get upgrade sudo apt-get install xserver-xorg-core xserver-xorg xorg which install these libs, xserver-xorg-input-all, xserver-xorg-input-evdev, xserver-xorg-inut-wacom, xserver-xorg-input-vmouse, xserver-xorg-input-synaptics, these are the exact missing parts(input device and drivers). it looks when uninstall video-dummy, these modules are deleted by accident. reboot, both keyboard and mouse work ! “sudo startx” through ssh now the user password doesn’t work in normal login, but when ssh login from another machine, the password verify well. which can be fixed by ssh login from remote host first, then run sudo startx, which will bring the user-password verification back virtual displayxdummy xdummy: xorg.conf run: Xorg -noreset +extension GLX +extension RANDR +extension RENDER -logfile ./10.log -config ./xorg.conf :10 test with glxgears/OpengGL works 1) DISPLAY=localhost:10.0 works 2) DISPLAY=:0 works, but you can’t see it, cause the worker host is in virtual display test with vkcube/Vulkan failed in summary, the vitual display can support OpenGL running, but doesn’t support Vulkan yet. unity simulation cloud SDK is the vendor’s solution, but licensed. refersample xorg.conf for dummy device Keyboard and mouse not responding at reboot after xorg.conf update how to Xconfigure no input drivers loading in X]]></content>
  </entry>
  <entry>
    <title><![CDATA[recent thoughts in ADS]]></title>
    <url>%2F2019%2F11%2F29%2Frecent-thoughts-in-ADS%2F</url>
    <content type="text"><![CDATA[the following are some pieces of ideas during discussion and online resources. system engineering in practicalthe following idea is coming from the expert of system engineering. originally, system engineering or model based design sounds come from aerospace, defense department. the feature of these products: 1) they are the most complex system 2) they are sponsored by the goverment 3) they are unique and no competitors which means they don’t need worry about money and time, so to gurantee the product finally works, they can design from top to down in a long time. the degrade order of requirements level comes as: areospace, defense product &gt;&gt; vehicle level product &gt;&gt; industry level product &gt;&gt; customer level product usually the techs used in the top level is gradually degrading into the next lower level in years. e.g. GPS, Internet, autonomous e.t.c. at the same time, the metholodies from top level go to lower level as well. I suppose that’s why system engineeering design comes to vehicle industry. however, does it really work in this competitional industry? I got the first experince when running scenaio testes in simulation SIL. as the system engineering team define the test cases/scenarios, e.g. 400 test scenarios; on the other hand, the vehicle test team does the road test a few times every week.the result is, most time the 400 test scenarios never catch a system failure; but most road test failure scenario does can be repeated in the simulation env. system engineering based design doesn’t fit well. there are a lot reasons. at first, traditionally the design lifetime of a new vehicle model is about 3~5 years, and startup EV companies recently has even shorter design life cycle, about 1~2 years. so a top-down design at the early time, to cover every aspect of a new model, does almost not make sense. in the V development model, most fresh engineers thought the top-down design is done once for all, the reality is most early stage system engineering desgin need be reconstructured. secondly, system engineering design usually is abstract and high beyond and except engineering considerations, as the system engineers mostly doesn’t have engineering experience in most sections of the sysetem. which results in the system engineering based requirements are not testable, can’t measure during section implementation. there are a few suggestions to set a workable system engineering process: the system engineering team should sit by the develop teams and test teams, they should have a lot of communication, and balance the high-level requirements and also testable, measurable, implementable requirements. basically, system engineering design should have product/component developers as input. both the system engineers and developers should understand the whole V model, including system requirements, component requirements are iteratable. focus on the special requirement, and not always start from the top, each special requirement is like a point, and all these existing points(already finished requirements) will merged to the whole picture finally. take an example, during the road test, there will come a new requirement to have a HMI visulization, then focus on this HMI requirement, cause this requirements may not exist in the top down design. but it is the actual need. system test and verification CI/CDas most OEMs have said they will massive product L3 ADS around 2022, it is the time to jump into the ADS system test and verification. just knew that Nvidia has the full-stack hardware lines: the AI chips in car(e.g. Xavier), the AI training workstation(e.g. DGX), and the ADS system verifcation platform(e.g. Constallation box). data needsthe ADS development depends a lot on data infrastructure: data collect --&gt; data storage --&gt; data analysis there are many small pieces as well, e.g. data cleaning, labeling, training, mining, visulization e.t.c from different dev stage or teams, there are different focus. road test/sensor team, they need a lot of online vehicle status/sensor data check, data logging, visulization(dev HMI), as well as offline data analysis and storage perception team, need a lot of raw image/radar data, used to train, mine, as well as to query and store. planning/control team, need high quality data to test algorithms as well as a good structured in-car computer. HMI team, are focusing on friendly data display fleet operation team, need think about how to transfer data in cloud, vehicle, OEM data centers e.t.c. sooner or later, data pipepline built up is a have to choice. data collection vendorsroad test data collection equipment used in ADS development, is actually not a very big market, compared to in-var computers. but still there are a few vendors already. the top chip OEMs, e.g. Nvidia, Intel has these products. chip poxy, e.g. Inspire traditional vehicle test vendors, e.g. Dspace, Vector, Prescan startups, e.g. horizon Nvidia constellationADS system test usually includes simulation test and road test. and the road test is also called vehicle-in-loop, which is highly expensive and not easy to repeat; then is hardware-in-loop(HIL) test, basically including only the domain controller/ECU in test loop; finally is the is software-in-loop(SIL) test, which is most controllable but also not that reliable. in practical, it’s not easy to build up a closed-loop verification(CI/CD) process from SIL to HIL to road test. and once CI/CD is setted up, the whole team can be turned into data/simulation/test driven. the difficult and hidden part is the supporting toolchain development. Most vehicle test vendors have their special full-stack solution toolchains, but most of them are too eco-customized, it’s really difficult for ADS team, specially OEMs, to follow a speical vendor solution. another reason, test vehicles include components from different vendors, e.g. camera from sony, radar from bosch, Lidar from a Chinese startup, logging equipment from dSpace, and ECUs from Conti. which makes it difficult to fit this mixed system into a Vector verification platform. Nvidia Constallation is trying to meet the gap from SIL, HIL to road test. as it can suppport most customized ECUs. from road test to HIL, it use the exactly same chip. for road test resimulation, Nvidia offer a sim env, and the road test log can feed in directly the ability to do resimulation of road test is a big step, the input is directly scanned images/cloud points, even lgsvl, Carla has no such direct support. but resimulation is really useful for CI/CD. Nvidia constallation as said, is the solution from captured data to KPIs. another big thing is about their high-level scenario description language(HLSDL), which I think is more abstract than OpenScenario. the HLSDL engine use hyper-parameters, SOTIF embedded scenario idea, and optimized scenario generator, which should be massive, random as well as KPI significantly, it should be a good scenario engine if it has these features. Bosch VMSvehicle management system(VMS) is cloud nature framework from Bosch, which is used to meet the similar requirements as Nvidia’s solution, to bring the closed-loop(CI/CD) from road test data collection, data anlaysis to fleet management. they have a few applications based on VMS: fleet batteries management(FBM) for single vehicle’s diaglostic, prediction; and for the EV market, FBM can be used as certification for second-hand EV dealers road coefficient system(RCS) Bosch has both in-vehicle data collection box and cloud server, RCS will be taken as additional sensor for ADS in prodcut VMS in itself Bosch would like to think VMS as the PLM for ADS, from design, test, to deployment. and it shoul be easy to integrate many dev tools, e.g. labeling, simulation e.t.c what about safetyas mentioned previously, 80% of Tesla FSD is to handle AI computing, Nvidia Xavier has about 50% GPU; Mobileye has very limited support for AI. all right, Tesla is most AI aggressive, then Nvidia, then Mobileye is most conserved. which make OEMs take Mobileye solution as more safety, but AI does better performance in perception, so how to balance these two ways? I realized the greats of Mobileye’s new concept: responsibility sensitive safety(RSS), RSS can be used as the ADS safety boundary, but inside either AI or CV make the house power. a lot of AI research on mixed traditional algorithms with AI algorithms, RSS sounds the good solution. would be nice to build a general RSS Mixing AI(RMA) framework.]]></content>
  </entry>
  <entry>
    <title><![CDATA[X11 GUI in docker]]></title>
    <url>%2F2019%2F11%2F29%2FX11-GUI-in-docker%2F</url>
    <content type="text"><![CDATA[XorgXorg is client-server architecture, including Xprotocol, Xclient, Xserver. Linux itself has no graphics interface, all GUI apps in Linux is based on X protcol. Xserver used to manage the Display device, e.g. monitor, Xserver is responsible for displaying, and send the device input(e.g. keyboard click) to Xclient. Xclient, or X app, which includes grahics libs, e.g. OpenGL, Vulkan e.t.c xauthorityXauthority file can be found in each user home directory and is used to store credentials in cookies used by xauth for authentication of X sessions. Once an X session is started, the cookie is used to authenticate connections to that specific display. You can find more info on X authentication and X authority in the xauth man pages (type man xauth in a terminal). if you are not the owner of this file you can’t login since you can’t store your credentials there. when Xorg starts, .Xauthority file is send to Xorg, review this file by xauth -f ~/.Xauthority ubuntu@ubuntu:~$ xauth -f ~/.XauthorityUsing authority file /home/wubantu/.Xauthorityxauth&gt; listubuntu/unix:1 MIT-MAGIC-COOKIE-1 ee227cb9465ac073a072b9d263b4954eubuntu/unix:0 MIT-MAGIC-COOKIE-1 71cdd2303de2ef9cf7abc91714bbb417ubuntu/unix:10 MIT-MAGIC-COOKIE-1 7541848bd4e0ce920277cb0bb2842828 Xserver is the host who will used to display/render graphics, and the other host is Xclient. if Xclient is from remote host, then need configure $DISPLAY in Xserver. To display X11 on remote Xserver, need to copy the .Xauthority from Xserver to Xclient machine, and export $DISPLAY and $XAUTHORITY 12export DISPLAY=&#123;Display number stored in the Xauthority file&#125;export XAUTHORITY=&#123;the file path of .Xauthority&#125; xhostXhost is used to grant access to Xserver (on your local host), by default, the local client can access the local Xserer, but any remote client need get granted first through Xhost. taking an example, when ssh from hostA to hostB, and run glxgears in this ssh shel. for grahics/GPU resources, hostA is used to display, so hostA is the Xserver. x11 forwardingwhen Xserver and Xclient are in the same host machine, nothing big deal. but Xserver, Xclient can totally be on different machines, as well as Xprotocol communication between them. this is how SSH -X helps to run the app in Xclient, and display in Xserver, which needs X11 Forwarding. test benchmark12ssh 192.16.0.13xeyes /tmp/.X11-unixthe X11(xorg) server communicates with client via some kind of reliable stream of bytes. A Unix-domain socket is like the more familiar TCP ones, except that instead of connecting to an address and port, you connect to a path. You use an actual file (a socket file) to connect. srwxrwxrwx 1 root root 0 Nov 26 08:49 X0 the s in front of the permissions, which means its a socket. If you have multiple X servers running, you’ll have more than one file there. is where X server put listening AF_DOMAIN sockets. DISPLAY deviceDISPLAY format: hostname: displaynumber.screennumber hostname is the hostname or hostname IP of Xserver displaynumber starting from 0screennumber starting from 0 when using TCP(x11-unix protocol only works when Xclient and Xserver are in the same machine), displaynumber is the connection port number minus 6000; so if displaynumber is 0, namely the port is 6000. DISPLAY refers to a display device, and all graphics will be displayed on this device.by deafult, Xserver localhost doesn’t listen on TCP port. run: sudo netstat -lnp | grep &quot;6010&quot;, no return. how to configure Xserver listen on TCP 1Add DisallowTCP=false under directive [security] in /etc/gdm3/custom.conf file. Now open file /etc/X11/xinit/xserverrc and change exec /usr/bin/X -nolisten tcp to exec /usr/bin/X11/X -listen tcp. Then restart GDM with command sudo systemctl restart gdm3. To verify the status of listen at port 6000, issue command ss -ta | grep -F 6000. Assume that $DISPLAY value is :0. virtual DISPLAY devicecreating a virtual display/monitoradd fake display when no Monitor is plugged in Xserver broadcastthe idea behind is to stand in one manager(Xserver) machine, and send command to a bunch of worker(Xclient) machines. the default way is all Xclient will talk to Xserver, which eat too much GPU and network bandwith resources on manager node. so it’s better that each worker node will do the display on its own. and if there is no monitor on these worker nodes, they can survive with virtual display. xvfbxvfb is the virtual Xserver solution, but doesn’t run well(need check more) nvidia-xconfigconfigure X server to work headless as well with any monitor connected unity headlessenv setupto test with docker, vulkan, ssh, usually need the following packages: vulkan dev envsudo add-apt-repository ppa:graphics-drivers/ppa sudo apt upgrade apt-get install libvulkan1 vulkan vulkan-utils sudo apt install vulkan-sdk nvidia envinstall nvidia-driver, nvidia-container-runtime install mesa-utils #glxgears docker envinstall docker run glxgear/vkcube/lgsvl in docker through ssh tunnelthere is a very nice blog: Docker x11 client via SSH, disccussed the arguments passing to the following samples run glxgearglxgear is OpenGL benchmark test. 12ssh -X -v abc@192.168.0.13sudo docker run --runtime=nvidia -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v &quot;$HOME/.Xauthority:/root/.Xauthority&quot; --net=host 192.168.0.10:5000/glxgears if seting $DISPLAY=localhost:10.0 , then the gears will display at master node(ubuntu) if setting $DISPLAY=:0, then the gears will display at worker node(worker) and w/o /tmp/.X11-unix it works as well. run vkcubevkcube is Vulkan benchmark test. 123ssh -X -v abc@192.168.0.13export DISPLAY=:0sudo docker run --runtime=nvidia -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v &quot;$HOME/.Xauthority:/root/.Xauthority&quot; --net=host 192.168.0.10:5000/vkcube in this way, vkcube is displayed in worker node(namely, using worker GPU resource), manager node has no burden at all. if $DISPLAY=localhost:10.0, to run vkcube, give errors: No protocol specified Cannot find a compatible Vulkan installable client driver (ICD). Exiting ... looks vulkan has limitation. run lgsvl123export DISPLAY=:0sudo docker run --runtime=nvidia -ti --rm -p 8080:8080 -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v &quot;$HOME/.Xauthority:/root/.Xauthority&quot; --net=host 192.168.0.10:5000/lgsvl /bin/bash ./simulator works well! the good news as if take the manager node as the end user monitor, and all worker nodes in cloud, without display, then this parameters will be used in docker service create to host in the swarm. so the next step is to vitual display for each worker node. referwiki: Xorg/XserverIBM study: Xwindowscnblogs: run GUI in remote serverxorg.conf in ubuntuconfigure XauthorityX11 forwarding of a GUI app running in dockercnblogs: Linux DISPLAY skillsnvidia-runtime-container feature: Vulkan support]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>X11</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[what about Tesla]]></title>
    <url>%2F2019%2F11%2F26%2Fwhat-about-Tesla%2F</url>
    <content type="text"><![CDATA[Tesla is ROCKING the industry. OTA, camera only, fleet learning, shadow mode, Autopilot, Gega Factory, Cybertruck etc. There is a saying: “看不到，看不懂，追不上(can’t see it; can’t understand it; can’t chase it)”. I have to say most Tesla news are more exciting than news from traditional OEMs. and my best wishes to Tesla to grow greater. Tesla timeline2019 release Cybertruck Tesla software V10.0 OTA: Smart Summon(enable vehicle to navigate a parking lot and come to them or their destination of choice, as long as their car is within their line of sight) Driving Visualization(HMI) Automatic lane change lane departure avoidance: Autopilot will warn the driver and slow down the vehicle emergency lane departure avoidance: Autopilot will steer the vehciel back into the driving lane if the off-lane may lead to collision Model 3 safety reward from IIHS and Euro NCAP Tesla Insurance Megapack: battery storage for massive usage V3 super charging station: more powerful station and pre-heating battery Powerpack: energy storage system in South Austrilia Model 3 release (March) and the most customer satisfied vehicles in China cut-off 7% employees globally(Jan) 2018 race mode Model 3 Model 3 the lowest probability of injury by NHTSA: what make Model 3 safe Tesla V9.0 OTA: road status info climate control Navigate on Autopilot Autosteer and Auto lane change combination blindspot warning(when turn signal in engaged but a vehicle or obstacle is detected in the target lane) use high occupancy vehicle(HOV) lane obstacle aware acceleration(if obstacle detected, acc is automatically limited) Dashcam (record and store video footage) Tesla privatization (Aug) 2017 super charing station 10000 globally collabration with Panasonic to produce battery at Buffalo, 1Mpw 2016 purchase SolarCity purchase Grohmann Enginering(German): highly automatic manufacture massive product of Tesla vehicles with hardwares to support fully self driving(Oct) 8 cameras to support 360 view, in front 250 meters env detection 12 Ultrasonic front Radar ADS HAD (x40 powerful than previous) ADS algorithm: deep learning network combine with vision, radar and ultrasonic(AEB, collision warning, lane keeping, ACC is not included yet) Autopilot 8.0 OTA, or namely “see” the world through Radar Tesla’s master plane 2 deadly accident across the truck(2016.7) HEPA defense “biological weapon” accept reservation for Model 3(march) Tesla 7.1.1 OTA: remote summon Tesla 7.1 OTA: vertical parking speed gentelly in living house area highway ACC, traffic jam following more road info in HMI, e.t.c truck, bus, motobike 2015 Autopilot 7.0 update: Autopark(requires driver in the car and only parallel parking) Autosteer Auto lane changing UI refresh Automatic emergency steering side collision warning Autopilot evoluvationin a nutshell, Autopilot is dynamic cruise control(ACC) + Autosteer + auto lane chang. Autopilot 7.0 relied primarily on the front-facing camera. radar hasn’t been used primarily in 7.0 was due to false positives(wrong detection). but in 8.0 with fleet learning. 8.0 made radar the main sensor input. almost entirely eliminate the false positive – the false braking events – and enable the car to initiate braking no matter what the object is as long as it is not large and fluffy but any large, or metallic or dense, the radar system is able to detect and initiate a braking event. both when Autopilot active or not(then AEB) even if the vision system doesn’t recognize the object, it actually doesn’t matter what the object is(while vision does need to know what the thing is), it just knows there is somehting dense. fleeting learning will mark the geolocation of where all the false alarm occurs, and what the shape of that object. so Tesla system know at a particular position at a particular street or highway, if you see a radar object of a following shape - don’t worry it’s just a road sign or bridge or a Christmas decoration. basically marking these locations as a list of exceptions. the radar system can track 2 cars/obstacles ahead and imporove the cut-in , cut-off reponse. so in case the car in front suddenly swerve out of the way of an obstacle. the limit of hardware is reaching, but there will be still a quite improvement as the software and data would improve quite amount. but still perfect safety is really an impossible goal, it’s really about improving the probability of safety. in Autopilot 9.0, Navigate on Autopilot(Beta) intelligently suggests lane changes to keep you on your route in addition to making adjustments so you don’t get stuck behind slow cars or trucks. Navigate on Autopilot will also automatically steer toward and take the correct highway interchanges and exits based on your destination. Autopilot is keeping evaluation with more exicting features: traffic light and stop signs detection enhanced summon naviagte multi-story parking lots automaticly send off vehicle to park Autopilot on city streets Robotaxi service Tesla HardwareHardware 1.0or Autopliot 1 or AP1, it was a joint development between Mobileye and Tesla. It featured a single front-facing camera and radar to sense the environment plus Mobileye’s hardware and software to control the driving experience. AP1 was so good that when Tesla decided to build their own system, it took them years to catch up to the baseline Autopilot functionality in AP1. Mobileye EyeQ3 is good to mark/label in free space, intuitive routing, obstacle-avoid, and traffic signal recognization etc. but it has a few limitations to env light, and reconstruct 3D world from 2D images etc does work as expect all the time. and EyeQ3 detects objects with traditional algorithms, not cool! AP1 Hardware Suite: Front camera (single monochrome) Front radar with range of 525 feet / 160 meters 12 ultrasonic sensors with 16 ft range / 5 meters Rear camera for driver only (not used in Autopilot) Mobileye EyeQ3 computing platform AP1 Core features: Traffic-Aware Cruise Control (TACC), start &amp; stop Autosteer (closed-access roads, like freeways) Auto Lane Change (driver initiated) Auto Park Summon Hardware 2.0AP2 highlights machine learning/neurual networks with camera inputs, so with more sensors and more powerful computing platforms. AP2 Hardware Suite: Front cameras (3 cameras, medium, narrow and wide angle) Side cameras (4 total, 2 forward and 2 rear-facing, on each side) Rear camera (1 rear-facing) Front radar with range of 525 feet / 160 meters 12 ultrasonic sensors with 26 ft range / 8 meters NVIDIA DRIVE PX 2 AI computing platform AP2 Core features: Traffic-Aware Cruise Control (TACC), start &amp; stop Autosteer (closed-access roads, like freeways) Auto Lane Change (driver initiated) Navigate on Autopilot (on-ramp to off-ramp) Auto Park Summon there was AP2.5 update, with redundant NVIDIA DRIVE PX2 and forward radar with longer range (170m) Hardware 3.0or Full Self Driving(FSD) Computer, Telsa guysSterling Anderson from 2015 - 2016, director of Autopilot program. Chris Latter in early 2017, VP for Autopilot software Jim Keller, from 2016 to 2017, VP for Autopilot hardware David Nister, from 2015 to 2017, VP for Autopilot Stuart Bowers from 2018 -2019, VP for Autopilot Pete Bannon, from 2016 to now, Director for Autopilot hardware Andrej Karpathy, from 2017 to now, Director of AI Tesla in mediaTeslaRati Tesla official Telsa motor club Autopilot review zhihu: Tesla Autopilot history 2017 Mercedes-Benz E vs 2017 Tesla Model S Tesla’s Autopilot 8.0: why Elon Mush says perfect safety is still impossible Transcript: Elon Musk’s press conference about Tesla Autopilot under v8.0 update Tesla reveals all the details of its autopilot and its software v7.0 Software update 2018.39 Tesla V10: first look at release notes and features Tesla Autopilot’s stop sign, traffic light recognition and response is operating in shadow mode Tesla’s full self-driving suite with enhanced summon Tesla’s Robotaxi service will be an inevitable player in the AV taxi race Tesla Autopilot AP1 vs AP2 vs AP3 Tesla Hardware 3 Detailed Future Tesla Autopilot update coming soon Autopilot and full self driving capability features multi view Tesla FSD chips zhihu: EyeQ5 vs Xavier vs FSD]]></content>
      <tags>
        <tag>Tesla</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deploy lgsvl in docker swarm-2]]></title>
    <url>%2F2019%2F11%2F21%2Fdeploy-lgsvl-in-docker-swarm-2%2F</url>
    <content type="text"><![CDATA[backgroundpreviously tried to deploy lgsvl by docker compose v3, which at first sounds promising, but due to lack of runtime support, which doesn’t work any way. docker service create --generic-resource is another choice. docker service optionsdocker service support a few common options --workdir is the working directory inside the container --args is used to update the command the service runs --publish &lt;Published-Port&gt;:&lt;Service-Port&gt; --network --mount --mode --env –config docker service create with generic-resourcegeneric-resourcecreate services requesting generic resources is supported well: 1234$ docker service create --name cuda \ --generic-resource &quot;NVIDIA-GPU=2&quot; \ --generic-resource &quot;SSD=1&quot; \ nvidia/cuda tips: acutally the keyword NVIDIA-GPU is not the real tags. generic_resource is also supported in docker compose v3.5: 1234generic_resources: - discrete_resource_spec: kind: &apos;gpu&apos; value: 2 --generic-resource has the ability to access GPU in service, a few blog topics: GPU Orchestration Using Docker access gpus from swarm service first tryfollow accessing GPUs from swarm service. install nvidia-container-runtime and install docker-compose, and run the following script: 12345678910export GPU_ID=`nvidia-smi -a | grep UUID | awk &apos;&#123;print substr($4,0,12)&#125;&apos;`sudo mkdir -p /etc/systemd/system/docker.service.dcat EOF | sudo tee --append /etc/systemd/system/docker.service.d/override.conf[Service]ExecStart=ExecStart=/usr/bin/dockerd -H fdd:// --default-runtime=nvidia --node-generic-resource gpu=$&#123;GPU_ID&#125;EOFsudo sed -i &apos;swarm-resource = &quot;DOCKER_RESOURCE_GPU&quot;&apos; /etc/nvidia-container-runtime/config.tomlsudo systemctl daemon-reloadsudo systemctl start docker to understand supported dockerd options, can check here, then run the test as: docker service create --name vkcc --generic-resource &quot;gpu=0&quot; --constraint &apos;node.role==manager&apos; nvidia/cudagl:9.0-base-ubuntu16.04 docker service create --name vkcc --generic-resource &quot;gpu=0&quot; --env DISPLAY=unix:$DISPLAY --mount src=&quot;X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; --constraint &apos;node.role==manager&apos; vkcube which gives the errors: 1/1: no suitable node (1 node not available for new tasks; insufficient resourc… 1/1: no suitable node (insufficient resources on 2 nodes) if run as, where GPU-9b5113ed is the physical GPU ID in node: docker service create --name vkcc --generic-resource &quot;gpu=GPU-9b5113ed&quot; nvidia/cudagl:9.0-base-ubuntu16.04 which gives the error: invalid generic-resource request `gpu=GPU-9b5113ed`, Named Generic Resources is not supported for service create or update these errors are due to swarm cluster can’t recognized this GPU resource, which is configured in /etc/nvidia-container-runtime/config.toml second tryas mentioined in GPU orchestration using Docker, another change can be done: ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock --default-runtime=nvidia --node-generic-resource gpu=${GPU_ID} which fixes the no suitable node issue, but start container failed: OCI.. 1234root@ubuntu:~# docker service ps vkcc ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSorhcaxyujece vkcc.1 nvidia/cudagl:9.0-base-ubuntu16.04 ubuntu Ready Ready 3 seconds ago e001nd557ka6 \_ vkcc.1 nvidia/cudagl:9.0-base-ubuntu16.04 ubuntu Shutdown Failed 3 seconds ago &quot;starting container failed: OC…&quot; check daemon log with sudo journalctl -fu docker.service, which gives: 1Nov 21 13:07:12 ubuntu dockerd[1372]: time=&quot;2019-11-21T13:07:12.089005034+08:00&quot; level=error msg=&quot;fatal task error&quot; error=&quot;starting container failed: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v1.linux/moby/9eee7ac30a376ee8f59704f7687455bfb163e5ea3dd6d09d24fbd69ca2dfaa4e/log.json: no such file or directory): nvidia-container-runtime did not terminate sucessfully: unknown&quot; module=node/agent/taskmanager node.id=emzw1f9293rwdk97ki7gfqq1q service.id=qdma7vr1g519lz9hx2y1fen9o task.id=ex1l4wy61kvughns5uzo6qgxy third tryfollowing issue #141 123456nvidia-smi -a | grep UUID | awk &apos;&#123;print &quot;--node-generic-resource gpu=&quot;substr($4,0,12)&#125;&apos; | paste -d&apos; &apos; -ssudo systemctl edit docker[Service]ExecStart=ExecStart=/usr/bin/dockerd -H fd:// --default-runtime=nvidia &lt;resource output from the above&gt; and run: docker service create --name vkcc --generic-resource &quot;gpu=1&quot; --env DISPLAY --constraint &apos;node.role==manager&apos; nvidia/cudagl:9.0-base-ubuntu16.04 it works with output verify: Service converged. However, when test image with vucube or lgsvl it has errors: 1Nov 21 19:33:20 ubuntu dockerd[52334]: time=&quot;2019-11-21T19:33:20.467968047+08:00&quot; level=error msg=&quot;fatal task error&quot; error=&quot;task: non-zero exit (1)&quot; module=node/agent/taskmanager node.id=emzw1f9293rwdk97ki7gfqq1q service.id=spahe4h24fecq11ja3sp8t2cn task.id=uo7nk4a3ud201bo9ymmlpxzr3 to debug the non-zero exit (1) : docker service ls #get the dead service-ID docker [service] inspect r14a68p6v1gu # check docker ps -a # find the dead container-ID docker logs ff9a1b5ca0de # check the log of the failure container it gives: Cannot find a compatible Vulkan installable client driver (ICD) check the issue at gitlab/nvidia-images forth trydocker service create --name glx --generic-resource &quot;gpu=1&quot; --constraint &apos;node.role==manager&apos; --env DISPLAY --mount src=&quot;X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; --mount src=&quot;tmp&quot;,dst=&quot;/root/.Xauthority&quot; --network host 192.168.0.10:5000/glxgears BINGO !!!!! it does serve openGL/glxgears in service mode. However, there are a few issues: constraint to manager node require host network the X11-unix and Xauthority are from X11 configuration, which need more study. also network parameter need to expand to ingress overlay mostly, vulkan image still can’t run, with the same error: Cannot find a compatible Vulkan installable client driver (ICD) generic-resource support discussionmoby issue 33439: add support for swarmkit generic resources how to advertise Generic Resources(republish generic resources) how to request Generic Resources nvidia-docker issue 141: support for swarm mode in Docker 1.12 docker issue 5416: Add Generic Resources Generic resources Generic resources are a way to select the kind of nodes your task can land on. In a swarm cluster, nodes can advertise Generic resources as discrete values or as named values such as SSD=3 or GPU=UID1, GPU=UID2. The Generic resources on a service allows you to request for a number of these Generic resources advertised by swarm nodes and have your tasks land on nodes with enough available resources to statisfy your request. If you request Named Generic resource(s), the resources selected are exposed in your container through the use of environment variables. E.g: DOCKER_RESOURCE_GPU=UID1,UID2 You can only set the generic_resources resources’ reservations field. overstack: schedule a container with swarm using GPU memory as a constraint label swarm nodes $ docker node update --label-add &lt;key&gt;=&lt;value&gt; &lt;node-id&gt; compose issue #6691 docker-nvidia issue #141 SwarmKitswarmkit also support GenericResource, please check design doc 1234$ # Single resource$ swarmctl service create --name nginx --image nginx:latest --generic-resources &quot;banana=2&quot;$ # Multiple resources$ swarmctl service create --name nginx --image nginx:latest --generic-resources &quot;banana=2,apple=3&quot; ./bin/swarmctl service create --device /dev/nvidia-uvm --device /dev/nvidiactl --device /dev/nvidia0 --bind /var/lib/nvidia-docker/volumes/nvidia_driver/367.35:/usr/local/nvidia --image nvidia/digits:4.0 --name digits swarmkit add support devices option refermanage swarm service with config UpCloud: how to configure Docker swarm Docker compose v3 to swarm cluster deploy docker compose services to swarm docker deploy doc alexei-led github Docker ARG, ENV, .env – a complete guide]]></content>
      <tags>
        <tag>lgsvl</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deploy lgsvl in docker swarm]]></title>
    <url>%2F2019%2F11%2F19%2Fdeploy-lgsvl-in-docker-swarm%2F</url>
    <content type="text"><![CDATA[Backgroundpreviously, vulkan in docker gives the way to run vulkan based apps in Docker; this post is about how to deploy a GPU-based app in docker swarm. Docker swarm has the ability to deploy apps(service) in scalability. Docker registryDocker Registry is acted as a local Docker Hub, so the nodes in the LAN can share images. update docker daemon with insecure-registries modify /etc/docker/daemon.json in worker node: &quot;insecure-registries&quot;: [&quot;192.168.0.10:5000&quot;] systemctl restart docker start registry service in manager node docker service create –name registry –publish published=5000,target=5000 registry:2 access docker registry on both manager node and worker node : $ curl http://192.168.0.10:5000/v2/ #on manager node $ curl http://192.168.0.10:5000/v2/ #on worker node insecure registry is only for test; for product, it has to with secure connection, check the official doc about deploy a registry server upload images to this local registry hubdocker tag stackdemo 192.168.0.10:5000/stackdemo docker push 192.168.0.10:5000/stackdemo:latest curl http://192.168.0.10:5000/v2/_catalog on worker run: docker pull 192.168.0.10:5000/stackdemo docker run -p 8000:8000 192.168.0.10:5000/stackdemo the purpose of local registry is to build a local docker image file server, to share in the cluster server. Deploy composedocker-compose builddocker-compose build is used to build the images. docker-compose up will run the image, if not exiting, will build the image first. for lgsvl app, the running has a few parameters, so directly run docker-compose up will report no protocol error. run vkcube in docker-composedocker-compose v2 does support runtime=nvidia, by appending the following to /etc/docker/daemon.json: 123456"runtimes": &#123; "nvidia": &#123; "path": "nvidia-container-runtime", "runtimeArgs": [] &#125; &#125; to run vkcube in compose by: xhost +si:localuser:root docker-compose up the docker-compose.yml is : 12345678910111213version: '2.3'services: vkcube-test: runtime: nvidia volumes: - /tmp/.X11-unix:/tmp/.X11-unix environment: - NVIDIA_VISIBLE_DEVICES=0 - DISPLAY# image: nvidia/cuda:9.0-base image: vkcube# build: . however, currently composev3 doesn’t support NVIDIA runtime, who is required to run stack deploy. support v3 compose with nvidia runtimeas discussed at #issue: support for NVIDIA GPUs under docker compose: 123456789services: my_app: deploy: resources: reservations: generic_resources: - discrete_resource_spec: kind: 'gpu' value: 2 update daemon.json with node-generic-resources, an official sample of compose resource can be reviewed. but so far, it only reports error: ERROR: The Compose file &apos;./docker-compose.yml&apos; is invalid because: services.nvidia-smi-test.deploy.resources.reservations value Additional properties are not allowed (&apos;generic_resources&apos; was unexpected` deploy compose_V3 to swarmdocker compose v3 has two run options, if triggered by docker-compose up, it is in standalone mode, will all services in the stack is host in current node; if triggered through docker stack deploy and current node is the manager of the swarm cluster, the services will be hosted in the swarm. btw, docker compose v2 only support standalone mode. take an example from the official doc: deploy a stack to swarm: 123456789docker service create --name registry --publish published=5000,target=5000 registry:2docker-compose up -ddocker-compose psdocker-compose down --volumesdocker-compose push #push to local registrydocker stack deploydocker stack services stackdemodocker stack rm stackdemodocker service rm registry after deploy stackdemo in swarm, check on both manager node and worker node: curl http://192.168.0.13:8000 curl http://192.168.0.10:8000 docker service runtimedocker run can support runtime env through -e in CLI or env-file, but actually docker service doesn’t have runtime env support. docker compose v3 give the possiblity to configure the runtime env and deploy the service to clusters, but so far v3 compose doesn’t support runtime=nvidia, so not helpful. I tried to run vkcube, lgsvl with docker service: docker service create --name vkcc --env NVIDIA_VISIBLE_DEVICES=0 --env DISPLAY=unix:$DISPLAY --mount src=&quot;/.X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; vkcube docker service create --name lgsvl -p 8080:8080 --env NVIDIA_VISIBLE_DEVICES=0 --env DISPLAY=unix$DISPLAY --mount src=&quot;X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; lgsvl for vkcube, the service converged, but no GUI display; for lgsvl, the service failed. Docker deploydocker deploy is used to deploy a complete application stack to the swarm, which accepts the stack application in compose file, docker depoy is in experimental, which can be trigger in /etc/docker/daemon.json, check to enable experimental features a sample from jianshu docker-compose.yml: 1234567891011121314151617181920212223242526272829303132version: "3"services: nginx: image: nginx:alpine ports: - 80:80 deploy: mode: replicated replicas: 4 visualizer: image: dockersamples/visualizer ports: - "9001:8080" volumes: - "/var/run/docker.sock:/var/run/docker.sock" deploy: replicas: 1 placement: constraints: [node.role == manager] portainer: image: portainer/portainer ports: - "9000:9000" volumes: - "/var/run/docker.sock:/var/run/docker.sock" deploy: replicas: 1 placement: constraints: [node.role == manager] a few commands to look into swarm services: 12345docker stack deploy -c docker-compose.yml stack-demo docker stack services stack-demo docker service inspect --pretty stack-demo # inspect service in the swarmdocker service ps &lt;service-id&gt; # check which nodes are running the servicedocker ps #on the special node where the task is running, to see details about the container summaryat this moment, it’s not possible to use v3 compose.yml to support runtime=nvidia, so using v3 compose.yml to depoly a gpu-based service in swarm is blocked. the nature swarm way maybe the right solution. refer run as an insecure registry https configure for docker registry in LAN a docker proxy for your LAN alex: deploy compose(v3) to swarm monitor docker swarm docker swarm visulizer swarm mode with docker service inspect a service on the swarm voting example enable compose for nvidia-docker nvidia-docker-compose compose issue: to support nvidia under Docker compose potential solution for composev3 with runtime swarmkit: generic_resources Docker ARG, ENV, .env – a complete guide]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>lg-sim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vulkan in docker to support new lgsvl]]></title>
    <url>%2F2019%2F11%2F13%2Fvulkan-in-docker-to-support-new-lgsvl%2F</url>
    <content type="text"><![CDATA[backgroundDocker is a great idea to package apps, the first time to try play with docker swarm. lg-sim has updated to HDRP rendering, which has a higher quality, as well requires more GPU features, Vulkan. currently Vulkan is not supported by standard docker neither nvidia-dcker, which is deprecated after docker enginer &gt; 19.03. there is nvidia images, the special one we are interesed is vulkan docker, and there is an related personal project, which is based on the cudagl=10.1, which is not supported by non-Tesla GPU. so for our platform, which has only Quadra P2000 GPUs, the supported CUDA is 9.0, so we need to rebuild the vulkan docker based on CUDA9.0. check the vulkan dockerfile, instead of using cudagl:9.0, change to: FROM nvidia/cudagl:9.0-base-ubuntu16.04 after build the image, we can build the vulkan test samples. if no issue, load lg-sim into this vulkan-docker. a few lines may help: 12345/usr/lib/nvidia-384/libGLX_nvidia.s.0 /usr/share/vulkan/icd.d/proc/driver/nvidia/version new lgsvl in dockerthe previous lg-sim(2019.04) can be easily run in docker, as mentioned here. the above vulkan-docker image is the base to host lgsvl (2019.09). additionaly, adding vulkan_pso_cache.bin to the docker. the benefit of host lgsvl server in docker, is to access the webUI from host or remote. so the network should be configured to run as --net=host. if configure as a swarm overlay network, it should support swarm cluster. a few related issue can be checked at lgsvl issues hub. VOLUME in dockerfilethe following sample is from understand VOLUME instruction in Dockerfile create a Dockerfile as: 123FROM openjdkVOLUME vol1 /vol2CMD [&quot;/bin/bash&quot;] 12docker build -t vol_test . docker run --rm -it vol_test check in the container, vol1, vol2 does both exist in the running container. 123bash-4.2# ls bin dev home lib64 mnt proc run srv tmp var vol2 boot etc lib media opt root sbin sys usr vol1 also check in host terminal: 1234root@ubuntu:~# docker volume ls DRIVER VOLUME NAMElocal 0ffca0474fe0d2bf8911fba9cd6b5875e51abe172f6a4b3eb5fd8b784e59ee76local 7c03d43aaa018a8fb031ef8ed809d30f025478ef6a64aa87b87b224b83901445 and check further: 123root@ubuntu:/var/lib/docker/volumes# ls 0ffca0474fe0d2bf8911fba9cd6b5875e51abe172f6a4b3eb5fd8b784e59ee76 metadata.db7c03d43aaa018a8fb031ef8ed809d30f025478ef6a64aa87b87b224b83901445 once touch ass_file under container /vol1, we can find immediately in host machine at /var/lib/docker/volumes : 1234root@ubuntu:/var/lib/docker/volumes/0ffca0474fe0d2bf8911fba9cd6b5875e51abe172f6a4b3eb5fd8b784e59ee76/_data# ls -lt total 0-rw-r--r-- 1 root root 0 Nov 7 11:40 css_file-rw-r--r-- 1 root root 0 Nov 7 11:40 ass_file also if deleted file from host machine, it equally delete from the runnning container. The _data folder is also referred to as a mount point. Exit out from the container and list the volumes on the host. They are gone. We used the –rm flag when running the container and this option effectively wipes out not just the container on exit, but also the volumes. sync localhost folder to containerby default, Dockerfile can not map to a host path, when trying to bring files in from the host to the container during runtime. namely, The Dockerfile can only specify the destination of the volume. for example, we expect to sync a localshost folder e.g. attach_me to container, by cd /path/to/dockfile &amp;&amp; docker run -v /attache_me -it vol_test. a new data volume named attach_me is, just like the other /vol1, /vol2 located in the container, but this one is totally nothing to do with the localhost folder. while a trick can do the sync: 1docker run -it -v $(pwd)/attach_me:/attach_me vol_test Both sides of the : character expects an absolute path. Left side being an absolute path on the host machine, right side being an absolute path inside the container. volumes in composewhich is only works during compose build, and has nothing to do with docker container. copy folder from host to container COPY in dockerfile ERROR: Service ‘lg-run’ failed to build: COPY failed: stat /var/lib/docker/tmp/docker-builder322528355/home/wubantu/zj/simulator201909/build: no such file or directory the solution is to keep the folder in Dockerfile’s current pwd; if else, Docker engine will look from /var/lib/docker/tmp. VOLUME summaryIf you do not provide a volume in your run command, or compose file, the only option for docker is to create an anonymous volume. This is a local named volume with a long unique id for the name and no other indication for why it was created or what data it contains. If you override the volume, pointing to a named or host volume, your data will go there instead. when VOLUME in DOCKERFILE, it actually has nothing to do with current host path, it actually generate something in host machine, located at /var/lib/docker/volumes/, which is nonreadable and managed by Docker Engine. also don’t forget to use --rm, which will delete the attached volumes in host when the container exit. warning: VOLUME breaks things understand docker-compose.ymlUnderstand and manage Docker container volumes what is vulkan SDK Graham blog]]></content>
      <tags>
        <tag>lgsvl</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[build cluster on Docker swarm]]></title>
    <url>%2F2019%2F11%2F06%2Fbuild-cluster-on-Docker-swarm%2F</url>
    <content type="text"><![CDATA[Docker micro serviceskey concepts in Docker when deploy an application(lg-sim) to swarm cluster as a service, which is defined in a/the manager node, the manager node will dispatch units of work as taskes to worker nodes. when create a service, you specify which container image to use and which commands to execute inside runing containers. In the replicated services, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state. For global services, the swarm runs one task for the service on every available node in the cluster. Docker swarm CLI commands1234567891011121314151617181920docker swarm init docker swarm joindocker service create --name --env --workdir --userdocker service inspect docker service ls docker service rmdocker service scale docker service psdocker service update --argsdocker node inspect docker node update --label-add docker node promote/demote # run in workerdocker swarm leave# run in managerdocker node rm worker-nodedocker ps #get running container IDdocker exec -it containerID /bin/bashdocker run -itdocker-compose up build/run delete unused Docker networkas Docker network may confuse external when access to the local network interfaces, sometimes need to remove the docker networks. 1234567docker network lsdocker network disconnect -f &#123;network&#125; &#123;endpoint-name&#125;docker network rm docker stop $(docker ps -a -q)docker rm $(docker ps -a -q)docker volume prune docker network prune the above scripts will delete the unused(non-active) docker network, then may still active docker related networks, which can be deleted through: 12345678910111213sudo ip link del docker0``` ### access Docker service Docker container has its own virutal IP(172.17.0.1) and port(2347), which allowed to access in the host machine; for externaly access, need to map the hostmachine IP to docker container, by `--publish-add`. the internal communication among docker nodes are configured by `advertise_addr` and `listen-addr`.#### through IP externalyTo publish a service’s ports externally, use the --publish &lt;PUBLISHED-PORT&gt;:&lt;SERVICE-PORT&gt; flag. When a user or process connects to a service, any worker node running a service task may respond.taking example from [5mins series](https://www.cnblogs.com/CloudMan6/tag/Swarm/) docker service create –name web_server –replicas=2 httpddocker service ps web_server access service only on host machine through the Docker IPcurl 172.17.0.1docker service update –publish-add 8080:80 web_server access service externallycurl http://hostmachineIP:80801234567#### configure websocket protocolfor lg-sim server to pythonAPI client, which is communicated through `websocket`, it&apos;s better if the service can be configured to publish through websocket.#### publish httpd server to swarm service docker service create –name web_server –publish 880:80 –replicas=2 httpd1the container IP is IP in network interface `docker0`(e.g. 172.17.0.1), which can be checked through `ifconfig`. `80` is the default port used by httpd, which is mapped to the host machine `880` port. so any of the following will check correctly: curl 172.17.0.1:880curl localhost:880curl 192.168.0.1:880curl 192.168.0.13:880 #the other docker node1234567891011121314151617181920 #### publish lg-sim into swarm service the previous version(2019.04) of lg-sim doesn&apos;t have a http server built-in, since 2019.7, they have `Nancy http server`, which is a great step toward dockerlize the lg-sim server.### manage data in Docker `Volumes` are stored in a part of the host filesystem, which is located `/var/lib/docker/volumes`, which is actually managed by Docker, rather than by host machine.Volumes are the preferred way to [persist data in Docker](https://docs.docker.com/v17.09/engine/admin/volumes/#more-details-about-mount-types) containers and services. some use cases of volume:* once created, even the container stops or removed, the volume still exist.* multiple containes can mount the same voume simultaneously; * when need to store data on a remote host * when need to backup, restore, or migrate data from one Dockr to another#### RexRayan even high-robost way is to separate volume manager and storge provider manager. [Rex-Ray](https://rexray.readthedocs.io/en/v0.3.3/) docker service create –name web_s \ –publish 8080:80 \ –mount “type=volume, volume-driver=rexray, source=web_data, target=/usr/local/apache2/htdocs” \ httpd docker exec -it containerIDls -ld /usr/local/apahce2/htdocschown www-data:www-data test visitcurl http://192.168.0.1:8080docker inspect containerID ``` source reprensents the name of data volume, if null, will create newtarget reprensents data volume will be mounted to /usr/local/apache2/htdocs in each container in RexRay, data volume update, scale, failover(when any node crashed, the data volume won’t lose) also be taken care. refer5mins in Docker Docker swarm in and out what is swarm advertise-addr can ran exec in swarm execute a command within docker swarm service]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux network tool]]></title>
    <url>%2F2019%2F11%2F05%2FLinux-network-tool%2F</url>
    <content type="text"><![CDATA[Linux network commandsipip command is used to edit and display the configuration of network interfaces, routing, and tunnels. On many Linux systems, it replaces the deprecated ifconfig command. 123456789ip link del docker0 # delete a virtual network interface ip addr add 192.168.0.1 dev eth1 #assign IP to a specific interface(eht1)ip addr show #check network interface ip addr del 192.168.0.1 dev eth1 ip link set eth1 up/downip route [show] ip route add 192.168.0.1 via 10.10.20.0 dev eth0 #add static route 192.168.0.1ip route del 192.168.0.1 #remove static routeip route add default via 192.168.0.1 # add default gw netstatenetstate used to display active sockets/ports for each protocol (tcp/ip) 12netstat -lat netstat -us nmclinmcli is a Gnome command tool to control NetworkManager and report network status: 12nmcli device status nmcli connection show = routeroute 1234route ==&gt; ip route (modern version) ##print router route add -net sample-net gw 192.168.0.1 route del -net link-local netmask 255.255.0.0 #delete a virtual network interface ip route flush # flashing routing table tracepathtracepath is used to traces path to a network host discovering MTU along this path. a modern version is traceroute. 1234tracepath 192.168.0.1 ``` ### networking service systemctl restart networking/etc/init.d/networking restart orservice NetworkManager stop123456789## network interface location at `/etc/network/interfaces` `eno1` is onboard Ethernet(wired) adapter. if machines has already `eth1` in its config file, for the second adapter, it will use `eno1` rather than using `eth2`.[ifconfig](https://www.ibm.com/support/knowledgecenter/ssw_aix_71/i_commands/ifconfig.html) is used to set up network interfaces such as Loopback, Ethernet network interface: a software interface to networking hardware, e.g. physical or virtual. physical interface, such as `eth0`, namely Ethernet network card. virtual interface such as `Loopback`, `bridges`, `VLANs` e.t.c. ifconfig -aifconfig eth0 #check specific network interfaceifconfig eth0 192.168.0.1 #assign static IP address to network interfaceifconfig eth0 netmask 255.255.0.0 #assign netmask ifconfig docker0 down/up 1234567891011121314151617181920212223242526replaced by `ip` command later.### why enp4s0f2 instead of eth0[change back to eth0](https://www.itzgeek.com/how-tos/mini-howtos/change-default-network-name-ens33-to-old-eth0-on-ubuntu-16-04.html)```shelllspci | grep -i &quot;net&quot;dmesg | grep -i eth0ip a sudo vi /etc/default/grub GRUB_CMDLINELINUX=&quot;net.ifnames=0 biosdevname=0&quot;update-grub# update /etc/network/interfaces auto eth0iface eth0 inet static sudo reboot Unknown interface enp4s0f2due TO /etc/network/interfaces has auto enp4s0f2 line, which always create this network interface , when restart the networking service. ping hostname with docker0usually there may be have multi network interfaces(eth0, docker0, bridge..) on a remote host, when ping this remote host with exisiting docker network (docker0), by default will talk to the docker0, which may not the desired one. build LAN cluster with office PCs setup PC IPs 1234567891011121314151617181920master node: IP Address: 192.168.0.1 netmask: 24Gateway: nullDNS serer: 10.3.101.101 worker node:IP address: 192.168.0.12netmask: 24Gateway: 192.168.0.1DNS: 10.255.18.3``` * `ufw disable` * update `/etc/hosts` file: 192.168.0.1 master 192.168.0.12 worker if need to update the default hostname to `worker`, can modify `/etc/hostname` file, and reboot. * ping test : ```script ping -c 3 master ping -c 3 worker set ssh access(optionally) sudo apt-get install openssh-server ssh-keygen -t rsa # use custmized key cp rsa_pub.key authorized_key referUbuntu add static route 10 useful IP commands]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里11]]></title>
    <url>%2F2019%2F10%2F29%2F%E6%9C%AA%E6%9D%A55%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C11%2F</url>
    <content type="text"><![CDATA[真实一直有隐藏的一面，而那些窥视过隐藏面的人，注定于大多数人不同，光鲜或者孤独。想想不甘心做韭菜又上升无望的漫漫岁月，简直注定了人生悲剧。 脆弱的感情自以为是个单纯的人，所以喜欢工程师氛围、高校科研氛围、美国的社区文化。回到国内，这些氛围都找不到了。扑面而来的气息，显得很黑暗森林。我怀疑一个单纯的人，在这样的环境下，如何能生存。所以，花尽心思去思考怎么顺应社会。话语中，充满了数字和术语，像一个机器在聊天，而不是一个有感情的人，女生都没办法接近我这样的人。 谈女生总会到一个话题，你稳定了吗。这个”未来5年“系列已经过了一年了，换环境、换公司、换小方向，到底有什么本质的变化，确定了一个方向，确定了一些问题吗？实际上都没有，甚至还没清楚自己要在哪里（国家、城市），做什么行业（汽车、咨询），都是一些自定义的限制。 小城市的行业回来的火车上，遇见了小城的公务员一家去武汉旅游。他们在当地的审计局工作，每年有一些培训在全国各地，比如，南京、武汉。然后一年还有5～10天年薪假。所以一年会有一两次国内的旅行，长一个月，短半个月的。他们的工资在5千 ～ 6千，小城市买了房子，买了10几万的德国/日本车；单位同事，家底好的可以开30-40万的abb。 体系内 学校、医院、法务机关、警察、税务、工商、建设、银行、电力、通信、交通、烟草、农林牧渔等等，庞大的网络可以解决三四线城市大量优质年轻人就业，因为是政府机关，在这些组织里面的年轻人，自然是党国的忠实跟随者。他们拿着高于当地平均值的工资，足够在小城市活的衣食无忧，甚至如上面的小夫妻，有车有房，有一年两次的度假。相比，大城市里租房、加班、年假等于零的奋斗青年，简直是苦逼。 体系外 三四线城市，（政府）体系以外的行业典型： 美容养生、学生教育、餐饮、休闲娱乐（电影、健身、文化项目）、汽修、服装、家具，是属于可以创业的行当。江浙以外，很少有三四线城市有自己的传统支柱产业，比如，农产品加工、能源石化、机械加工、小商品生产等等；更鲜有高附加值支柱产业，诸如，电子器件产业群、芯片产业群、汽车产业群、生物制药产业群、it产业群、精密制造产业群、金融服务业等等。 小城就一家国有钢铁厂，因为北方的能源、钢铁产能过剩，也气数不多。在这些小城市发展高附加值的产业，相当于空中楼阁。所以，体系外的年轻人生存状态其实很空心化。 空心化 “空心化”不是对服务业的贬低，但是相比发达欧美国家的小城市，它们会有一些其貌不扬，但历史百年的世界级的公司和产品。比如，德国狼堡，世界汽车产业中心；美国密西根奥本山小镇，世界级汽车应用软件商聚集地。生活在这些地方的年轻人，当然可以选择进入当地的服务业，诸如快餐连锁、超市服装、甚至房租装修、水电工等等。但是对于做研究、懂技术的年轻人，他们也能找到很容易进入一个创造绝对价值的公司。国内三四线小城市的“空心化”，就是除了传统的服务行业以及政府体系可以吸纳年轻人之外，缺乏生产绝对价值的支柱公司。 倒逼年轻人只能进入传统服务业或者政府机构，能够发挥年轻人创作力的机会太少了。服务业是景上添花，没有高附加值制造业的世界地位，服务业只会被局限在传统的衣食住行。这样一个显著的问题，就是影响了年轻人的价值体系。研究生毕业以后，收入水平、价值实现竟然不如隔壁小学毕业开早点的王老二。那国家投入这么多教育，是为了让这些年轻人去自卑吗。 结局 愚民之策，老百姓根本看不到问题，实际上在短期内，服务业是滞后价值创造产业的。开早点的王老二，还可以一个月赚5万，而且不需要为一个大学毕业生买单。长远点，低端/传统服务业，比如餐厅、修车店、服装店会慢慢饱和，甚至开展恶性竞争。每家新开的餐厅，头两个月可以吸引顾客，之后就被隔壁新开的餐厅带流量了。 一旦大部分传统服务行业进入这个阶段，除了政府直接控制的体系内，老百姓可能就受不了。要么政府强行维稳，全国陷入“中等收入陷阱”。要么社会就会开始乱。 中国有大量的三四线城市，可能都面对“空心化”的问题。大概率进入“中等收入陷阱”。看到问题而又无奈的人，会选择在这场演化之前逃离。国家层面的软实力，不是一天能建立起来的。这恐怕也是中国虽然有着表面的gdp数据，但实际上社会系统相比成熟的发达国家还差很远的地方。如果在演进之前，能慢慢发展出好的系统话。 自动驾驶失业潮有预测，到2030年，l3+自动驾驶的行业渗透可能只在10%。相比普遍的乐观预测，到2020年以后，l3+前装量产。国内近来发了一些很政策导向的产业牌照：全球第一张5G商用牌照， 全球第一张自动驾驶商用牌照。相比，苹果在2020年都不会上5G，美国撤销自动驾驶交通委员会。到底是国内领先，还是国家在画大饼？要保障这么多新生劳动力进入市场，国家也很南啊。 政策可以画行业大饼，但市场比政策更理性。waymo已经被资本市场降低估值，应该就是市场对这个行业的真实回应。按照资本的延后性，大概这个行业的裁员潮会尾随而来。 如果失业了，新职业选择在哪里？变化是永恒的。越是高级的打工仔，受裁员影响越高。在互联网级的快速迭代市场，专业的深度也许不如专业的可移植性对打工仔更合适。 进军服务业看《中国机长》又一次加深的这样的体会。飞机设计、航空发动机、自动驾驶飞控系统是远在普通人的关心之外的；相比，机长、空姐、机场运营人员、服务人员容纳着大量就业，也最直接产生价值。而且他们服务产生的价值，也是经济活力的主要贡献和决定者。相比，虽然国家没有自主的民航发动机、没有自主的cae软件，谁又觉得有什么问题呢。不过是一小群人为此担忧，大部分百姓甚至政策决策者只需要看到中国的航运数据又上升了，航运公司对各地gdp的贡献又大了，就可以士气高涨。 离客户越近，价值越大。在核心技术没有先发优势的市场，做技术只是backup的需求。相比，服务业更靠近广大终端客户人群，价值传递最短、损耗最小、获利最大。所以，空心化后的精英都会去高附加值服务业，比如，金融、互联网。 我们确实不该操政策精英的心，相比，做一个短视的普通人，识时务，进挣钱的行业，就是普通人该干的。 所以，还是要好好准备mba。]]></content>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里 12]]></title>
    <url>%2F2019%2F10%2F29%2F%E6%9C%AA%E6%9D%A55%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C-12%2F</url>
    <content type="text"><![CDATA[国内工作半年了，先后对开源的仿真软件做了定制化实现，对高精地图、ros、场景定义等有了大量快速的接触。同时也搭建了团队的gitlab, jenkins服务, rosbag管理服务、webviz服务，pg地图服务等等，并且也投入了一些时间去搭建数据基础框架，但并没有实际业务需求。在传统oem，用DevOps去规范开发流程，以及探索数据业务、云服务等相对较新颖。相对，新兴的造车企业，会比较好的融合DevOps流程，开发上云。 本质上是开发ADS支撑工具及数据基础设施，但由于OEM内部的ads开发团队比较弱，这些支撑工具以及数据工具并不发挥价值。作为工具开发者，存在感会变弱。因为离核心业务较远，虽然技术很通用。 OEM’s role in emerging techOEM itself mostly doesn’t build tools itself, even like Ford, who hired a great team to develop CAE tools in early days and build them tremendous, still retired most of them or sell them as packages to pure software vendors later. on the other side, the pioneers or leaders of the industry, usually build and apply new methodlogy before the market is there. and after the market is there, they can have the professional team to take in charge. the leaders take the role as incubators, as many Internet startups got the ideas from giants e.g. Google, Facebook e.t.c ADS at this moment is the emerging tech, there is no mature solution or tool chain for ADS, so the leaders of automobile makers and suppliers are the guys who should take the lead to set up standards and tools till ADS market goes to mature. and the giants in automobile should catch this great opportunity to take control in long term. the problems of Chinese OEMs (sadly there is no strong Chinese suppliers yet), they are customed to be followers, when US or German auto makers do the research and make the new bussiness stratagey, Chinese OEMs will follow. as the blog said, Chinese has no friend, neither Chinese goverment. be the followers or imitators is minimal loss strategy for most Chinese companies as well. and this is also why west coutry companies are afriad of the competition from Chinese. Chinese take copy others ideas as a strategy to success, rather than a shame. a tool developer机缘巧合，职业最初，就属于工具支撑组，开发和维护工具。随着，ads的爆发，more and more software skills are required by tradititional automobile makers and suppliers; and they prefer transfer themselves to modern digital driven. but a special tool is never the core product of auto makers. there comes a time, the OEM engineers asked, why you guys build tools, rather than buy from suppliers. The right thing for OEMs is to integrate components, rather than building tools. the cost of building tools by OEM itself is really too much after a while we realized. But at first, OEMs thought it was too costy to buy commecial tools, which gives survival space of a small team to grow inside OEM teams. actually at the early time, small team dreams big, especially in these days, many open source projects can be used freely. the small team almost support every aspect related to software tools, and as well to add features to simulation platform, which is also based on an open source project lg-sim, software manager looks like a good role in the team. but the reality is, to customize any tool, will cost a huge energy for a small team, as we are not involved in these open source projects. more, the manager team doesn’t consider an independent tool team. and even like this, we are moving slowly. infrasturcture for ABCfor ADS startups, they emphasize their infrasturcture features with : cloud platform remote monitor/control system (for vehicle team management) OTA data infrastructure(storage, management, analysis) and in single vehicle features with powerful computing ability, from GTC 2018 speak, most startup use the powerful Xdriver GPU； WeRide Momenta Tusimple AutoX Roadstar.AI (dead) plusAI ponyAI all these looks very Internet, but too far from massive vehicle product. the other mind is these teches are actually common for average engineers, but the chanllenge is to build a team, to realy make it work. survive in OEMit’s maybe more interested to take myself as an ABC(AI, big data, cloud computing) guy to find application scenarios in automobile industry product development. while ABC application scenarios are not matured yet of course, usually it is driven by leader teams, when it comes to drive by market, may be too late. beyond automotive, industry IoT maybe an even general domain to quickly apply ABCs. there should be three steps: familiar with the tech methods understand the hurting points of the industry product and management update -&gt; value return the life leap happens to a few lucky guys. most guys are common and grow in a linear speed. there is a burden the more you know, the harder your life. sadly … digitalizeOEMs are involved more and more in transfering digital, smart. previously most invovled in market, ads strategy, supply chain management; and as connected vehicles, self driving, future mobility service bursting in recent years, digitalization is sinking in vehicle product R&amp;D. AI, big data, cloud, blockchain e.t.c. new techs will be transfering the traditional industry. hopefully we can define the new bussniess together.]]></content>
  </entry>
  <entry>
    <title><![CDATA[lg-sim source code review (2019.09)]]></title>
    <url>%2F2019%2F10%2F22%2Flg-sim-source-code-review-2019-09%2F</url>
    <content type="text"><![CDATA[the recent version of lg-sim 2019.09 has additionally support for webUI, cloud, which is a big enhencement. the following code review is focusing on new components to support WebUI, cluster management. databasedatabase used sqlite project, which is an embedded in-memory db; the ORM used PetaPoco project; there are a few db related serices, e.g. vehicleService, simulationService, which will be used in RESTful web request. DatabaseManager: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152static IDatabaseBuildConfiguration DbConfig ;Init()&#123; DbConfig = GetConfig(connectionString) using db = new SqliteConnection(connectionString)&#123; db.Open()&#125;CreateDefaultDbAssets()&#123; using db = Open() if (info.DownloadEnvs != null)&#123; map = new MapModel()&#123;url = url LocalPath=localPath;&#125; id = db.Insert(map) &#125; if (info.DownloadVehicles != null)&#123;&#125; if(defaultMap.HasValue) &#123; sim1 = new SimulationModel()&#123; Cluster=0, Map=defaultMap.Value, ApiOnly=false&#125;; AddSimulation(db, sim1, noBridgeVehicle); AddSimulation(db, sim, apolloVehicle); db.Insert(simx) &#125;&#125;AddSimulation(IDatabase db, sim,vehicle)&#123; conn = new ConnectionModel()&#123; simulation = id, vehicle = vehicle.Value, &#125; db.Insert(conn)&#125;AddVehicle(db, sim,vehicle)&#123; conn = new VehicleModel()&#123; Url = url , &#125; db.Insert(vehicle) return vehicle.Id;&#125;PendingVehicleDownloads()&#123; using db=Open()&#123; var sql = Sql.Builder.From(&quot;vehicles&quot;).Where(&quot;status=00&quot;, &quot;Downloading&quot;); return db.Query&lt;VehicleModel&gt;(sql); &#125;&#125; database ORMsusing PetaPoco to define ORM models, such asusers, sessions, maps, vehicles, clusters, simulations PetaPocoa few operators in PetaPoco, such as Page, used to query a page; Single, used to query a single item; Insert; Delete; Update database provider123SQLiteDatabaseProvider =&gt; GetFactory(&quot;Mono.Data.Sqlite.SqliteFactory, Mono.Data.Sqlit&quot;) called inside DatabaseManager::Init(). database services VehicleService : IVehicleService MapService NotificationService =&gt; NotificationManager.SendNotification() DownloadService =&gt; DownloadManager.AddDownloadToQueue() ClusterService SessionService SimulationService Web modelweb model is where WebUI server located, which is served by Nancy project, in which a new design metholody is applied: Inversin of Control Contaner(IoC), namely 控制反转 in chinese. IoC is used to inject implementation of class, and manage lifecycle, dependencies. web model is the server where talk to webUI through routes, which is defined in the React project. Nancyused to build HTTP based web service, FileStream StreamResponse TinyIoC UnityBootstrapper : DefaultNancyBootstrapper, used to automatic discovery of modules, custm model binders, dependencies. /Assets/Scripts/Web/Nancy/NancyUnityUtils 12345678910111213141516171819202122232425ConfigureApplicatinContainer(TinyIoCContainer container)&#123; container.Register&lt;UserMapper&gt;(); container.Register&lt;IMapService&gt;(); container.Register&lt;IClusterService&gt;(); container.Register&lt;IVehicleService&gt;(); container.Register&lt;ISimulationService&gt;();&#125; ``` ### route modules ```c#SimulationModule : NancyModule()&#123; class SimulationRequest&#123;&#125; class SimulationResponse&#123;&#125; Get(&quot;/&quot;, x=&#123;&#125;) Post(&quot;/&quot;, x=&#123;&#125;) Put(&quot;/&#123;id:long&#125;&quot;, x=&#123;&#125;) Post(&quot;/&#123;id:long&#125;/start&quot;, x=&#123;&#125;) Post(&quot;/&#123;id:long&#125;/stop&quot;, x=&#123;&#125;) &#125; inside some of the route modules will do query or update the sqlite database for special objects. till here is the complete web back-end server, with a http server and an in-memory database. the webUI frontend is based on React, about React check a previous blog. in the React WebUI project will have http requests corresponding to each of routes defined here. React front end is very independent framework, which should be handled by an independent team in standard pipeline. Config &amp; Loader /Assets/Scripts/Web/Config.cs: used to define WebHost, WebPort, ApiHost, ClourUrl, Headless, Sensors, Bridges e.t.c Loader.cs : 12345678910111213141516171819202122232425262728293031323334353637383940Loader Instance ; //Loader object is never destroyed, even between scene reloadsLoader.Start()&#123; DatabaseManager.Init(); var config = new HostConfguration&#123;&#125; ; // ? Server = new NancyHost(Config.WebHost); Server.Start(); DownloadManager.Init();&#125;Loader.StartAsync(simulation)&#123; Instance.Actions.Enqueue() =&gt; var db = DatabaseManager.Open() AssetBundle mapBundle = null; simulation.Status &quot;Starting&quot; NotificationManager.SendNotification(); Instance.LoaderUI.SetLoaderUIState(); Instance.SimConfig = new SimlationConfig()&#123; Clusters, ApiOnly, Headless, Interative, TimeOfDay, UseTraffic... &#125; &#125;Loader.SetupScene(simulation)&#123; var db = DatabaseManager.Open() foreach var agentConfig in Instance.SimConfig.Agents: var bundlePath = agentConfig.AssetBundle var vehicleBundle = AssetBundle.LoadfromFile(bundlePath) var vehicleAssets = vehicleBundle.GetAllAssetNames(); agentConfig.Prefab = vehicleBundle.LoadAsset&lt;GO&gt;(vehicleAssets[0]) var sim = CreateSimulationManager(); Instance.CurrentSimulation = simulation &#125; DownloadManager class Download(); Init(){ client = new WebClient(); ManageDownloads(); } network modulenetwork module is used for communication among master and clients(workers) in the cluster network for cloud support. the P2P network is built on LiteNetLib which is a reliable UDP lib. LiteNetLibLiteNetLibis Udp package, used to P2P communication among master node and slave nodes here, where are defined as MasterManager, ClientManager. usage of LiteNetLib can be found in the wiki-usage)]]></content>
      <tags>
        <tag>lgsvl</tag>
        <tag>network</tag>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IoC in C#]]></title>
    <url>%2F2019%2F10%2F22%2FIoC-in-C%2F</url>
    <content type="text"><![CDATA[Inversion of Control(IoC) is a design mechanism to decouple components dependencies, a light-weight implementation is: TinyIoC, which is also part of Nancy. IoC idea uses commonly in webUI(and backend server) apps, which is an user friendly solution to cloud deployment management as well as apps in mobile, which should be the right direction in future ADS software tool development. the idea of IoC can be explained by the following example from register and resolve in unity container 12345678910111213141516171819202122232425262728293031323334353637383940public interface ICar&#123; int Run();&#125;public class BMW : ICar&#123; private int _miles = 0; public int Run() &#123; return ++_miles; &#125;&#125;public class Ford : ICar&#123; private int _miles = 0; public int Run() &#123; return ++_miles; &#125;&#125;public class Driver&#123; private ICar _car = null; public Driver(ICar car) &#123; _car = car; &#125; public void RunCar() &#123; Console.WriteLine(&quot;Running &#123;0&#125; - &#123;1&#125; mile &quot;, _car.GetType().Name, _car.Run()); &#125;&#125; the Driver class depends on ICar interface. so when instantiate the Driver class object, need to pass an instance of ICar, e.g. BMW, Ford as following: 123456789101112Driver driver = new Driver(new Ford());driver.RunCar()``` **to use IoC**, taking UnityContainer framework as example, a few other choices: TinyIoC e.t.c.```c# var container = new UnityContainer(); Register create an object of the BMW class and inject it through a constructor whenever you need to inject an ojbect of ICar. 12container.Register&lt;ICar, BMW&gt;(); Resolve Resolve will create an object of the Driver class by automatically creating and njecting a BMW object in it, since previously register BMW type with ICar. Driver drv = container.Resolve&lt;Driver&gt;(); drv.RunCar() summarythere are two obvious advantages with IoC. the instantiate of dependent class can be done in run time, rather during compile. e.g. ICar class doesn’t instantiate in Driver definition automatic new class management in back. the power of IoC will be scaled, once the main app is depends on many little services.]]></content>
      <tags>
        <tag>C#</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rosbag tools in ADS]]></title>
    <url>%2F2019%2F10%2F21%2Frosbag-tools-in-ADS%2F</url>
    <content type="text"><![CDATA[backgroundin ADS, data fusion, sensor performance, L3+ perception, localization algorithms development relys a lot on physicall data collection, commonly in rosbag format with information/data about gps, rtk, camera, Lidar, radar e.t.c. to build up the development process systemly is a critial thing, but also ignored by most ADS teams. for large OEMs, each section may have their own test vehicles, e.g. data fusion team, sensor team e.t.c, but few of them take care data systematically, or build a solution to manage data. one reason is the engineers are lack of ability to give software tool feedbacks/requirements, so they still stay and survive with folders or Excel management, which is definitely not acceptable and scalable for massive product team. thanks for ROS open source community conributing a great rosbag database manage tool: bag_database. with docker installation, this tool is really easy to configure. a few tips: web server IP port in Docker can be accessed from LAN by parameter -p during docker run. mount sbm driveas mentioned, most already collected rosbag is stored in a share drive, one way is to mount these data. 1234sudo mount -t cifs //share_ip_address/ROS_data /home/david/repo/rosbag_manager/data -o uid=david -o gid=david -o credentials=/home/david/repo/rosbag_manager/data/.pwd sudo umount -t cifs /home/david/repo/rosbag_manager/data Tomcat server configurebag_database is hosted by Tomcat, the default port is 8080. For our services, which already host pgAdmin4 for map group; gitlab for team work; xml server for system engineering; for webviz. check the port is occupied or not: 1netstat -an | grep 8088 so configure /usr/loca/tomcat/conf/server.xml: 12345 &lt;Service name="Catalina"&gt; &lt;Connector port="your_special_port" ...&lt;/Service&gt; a few other toolsros_hadoopros_hadoop is a rosbag analysis tool based on hdfs data management. which is a more scalable platform for massive ADS data requirements. if there is massive ros bag process in need, ros_hadoop should be a great tool. there is a discussion in ros wiki install hadoopapache hadoop download single alone install concept in hdfs namenode daemon process, used to manage file system datanode used to data block store and query secondary namenode used to backup hdfs shell command hdfs path URL hdfs-site.xml/usr/local/hadoop/etc/hadoop/hdfs-site.xml configure file dfs.datanode.data.dir -&gt; local file system where to store data blocks on DataNodes dfs.replicaiton -&gt; num of replicated datablocks for protecting data dfs.namenode.https-address -&gt; location for NameNode URL dfs.https.port -&gt; copy local data into hdfs hdfs dfs -put /your/local/file/or/folder [hdfs default data dir] hdfs dfs -ls mongodb_storemongodb_store is a tool to store and analysis ROS systems. also in ros wiki mongo_rosmongo_ros used to store ROS message n MongoDB, with C++ and Python clients. mongo-hadoopmongo-hadoop allows MongoDB to be used as an input source or output destination for Hadoop taskes. ros_pandasrosbag_pandas tabblestabbles used to tag any rosbags or folders. hdfs_fdwhdfs for postSQL at the endtalked with a friend from DiDi software team, most big Internet companies have their own software tool teams in house, which as I know so far, doesn’t exist in any traditional OEMs. is there a need for tool team in OEMs? the common sense is at the early stage, there is no need to develop and maintain in-house tools, the commericial ones should be more efficient; as the department grows bigger and requires more user special development and commericial tools doesn’t meet the needs any more, tool teams may come out. still most decided by the industry culture, OEM’s needs is often pre-defined by big suppliers, so before OEMs are clear their software tool requirements/need, the suppliers already have the solutions there – this situation is epecially true for Chinese OEMs, as their steps is behind Europen suppliers maybe 50 years.\ I am interested at the bussiness model of autovia.ai, which focus on distributed machine learning and sensor data analytics in cloud with petabyte of data, with the following skills: large scale sensor data(rosbag) processing with Apache Spark large scale sensro data(rosbag) training with TensorFlow parallel processing with fast serialization between nodes and clusters hdmap generation tool in cloud metrics and visulization in web loading data directly from hdfs, Amazon S3 all these functions will be a neccessary for a full-stack ADS team in future to development safety products, which I called “ data infrastructure for ADS”. referMooreMike: ros analysis in Jupter mount smb share drive to ubuntu autovia.ai]]></content>
      <tags>
        <tag>ros</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[safety guy in AV]]></title>
    <url>%2F2019%2F10%2F16%2Fsafety-guy-in-AV%2F</url>
    <content type="text"><![CDATA[backgroundwhy Waymo still can’t release fully driveless cars on road; why vehicle startups, NIO is going to brankruptcy? one of the hidden reasons vehicle as a consumer product can’t go to fast iteration as social media, mobility apps in Internet companies, is safety. vehicle development(VD) needs to satisfy safety requiements at first priority. so in VD, the cutting edge tech/ new solutions usually doesn’t make a difference, but the processes, how to handle safety requirements, system requirements, e.t.c are really the first thing in vehicle engineers’ mind. so now, deep learning AI in object detection is fairly matured, but it’s not used common in vehicle camera products. and for these start ups, who say themselves as tech-driven doesn’t survive well, because VD should be process driven. the best managment/survive way for original equipment manufacture(OEM) companies is build up their process system. e.g. GM managemnet is about process, and Toyota agile management is about a more flexible process, too. no OEMs said their team is tech-driven. what is SOTIF ?safety of the intended functionality(SOTIF): the absence of unreasonable risk due to hazards resulting from functional insufficiencies of the intended functionaliyt or by reasonably foreseeable misuse by persons. SOTIF is a way to define safey by design, while still there are unsafe post design, which is beyond the standards can handle. safety chanllenges in ADSis conserative behaivor of AI unsafe ? YES. as Waymo cars driving in San Fransisco, it’s more conservative than human drivers, which makes itself unsafe and surrounding vehicles unsafe. and the definiation of conservative or aggressive of driving behavior is further area depends, e.g. how human drivers drive in San Fransisco is different than drivers from Michigan, how AI handle this? how does AI satisfy VD requirements, and how to verify the AI really satisfy? there is no standard solution yet. since AI is black box, it may satisfy VD requirements in 99.9% situations, but failed in 0.1% case. an ADS solution works well in 99% scenarios is not an acceptable solution at all, which is really big chanllenge for most self driving full-stack solution start-ups. so Waymo and a few giants are working on test driven verification, which requires build up a very strong simulation team to power up safety test by simulation, which’s even chanllenging for most traditional OEMs. and definitely, there is no way to handle unstructured stochastic traffic environments, classification of safety hazards IS26262 internal cyber security (ISO21434) functional insufficient &amp; human misuse (IS02148) external infrastructura cyber security (traffic light system, jamming sensors signals) malfunction behaviors of EEHarzard Analysis &amp; Risk Assignment | | V ASIL B/C/D &lt;-- sys requires * func redundency * system safety mechanism * predictive safety mechanism this is a close-loop analysis, to satisfy sys requirement will get new situations, which lead to new system requirements system requirements ^ | | V SOTIF Analysis this is a double direction design, with any new requiremenst, there is a need to do SOTIF analysis, which may find out some potential bugs/problems in the design, then back to new system requirements. in summary, the input to SOTIF analysis is system requirements, and output is new system requirements. functional insufficients unsafe by design e.g. full pressure brake design, is not sufficient in scenarios, when there is rear following vehicle in full speed Tech limitations safe known unsafe known safe unknown unsafe unknown SOTIF is to identify all unsafe cases and requies to make them safe, but not know how, but SOTIF can’t predict unknown. back to why Waymo doesn’t release their driveless vehicle yet, cause they don’t have a way to prove their ADS system has zero unsafe unknown situations, and most possiblely the current ADS solution may be reached 80% functions of the final completed fully ADS solution, but that’s even not the turning point for ADS system. where to go? rule based vs AI based as neither purely rule based nor purely AI based can achieve safety goals, so there are combined ways, but then there need a manager module to decide which should be weighted more in a special situation. a new undeterministic/unpredicted system explaination reinforcement learning is actually a good way to train agents in unpredicted simulation environemnt, but even in simulation, it can’t travel every case in the state space; then to use R.L to train vehicles in real traffic environment with random pedestrains, npcs, situations, which’s impossible so far. a new AI system current AI is still correlation analysis, while human does causal reasoning. refersafety first for automated driving handover to PR ISO 26262 standard how to reach complete safety requirement refinement for autonomous vehicle]]></content>
  </entry>
  <entry>
    <title><![CDATA[cruise webriz configure]]></title>
    <url>%2F2019%2F10%2F16%2Fcruise-webriz-configure%2F</url>
    <content type="text"><![CDATA[backgroundduring ADS development, ros bag is used a lot to record the sensor, CAN and scenario info, which helps in vehicle test, data fusion, perception.cruise webviz is an open source tool to visualize rosbag and allow user defined layout in web environment, without the pain to run roscore and other ros nodes. while js is not a common tool in my visibility, the way how an js project is organized is really confused at the first time. there are a punch of new things: react [regl]https://www.giacomodebidda.com/how-to-get-started-with-regl-and-webpack/) and js functions/modules are really patches, they can be pached anywhere in the project, so usually it requires a patch manager(e.g. lerna) to deal with versions, dependencies etc; and bigger there are packages, which is independent function component. webviz has a few packages, e.g. regl-worldview, webviz-core, which has more than 40 patches(modules) used. cruise:worldview cruise:webviz core rosbag.js lernalerna is an open source tool to manage js project with multi packages. lerna init will generate lerna.json, which defines the required modules package.json defines useful infomation about the module depencies, CLI(which give a detail about how things work behind) and project description lerna bootstrap --hoist react will install all required modules in the root folder /node_modules, if doesn’t work well, leading to Can&#39;t Resovle module errors, may need manually install some. webpackwebpack is an open source tool for js module bundler. webpack.config.js, defines entry, where the compiler start; output , where the compiler end; module, how to deal with each module; plugin, additionaly content post compiliation; resovle.alias define alias for modules. 1234567891011121314var path=require("path")module.exports =&#123; entry: &#123; app:["./app/main.js"] &#125; output: &#123;path: path.resolve(__dirname, "build")&#125; &#125;npm install webpack-dev-servernpm list | head -n 1 webpack-dev-server --inline --hot the JS bundle could be loaded from a static HTML page, served with any simple web server. That’s exactly what this /packages/webviz-core/public/index.html file is, which is used for https://webviz.io/try. The webpack dev server is configured to serve it at /webpack.config.js npm commands123456npm config list npm install packages@version.minor [-g] [--save]npm uninstall packages npm search packagesnpm cache clean --force webpack-serverwebpack-server is a little nodejs Express server. to install it as a CLI tool first, then run it under the webviz project root folder, with optional parameters, e.g. –host, –port e.t.c webviz configure npm config set registry https://r.npm.taobao.org npm install puppeteer –unsafe-perm=true npm run bootstrap if failed: sudo npm cache clean –force npm install -g lerna npm run bootstrap sudo npm install/rebuild node-sass npm run build if failed, manually installed unresovled modules npm test if failed based on a few test modules, install then npm install webpack-dev-server –save webpack-dev-server.js –host IP –port 8085 #under project root a little hack, by default npm install inter-ui phli’s inter-ui), where insidewebviz/packages/webvix-core/src/styles/fonts.module.scess, it uses: url(&quot;~inter-ui/Inter UI (web)/Inter-UI.var.woff2&quot;) format(&quot;woff2-variations&quot;), modified this line to: url(&quot;~inter-ui/Inter (web)/Inter.var.woff2&quot;) format(&quot;woff2-variations&quot;) refertabbles webviz in ros community access web-server in LAN issue: how to run cruise webviz Ternaris Marv minio: high performance object storage for AI webviz on remote bag visuaize data from a live server npm package install in China taobao.npm]]></content>
      <tags>
        <tag>ros</tag>
        <tag>cruise</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[reinforcement learning in nutshell-2]]></title>
    <url>%2F2019%2F10%2F13%2Freinforcement-learning-in-nutshell-2%2F</url>
    <content type="text"><![CDATA[function approximationwrite the mapping from \ pair to value as $ S X A -&gt; Q $. usually S, Q can be continuous high-dimensional space, and even in discrete state problem. e.g. in GO game, the state space is about 10^170, to store each \ pair in a table is almost impossible. a good and approximated way is using a hash map, where input a state, and through this hash map to get its value, which is one step, rather than many steps even with a tree strucutre of the whole state-action/value space. in math, this is called function approximation, it’s very intuitive idea, to fit a function from the sample points; then any point in the space, it can be represented by the function’s paramters, rather than search in all the sample points, which may be huge; and further as only the function’s parameters is required to store, rather than the whole sample points information, which is really a pleasure. Deep Q-Learningpreviously in Q-Learning, to update Q(S,A): $ Q(S,A) &lt;- \delta ( Q(S, A) + R + \gamma ( Q(S&apos;, A&apos;) - Q(S, A)) to approximate q(s,a) can use a neural network(NN), the benefit of NN is to approximate any function to any precise, similar to any complete orthonormal sequence, e.g. Fourier Series. CNN approximatorthe input to Q neural network is the eigenvector of state S, assuming the action set is finite. the output is action value in state-action-hyperparameter: q(s, a, \theta), Q(S, A, \theta) ~= Q(S, A) as the right most section, here assuming the action set is finite, so for each action, there is an output. experience replayinstead of discarding experiences after one stochastic gradient descent, the agent remembers past expereicnes and learns from them repeatdely, as if the experience has happened again. this allows for greater data efficiency. another reason, as DNN is easily overfitting current episodes, once DNN is overfitted, it’s difficult to produce various experinces. so exprience replay can store experiences including state transitions, rewards, and actions, which are necessary to perform Q learning. at time t, the agent’s experience e_t is defined as: all of agent’s experiences at each time step over all episodes played by the agent are stored in the replay memory. actually, usually the replay memory/buffer set to some finite size limit, namely only store the recent N experiences. and then choose random samples from the replay buffer to train the network. the key reason here is to break the correction between consecutive samples. if the CNN learned from consecutive samples of experience as they occurred sequentially in the environment, the samples are highly correlated, taking random samples from replay buffer. target networkuse a separate network to estimate the target, this target network has the same architecture as the CNN approximator but with frozen parameters. every T steps(a hyperparameter) the parameters from the Q network are copied to this target network, which leads to more stable training because it keeps the target function fixed(for a while) go futher in DQNDouble DQNhere use two networks, the DQN network is for action selection and the target network is for target Q value generation. the problem comes: how to be sure the best action for the next state is the action with the highest Q-value ? Prioritized Replay DQNDueling DQNyet a summaryat the begining of this series, I was thinking to make a simple summary about DQN, which looks promising in self-driving. After almost one month, there are more and more topics and great blogs jumping out, the simple idea to make a summary is not easy any more. I will stop here, and once back. refermath equation editor Petsc Krylov zhihu: function approximate dqnbook: experience replay Toward Data Science explained replay memory going deeper into RL: understanding DQN improvements in DQN]]></content>
      <tags>
        <tag>reinforcement learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[configure pgAdmin4 in server mode]]></title>
    <url>%2F2019%2F10%2F13%2Fconfigure-pgAdmin4-in-server-mode%2F</url>
    <content type="text"><![CDATA[backgroundpgSQL is common used to store hd map in ADS stack, and the client is usually pgAdmin4. in big tech companies, e.g. Tencent, Alibaba, Uber e.t.c, their data infrastructure for ADS stack development is usually based on pgSQL. for our team work, we also store most map info in pgSQL, and for better development toolchain, there is a need to configure the client pgAdmin4 as a web server, rather than request every developer to install a copy of pgAdmin4 at their local side. in LAN web server, apache2 is used to serve a few services, so there is a need to configure multi virutal host in Aapche2, with differnt port and same IP. pgSQLServer official install postgresql start pgSQLserver jump into pgSQL server: 1234/etc/init.d/postgresql start sudo su - postgres pgAdmin4installed by apt-get install pgadmin4, will put pgAdmin4 under local user or root permission, which will be rejected if accessed by remote clients, whose permission is www-data so it’s better to install from src and in a different Python virtual env. setup Python virtual Env install pgAdmin4 from src there maybe a few errors as following: * No package &apos;libffi&apos; found * error: invalid command &apos;bdist_wheel&apos; [sol](https://stackoverflow.com/questions/34819221/why-is-python-setup-py-saying-invalid-command-bdist-wheel-on-travis-ci) * error: command &apos;x86_64-linux-gnu-gcc&apos; failed with exit status 1, [sol](https://stackoverflow.com/questions/21530577/fatal-error-python-h-no-such-file-or-directory) configure pgAdmin4follow this configure pgadmin4 in Server mode configure Apache2follow previous blog to set Apache2 for pgadmin4: &lt;VirtualHost *:8084&gt; ServerName 10.20.181.119:8084 ErrorLog &quot;/var/www/pgadmin4/logs/error.log&quot; WSGIDaemonProcess pgadmin python-home=/home/david/py_venv/pgenv WSGIScriptAlias /pgadmin4 /home/david/py_venv/pgenv/lib/python3.5/site-packages/pgadmin4/pgAdmin4.wsgi &lt;Directory &quot;/home/david/py_venv/pgenv/lib/python3.5/site-packages/pgadmin4/&quot;&gt; WSGIProcessGroup pgadmin WSGIApplicationGroup %{GLOBAL} Require all granted &lt;/Directory&gt; &lt;/VirtualHost&gt; restart Apache2 server will make this works. referwhy sudo - su godaddy about sudo - su configure pgadmin4 in Server mode]]></content>
      <tags>
        <tag>pgAdmin4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[host mod_wsgi in Apache2]]></title>
    <url>%2F2019%2F10%2F13%2Fhost-mod-wsgi-in-Apache2%2F</url>
    <content type="text"><![CDATA[Apache2 BackgroundApache2 virutal host site-available all virtual hosts are configured in individual files with /etc/apache2/sites-available site-enabled until the *.config in site-available are enabled, Apache2 won’t know know. sudo sevice apache2 reload IP based virtual host use IP address of the connection to determine the correct virtual host to serve, so each host needs a separate IP we had name-based vhost in LAN, a few web servers sharing the same IP in the same physical machine, but with different Ports, and even we don’t use DNS server to tell the domains. apache mode coresrefer VirtualHost the title to define this virutal host, and tell which port to listen. by default is port80 ServerName sets the request scheme, hostname, and port that the server uses to identify itself ServerAlias sets the alternate name for a host WSGIDaemonProcess for wsgi app, which usually define in a seperate Python virtual environment, rather than the default localhost user or root. so WSGIDaemonProcess point to the python virtual env. WSGIScriptAlias point to the wsgi_app.wsgi DocumentRoot set the directory from which httpd/apache2 will serve files /var/www/html mod_wsgi install apt-get install apache2 libapache2-mode-wsgi-py3 config in Apache2config with configure file create example.conf under /etc/apache2/conf-available/ 12345678&lt;VirtualHost *:8084&gt; ServerName 10.20.181.119:8084 ServerAlias example.com DocumentRoot &quot;/var/www/html&quot; ErrorLog &quot;/var/www/example/logs/error.log&quot; &lt;/VirtualHost&gt; the served web content is stored at /var/www/html, which can simple include a index.html or a few js. enable configure, which will create corresponding conf under conf-enable folder sudo a2enconf example check configure sudo apachectl -S sudo apachectl configtest config with virtual host create example.conf under /etc/apache2/site-available/12345678&lt;VirtualHost *:8085&gt; ServerName 10.20.181.119:8085 ServerAlias application.com DocumentRoot &quot;/var/www/wsgy_example&quot; ErrorLog &quot;/var/www/wsgy_example/logs/error.log&quot; WSGIScriptAlias /application /var/www/wsgy_app/application.wsgi&lt;/VirtualHost&gt; here used the additional WSGI script, link 123456789101112131415161718import osimport syssys.path.append(&apos;/var/www/wsgy_app/&apos;)os.environ[&apos;PYTHON_EGG_CACHE&apos;] = &apos;/var/www/wsgy_app/.python-egg&apos;def application(environ, start_response): status = &apos;200 OK&apos; output = b&apos;Helo World&apos; response_headers = [(&apos;Content-type&apos;, &apos;text/plain&apos;), (&apos;Content-length&apos;, str(len(output)))] start_response(status, response_headers) return [output] enable configure, which will create corresponding conf under site-enable folder sudo a2ensite wsgy_example check configure sudo apachectl -S sudo apachectl configtest add multi ports:in /etc/apache2/ports.conf: Listen 8083 Listen 8084 since this Apache server host system_engineering web and pgadmin4 web, and share the same IP. restart apachesudo systemctl restart apache2 test apachein browser: 10.20.181.119:8085/application should view “hello world” refername based virtual host Apache2 virutal host vhost with different ports]]></content>
      <tags>
        <tag>apache2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gitlab integrated Jenkins]]></title>
    <url>%2F2019%2F10%2F09%2FGitlab-integrated-Jenkins%2F</url>
    <content type="text"><![CDATA[backgroundafter built the Gitlab server in LAN for two month, managing about 20 projects in ADS group currently. the needs for CI is coming up. at very beginning, I tried Gitlab CI runner, doesn’t work through. so Jenkins! Jenkins installationthere is a great series Maxfields jekins-docker tutorial, since originally I had Docker env and which is a prepartion for cloud deployment in future. Jenkins in Docker installation, start Jenkins in Docker 1docker run --rm -p 8080:8080 -p 50000:50000 -v jenkins-data:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock jenkinsci/blueocean setup wizard also Jenkins installed directly in Ubuntu Jenkins integrated with GitlabJenkins default has integrated with BitBucket and Github, to integrate with Gitlab: set Gitlab token in Gitlab Project main page, go to User Profile &gt;&gt; Settings &gt;&gt; Peronsal Access Token, naming the token and click api domain, the token is generated. install Gitlab Plugins in Jenkins main page: Manage Jenkins &gt;&gt; Manage Plugins &gt;&gt; Search Available Plugins (gitlab). add gitlab key credentials in Jenkins main page, go to Credentials, choose the key type to Gitlab API token, then generate the key credential, used to access Gitlab project. configure Jenkins with gitlab in Jenkins main page, go to Manage Jenkins &gt;&gt; Configure System, at the gitlab section, add the gitlab host url and use the credential created at previous step. since the gitlab server is hosted in LAN, even don’t have DNS for the gitlab server, purely raw IP address. so the gitlab host URL is like: http://10.20.110.110:80 rather than the project git url (e.g. http://10.20.110.110/your_name/your_project.git) Gitlab Hook Plugingitlab events will be post to Jenkins through webhook, which is a common way to notify external serivces, e.g. dingding office chat, JIRA e.t.c. To set it up, need to configure Gitlab Hook plugin in Jenkins. as mentioned in this post, JDK10 or 11 is not supported for Jenkins, if the current OS system has already JDK11, need addtionally install jdk8, and configure the default jdk=8: 123456update-java-alternatives --list sudo update-alternatives --config java java -version Jenkins Projectadd a new Jenkins Item, select FreeStyle, go to Configure. in Source Code Management section, select Git. add Repository URL and set gitlab username and password as Jenkins Credentials. in Build Triggers section, select Build when a change is pushed to Gitlab, which display the Gitlab webhook URL: http://localhost:8080/jenkins/project/demo; go to Adavance section to generate the secret token. when getting all these done, back to Gitlab project, as Admin or Maintainer, in Project Settings &gt;&gt; Integrations &gt;&gt; Add Webhooks.URL and Secret Token are from the previous settings. there is a common issue: Url is blocked: Requests to localhost are not allowed, please refer to allow request to localhost network from system hooks referriot games: putting jenkins in docker jenkins to gitlab authentication devops expert: Emil mind the product: PM’s guide to CD &amp; DevOpsmanage multi version of JDK on Ubuntu a jianshu refer a aliyun refer tencent refer allow request to localhost network from system hooks]]></content>
  </entry>
  <entry>
    <title><![CDATA[warm up Hilber space]]></title>
    <url>%2F2019%2F09%2F29%2Fwarm-up-Hilber-space%2F</url>
    <content type="text"><![CDATA[abstract space Cauchy Series in metric space, existing an any positive and small \etta, exist a natural number N, $m, n &gt; N$, which satisfies: it’s called a Cauchy Series. intuitively, as the numbers goes above, the elements get closer. Complete Space any Caunchy Series in these abstract space, its convergence is still in the original space. e.g. rational number space is not complete space, as is not in rational number space anymore. intuitively, a complete space is like a shell without any holes on its surface. Linear Space with linear structural set, which can be described by base vector, so any hyper-point in a linear space can be represented by the linear combination of its base, so also called as vector space. in another way, linear space has only add and scalar multi operator. Metric Space to describe length or distance in linear space, adding normal in the linear space, gives normed linear space or metric space. Banach Space a complete metric space Inner Product Space with inner product feature in metric space. for the infinite space, there are two different sub space, either the inner product of the series converged or not. the convergence sub space is a completed space. Hilbert Space a completed inner product space Eurepean Space a finite Hilbert Space. functional analysisthe optimzed control theory problem, is to find the functional, under system dynamic constraints, which gives the extremum point(function) in the infinite normed linear space; in general. in general, the functional is depends on the system dynamic path, which gives some famuous theorem: Banach fixed point. PDE theory, partial geometry, optimized control theory are all part of functional analysis. from computational mechanics field, there is a chance go to PDE theory; from modern control field, there is a chance go to optimized control theory; from modern physics field, the students may also get familiar with partial geometry. the beauty of math is show up, all these big concepts finally arrive to one source: functional analysis. to transfer all these big things into numerial world, which gives CAE solver, DQN solvers e.t.c. the closest point property of Hilbert SpaceA subset of A of a vector space is convex: if for all a, b belongs to A, all \lamba such that 0 &lt; \lamba &lt; 1, the point belongs to A assuming A is non-empty closed convex set in Hilbert Space H, for any x belongs to H, there is a unique point y of A which is closer to x than any ohter point of A: orthogonal expansionsif (e_n) is an orthonormal sequence in a Hilbert space H, for any x belongs to H, (x, e_n) is the nth Fourier coefficient of x with respect to (e_n), the Fourier series of x with respect to the sequence (e_n) is the series: pointwise converges where f_nis a sequence of functions in complete orthonormal sequence given an orthonormal sequence of (e_n) and a vector x belongs to H then: but only when this sequence is complete, the right-hand side converge to left-hand side workks. an orthonormal sequence (e_n) in Hilbert Space H is complete if the only member of H which is orthogonal to every e_n is the zero vector. a complete orthonal sequence in H is also called an orthonormal basis of H. H is separable if it contains a complete orthonormal sequence, the orthogonal complement (which named as E^T) of a subset E of H is the set: for any set E in H, E^T is a closed linear subspace of H. the knowledge above gives the convergence in Hilbert Space, application like FEA, once created the orthonormal basis of solution space, the solution function represented by the linear combination of these basis is guranted to convergence. Fourier seriesit is a complete orthonormal sequence in L^2(-\pi, \pi), and any function represented by a linear combination of Fourier basis is converged. but here Fourier basis is the arithmetic means of the nth partial sum of the Fourier series of f, not the direct basis itself. Dual spaceFEA solutions for PDEPDE can be represented in a differential representation: or an integral representation: integral formulation has included existing boundary conditions. by multiplying test function \phi on both side of integral representation, and using Green’s first identity will give the weak formulation(variational formulation) of this PDE. in FEA, assuming the test function \phi and solution T belong to same Hilbert space, the advantage of Hilbert space, is functions inside can do linear combination, like vectors in a vector space. FEA also provides the error estimates, or bounds for the error. the weak formulation should be obtained by all test functions in Hilbert space. this is weak formulation due to now it doesn’t exactly requries all points in the domain need meet the differential representation of the PDE, but in a integral sense. so even a discontinuity of first derivative of solution T still works by weak formulation. Hilbert space and weak convergenceconvergence in another words means, the existence of solution. for CAE, DQN algorithms, the solution space is in Hilbert space. Lagrange and its dualityto transfer a general convex optimization problem with lagrange mulitplier: =&gt; we always looks for meaning solution, so L should exist maximum extremum. if not, as x grow, the system goes divergence, no meaningful solution. define: so the constrainted system equation equals to : to rewrite the original equation as: then the system’s duality is defined as : for a system, if the original state equation and its duality equation both exist extremum solution, then : the benefits of duality problems is the duality problem is always convex, even when the original problem is non-convex the duality solution give an lower boundary for the original solution when strong duality satisfy, the duality solution is the original solution Krylov Spacereferconvex func finite element method Lagranget duality standford convex optimization Going deeper into RF: understaing Q-learning and linear function approximation Nicholas Young, an introduction to Hilbert Space why are Hilbert Space important to finite element analysis]]></content>
  </entry>
  <entry>
    <title><![CDATA[reinforcement learning in nutshell-1]]></title>
    <url>%2F2019%2F09%2F18%2Freinforcement-learning-in-nutshell-1%2F</url>
    <content type="text"><![CDATA[RL conceptsassuming timestep t: the environment state S(t) agent’s action A(t) discouted future reward R(t), which satisfy: with current state `s` and taking current action `a`, the env will give the reward `r_{t+1}`. `\gamma` is the discount ratio, which usually in [0,1], when $\gamma = 0 $, agent&apos;s value only consider current reward. check [discount future reward]() in following chapter. agent’s policy P(a|s), in current state s, the propability of action a agent’s state value V(s), in state s and took policy \pi, which usually described as an expectation: agent’s action value Q(s, a), which consider both state s and action a effects on value calculation. namely, agent’s value is the expectation of its action value with probability distribution p(a|s). state transfer propability P explore rate \eta, it’s the chance to choose non-max value during iteration, similar as mutation. Markov decision processassuming state transfer propability, agent&#39;s policy, and agent&#39;s value follow Markov assumption. namely, the current state transfer propability, agent’s policy and value only relates to current state. agent’s value function V(s) meets Belman’s equation: and agent’s action value function q(s) also has Belman’s equation: in general, env’s state is random; the policy to take action is policy P(a|s). Markov Decision process is: state, action, policy, each [state, action, policy] is an episode, which gives the state-action-reward series: s0, a0, r1, s1, a1, r2, ... s(n-1), a(n-1), rn, sn Markov assumption is state(n+1) is only depends on state(n). Belman’s equationBelman’s equation gives the recurrence relation in two timesteps. the current state value is calcualted from next future status with given current env status. discounted future rewardto achieve better reward in long term, always conside the reward as sum of current and future rewards: R(t) = r(t) + r(t+1) + ... r(n) on the other side, as the env state is random in time, the same action doesn’t give the same reward usually, and as time goes, the difference is even larger. think about the B-tree, as it goes deeper, the number of nodes is larger. so the reward in future doesn’t count the same weight as earlier reward, here define the discount ratio \gamma &lt;- [0, 1] R(t) = r(t) + \gamma r(t+1) + ... + \gamma^(n-t) r(n) R(t) = r(t) = \gamma R(t+1) policy iterationthere are two steps: policy evaluation, with current policy \pi to evaluate state’s value V&#39; policy improvment, with the state value V&#39;, with a special policy update strategy(e.g. greedy) to update policy. the ideal result is to find the fixed point in value space, which corresponds to the optimized policy at current state with current policy update strategy. in policy iteration, we only consider policy-value mapping. value iterationwhere policy iteration based valued is implicit, only value iteration explicit by Belman’s equation. this is also a fixed point application, as we can use the explicit mapping (Belman’s equation) in state value space. so there should guarantee existing an optimized policy. an optimization problemreiforcement learning solution is to find the optimal value function to achieve the max reward in each timestep, which leads to optimal policy \pi*, or max value func, or max action value func. $$ v(s) = max(v_i(s)) foreach i in value funcs $$ $$ q(s, a) = max(q_i(s,a)) foreach i in action value funcs $$ mostly the solution space is not known at hand, if else, the optimal solution can get directly. on the other hand, current state, action set tells a little information about the solution space, as they are part of the solution space, so the optimization algorithms used in convex space can be applied here. a few common optimization algorithms includes: dynamic programming(DP)DP can used in both policy evaluation and policy iteration, but in each calculation step(not even one physical step, and in one physical step, there can be hundreds or thousands of calculation steps) iteration, DP has to recalculate all state(t=current) value to the very first state(t=0) value, which is disaster for high-dimensional problem, and DP by default is kind of integer programming, not fitted to continuous domain either. Monte Carlo (MC)MC gets a few sample solutions in the solution space, and use these samples solution to approxiamte the solution space. in a geometry explaination, the base vectors of the solution space is unknown, but MC finds a few points in this space, then approximate the (nearly) base vectors, then any point in this space can be approximated by the linear combination of the nearly base vectors. MC has no ideas of the propability of state transfer, MC doesn’t care the inner relations of state variables, or model-free. MC is robost as it’s model free, on the other hand, if the solution space is high-dimensional, there is a high chance the sampling points are limited in a lower dimension, which weak the presentation of MC approximation; also MC needs to reach the final status, which may be not avaiable in some applications. time-serial Time Difference(TD)MC use the whole list of status for each policy evaluation; in 1st order TD, policy evaluation only use next reward and next state value: $ v(s) = R(t+1) + \gamma S(t+1)|S $ for n-th order TD, the update will use n-th reward : $ v(s) = R(t+1) + \gamma R(t+2) + ... + \gamma^(n-1) R(t+n) + \gamma^n S(t+n)|S $ it’s clear here, as n-&gt;infinitly, n-th order TD is closer to MC. so how close is enough usually, namely which n is enough ? SARSAin TD, there are two ways: on-line policy, where the same one policy is using to both update value func and upate action; while off-line policy, where one policy is used to update value func, the other policy is used to update action. SARSA is kind of on-line policy, and the policy is e-greedy, to choose the max value corresponding action in every iteration with a high probablitiy (1-e), as e is very small. $ \delta(t) v(S, A) = R + \gamma (v(S&apos;, A&apos;) - v(S, A)) $ in the iteration equation above, \delta(t) will give the iteration timestep size, S&#39;, A&#39; are next state and next action, compare to pure 1-st order TD, where the next state value is modified as: $(Q(S’, A’) - Q(S, A)$, in this way value func keep updated in every func, which can be considered as local correction, compared to the unchanged pure 1-st order TD, which may be highly unstable/diverge in long time series. as time-serial TD can’t guarantee converge, so this local correction makes SARSA numerical robost. SARSA(\lambda) in mutli-step based is same as n-th order TD. $ v(s) = R(t+1) + \gamma R(t+2) + ... + \gamma^(n-1) R(t+n) + \gamma^n ( v(S`) - v(S) ) $ the problem of SARSA is v(S, A) may go huge, as the algorithm is on-line policy, which requires huge memory. Q-learningQ-learning is off-line policy TD. the policy iteration is use e-greedy policy, same as SARSA; while the policy evaluation to update value func use greedy policy. in a word: from status S, using e-greedy policy to choose action A, and get reward R, and in status S&#39;, and using greed policy to get action A&#39;. $ \delta(t) Q(S, A) = R + \gamma (Q(S&apos;, A&apos;) - Q(S, A)) $ (a) where $ A= max v(a|S) $. while in SARSA, both S&#39; and A&#39; update using e-greed. usually Q(S,A) is called Q-valued. the benefit of choosing a different policy to update action, is kind of decouping status and action, so in this way, they can reach more area in real state-action space, which also lead the solution space a little more robost. but still equation (a) is not guaranted to converge as time goes. the converged Q(S,A) should be convex, which means its second-order derivative must be less than 0, then the max Q values (max extremum) achieves when first-order derivate is 0. while Q-learning has the same problem as SARSA, the huge memory to store Q(S,A) table. to make intuitive example, think about an robot walk with 2 choice(e.g. turn left, turn right), and the grid world has 20 box in line, which has 2^20 ~= 10e6 elements in Q(S, A). it can’t scale to any real problem. referSutton &amp; Barto, Reinforcement learning: an introduction Pinard fromeast mathxml editor equation html editor fixed point mechanism]]></content>
      <tags>
        <tag>reinforcement learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python modules in help]]></title>
    <url>%2F2019%2F09%2F18%2Fpython-modules-in-help%2F</url>
    <content type="text"><![CDATA[unittestwhy unit test ?unit test is to test the components of a program automatically. a few good reasons of unit tests: test driving practice early sanity checking for regression test, that the new changes won’t break early work unit test is to test the isolated piece of code, usually white box test written by developer; functional test is to test the function requirements, usually black box test written by testers. the testcase output can be OK, FAIL, ERROR. assertion functions basic boolean asserts: assertEqual(arg1, arg2, msg=None) assertIsNot(arg1, arg2, msg=None) comparative asserts: assertAlmostEqual(first, second, places=7, msg=None, delta=None) asserts for collections: assertListEqual(list1, list2, msg=None) assertTupleEqual(tuple1, tuple2, msg=None) assertSetEqual(set1, set2, msg=None) assertDictEqual(dic1, dic2, msg=None) command line interface run all unittests python3 -m unittest discover -v -c run single test module python3 -m unittest -v -c tests/test_XXXX.py run individual test case python3 -m unittest -v tests.test_XXX.TestCaseXXX.test_XXX how to write a unittestunittest.TestCaseany user defined test class should first derived from unittest.TestCase setUp() &amp; tearDown()setUp() provides the way to set up things before running any method starting with test_xx() in user defined test class. tearDown() is where to end the setting ups depends on other modules/classusually the class/methods we try to test have depends on other modules/class, but as unit test want to isolate these depends, so either mock the other methods or fake data. in general there are three types of depends: pure variablethis is the most simple case, then you can directly define the same pure type variable in your test_xx() method methods in other moduleshere need to use patch, to mock the method and define its return value is your test_xx() method, then in the class where call the real method will be replace by this mocked one. methods in other classhere need to use mock, either MagicMock or Mock will works. mock &amp; patchmock module is built in unittest.mock after a later Python version, if not, mock module can be installed by pip install mock, then directly import mock in code. the philosophy of unit test is to isolate classes test, but in reality most classes have some instances of other classes, so to test the ego class isolated from other classes, that’s where mock helps. 123456789101112131415from mock import MagicMock, patchclass TestScenario(TestCase): def test_mock(self): sim.method = MagicMock(return_value="xx") agent = sim.method() self.assertEqual(agent, "xx") @patch("module/add_agent") def test_patch(self, mock_add_agent): mock_add_agent.return_value = "xx" agent = module.add_agent() self.assertEqual(agent, "xx") Mock helps to mock the method in a class, while patch helps to mock a global method in a module. automatical unittest generatorthere are projects assit to generate unittest code automatically, e.g auger test with while LoopTODO test with raise ErrorTODO coverage.py (one time only) install coverage.py pip3 install –user coverage run all tests with coverage ~/.local/bin/coverage run -m unittest discover generate html report ~/.local/bin/coverage html –omit “~/.local/“,”tests/“ ps, output is in htmlcov/index.html logginglogging() supports more detail message type(info, warn, error, debug) than print(), and the message format is more strict with \%. while print() works both formatted message and message as string simply: 12print("1+1 = ", num)print("1+1 = %d", % num) the following is a general log class, which can plug in any existing Python project: import logging import os class log(object): def __init__(self, logger_name): self.log = logging.getLogger(logger_name) self.log.setLevel(logging.DEBUG) console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) self.log.addHandler(console_handler) def set_output_file(self, filename): file_handler = logging.FileHandler(filename) file_handler.setLevel(logging.INFO) formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s') file_handler.setFormatter(formatter) self.log.addHandler(file_handler) def main(): test = log('scenario-mm') file_name = "record.log" dir_name = "/home/python_test/" try: os.makedirs(dir_name) except OSError: pass file_path = os.path.join(dir_name, file_name) test.set_output_file(file_path) test.log.debug("debug in episode 1...") test.log.info("info ...") test.log.warning("warn ...") test2 = log("zjjj") test2.set_output_file('record.log') test2.log.info("test2 info") test2.log.warning("test2 warn") if __name__ == "__main__": main() pygamereferpython automatic test series auger: automated Unittest generation for Python unittest official unittest in lg-sim project coverage.py official unittest + mock + tox]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[math in ADS system]]></title>
    <url>%2F2019%2F09%2F14%2Fmath-in-ADS-system%2F</url>
    <content type="text"><![CDATA[when dealing with plan and control models in ADS system, there are plenty of basic math, and the understanding of these math are definitely helpful to build a better and stronger algorithm. to design a ADS software system, there are two parts, the engineering background and software development. enginneering side is the core logic, while software is about how to organize the software and present, especially with new ideas, e.g. cloud based architecture, web user interface. vector analysiscoordinate systemasssumed a Cartesian system [O: e1, e2, e3], a vector r can be represented as (x, y, z). vector operator vector add a + b = (a_x + b_x) i + (a_y + b_y) j + (a_z + b_z) k the triangle inequality is: |a + b| &lt;= |a| + |b| multiply scalar a . b = a . b_x i + a . b_y j + a . b_z k namely, by multiplying each direction component of the vector by a. dot product a . b = |a|.|b|. \cos = a_x . b_x + a_y . b_y + a_z . b_z a simple physical explaination: assume a is displacement, b is force, then the power of force in the displacement is by dot product. the geometric meaning of dot product is projection. namely, |a|cos means project of vector a on vector b, which also gives the decomposion of a, which has a unique projection component vector and vertical component vector. a = a_p + a_v a_p = ( a . e_p ) . e_p cross product assume three vectors non-co-plane, (a, b, c), rotating start from a to b, the thumb point to c, which is a right-hand-coordinate system. a simple physical meaning: assume a is the force, b is the force arm, then a x b gives the torque, which perpendicular to plane. a x b = |a||b| \sin the geometric meaning of cross product is the area of parallelogram by . base vectors in right-hand Cartesian coordiante system satisfy: ei x ej = ek (i != j) mixed product $$ a x b \cdot c $$ which gives the volume of parallel hexagonal propped by curve(volume) integration Gauss integration Stokes integration scalar field directional derivative assuming vector l in space, its unitfied vector can be present as directional cosine [cos\alpha, cos\beta, cos\gamma]^T. for any function u=u(x, y, z), who is derivable at M0(x0, y0, z0), then: \frac{\partial u}{\partial l} = dot_product(\Delta u, &lt;\cos \alpha, \cos \beta, \cos \gamme&gt; ) gradient \Delta u = \frac{\partial u}{\parial x} \b{i} + \frac{\partial u}{\parial y} \b{j} + \frac{\partial u}{\parial z} \b{k} the directional derivative which gives the maximum of \Delta u at a point, it’s the gradient direction. the gradient direction in space, stands for the direction from lowest co-value layer to highest co-value layer, which in physical, means the most high rate of change in general. Halmilton operator \Delta = \frac{\partial}{\parial x} \b{i} + \frac{\partial}{\parial y} \b{j} + \frac{\partial}{\parial z} \b{k} vector field directional derivative the gradient field of a vector field gives a tensor field, which raise the dimension one more. the dot product(inner product) of a vector field, decrease to scalar field, the cross product of a vector field keeps a vector field. for a vector field, usually talk about its flux and divergence. analytical geometryplane equationassume plane \pi in Cartesian coordinate system [O: X, Y, Z], O‘s projection in plane is N, the directional cosine of ON is (l, m, n), for any point P in plane, NP is always perpendicular to the directional cosine, namely: dot_product( NP, (l, m, n) ) = 0 as l**2 + m**2 + n**2 == 1, which gives: lx + my + nz - p = 0 (1) equation 1) is the normalized plane equation. and (l, m, n) is the normal vector of lane \pi, and p should be no lesss than 0. linear equationassumed a line go across point P_0 $(x_0, y_0, z_0)$ and in direction \lamba, then any point P $(x,y,z)$ on this line, satisfy: x - x_0 = |PP_0| . l y - y_0 = |PP_0| . m z - z_0 = |PP_0| . n taking |PP_0| as t, the equation above is the parmeterized line equation, namely: \frac{x-x0}{l} = \frac{y-y0}{m} = \frac{z-z0}{n} in general, a linear is the cross interface of two linear plane, so its genearl equation is to satisfy both plane equation: A1.x + B1.y + C1.z + P1 = 0 A2.x + B2.y + C2.z + P2 = 0 coordiante transferin general, coordiante transfer means base vector linear transfer. assuming original coordinate system [O: e1, e2, e3], and new coordinate system [O’, e1’, e2’, e3’] both the original base vector and new base vector &lt;e1’, e2’, e3’&gt; prop up the same 3D linear space. and there should be transfer matrix between them: e = matrix[3x3] \cdot e’ for any point P transfer from original to new coordinate system, {x} = {a} + {x’} . matrix[3x3] matrix[3x3] is: a11 a21 a31 a12 a22 a32 a13 a23 a33 each raw in the matix above, stands for a base vector tranfer from original one to the new one, namely: $$ e1&apos; = a11.e1 + a21.e2 + a31.e3 $$ the component value of each coordinate, can be simply by: x1’ = e1’ . OP’ referwrite mathematic fomulars in Markdown]]></content>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里 10]]></title>
    <url>%2F2019%2F09%2F14%2F%E6%9C%AA%E6%9D%A55%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C-10%2F</url>
    <content type="text"><![CDATA[个人与公司的生存和发展法则。因为企业生存状态以及老板的眼光，国内企业会有一种氛围：老板花钱是让来工作的，来学习都不该提倡。个人而言，学习才能保持议价权。也需要有个良好的心态，面对这样的企业现状。 国内就业大环境也长期不看好。本来优质的大公司和岗位不多，相对缺乏壁垒的，最近的一个新闻： 一汽大众不招传统专业，比如，车辆、机械。不过话说回来，一汽大众的工作在长春也是绝对高薪。不招人背后，是技术岗位都在德国，那国内的年轻人在这样的企业怎么积累？ 当然，即使在这样的企业内，还会分三六九等。年轻人被局限的穷迫程度可想而知，这样缺乏格局的环境，也没办法培养年轻人的领导力。 上一篇 提到北欧国家。国家大环境，确实对年轻人的选择有太大影响。国内年轻人如果能选择行业，确实应该避开传统制造业、农业等低增长周期的企业。当然，互联网、金融、地产、消费服务等快增长周期的行业，算适应国家的大环境，人也不轻松。 国与家选择一个国家，就是选择了一套制度。在中秋夜，每有悠闲的时候，心里反而更沉，似乎只有加班工作，可以抵消内心深处的不安。 回来看了很多关于中国发展的报道《中国制造2025》以及生活现状：住在筒子楼里的爱情，缺乏社会秩序和行业规则的生活和工作环境，慢慢溢出。 回国前，对国内的生活的1万种可能，都不包括现实的样子。 小镇青年的生活，完全是另一个折叠。我以为的真实生活到底只是我愿意认知的真实吧了，包括对国与家的态度。有时候觉得，是不是自己跟周围的人太格格不入了，可惜看到《计算机应用》学报上的文章，真的很无奈，大部分年轻人在这样的状态下追求的是什么价值？ 商业的逻辑小城唯一的一家沃尔玛，进去看到商品的丰富，真的是感动了一下。想当年在美国，出入walmart, whole food, wegments, kroger, 虽然也感谢生活的便利舒适和优雅，不过没有这么渴望。 在国内生活，思路也发生了转变。进沃尔玛看到陈列的各式衣服、首饰，第一反应，竟然是如何成为沃尔玛的供货商。这个平台大到足够小富即安，年入百万。这样的思路，放在美国生活的时候，估计有点丧心病狂。 也可能国内的环境慢慢教人认识到：商人和企业家是不同的吧。 记得之前评价恒大造车，说他是对行业的玷污，那不过是把恒大想成了汽车行业的企业家，需要对这一行有超出商业范畴的责任。不过人家本质上是一名商人。 读《中国制造2025》另外一个感受，就是制造业升华的制约，已经不在制造业本身，而是体制。企业家、商人做了他们该做的，甚至在中国这样行业秩序、商业秩序、社会秩序相对匮乏的条件下，他们已经做了足够多了。 记得有位中国vp评价说，相比中国的投资人，哪有外国的投资团队的活路。大概就是想表达，中国各行各业的生存之多艰，相对好的制度下的培育的行业团队，中国行业是颇有杀伤力的。 当然，这样的杀伤力更是“劣币驱逐良币”。长久看是恶化商业环境，既不是鼓励了资源优化、也不是鼓励创新、更不是鼓励更人性的生存环境。所以这样的“强势”，真叫人哭笑不得。也大约是全世界都逐渐清醒地抵制中国发展模式的根因。 商人与企业家的不同，更突出的体现在mindset不同。于我，可能默认自己对行业还有一些初心，不是唯利是图的。所以，愿意了解的都是行业内技术、管理实践等等。当我在拼命地理解和转化自动驾驶新技术和趋势，对国外成熟管理模式的移植，当然也很力不从心。开淘宝店，开咖啡馆的朋友，根本没有这样的mindset。服务行业不是没有科学管理和技巧可言，只是偏技术迭代的mindset偏离了服务的本质。 然鹅，在基础科学研究和应用科学转化上的各种弊病，以及反应在知名企业可供选择的求职口上的差距，实际上让国内想成长为企业家的年轻人，或安心当工程师的年轻人，是非常吃亏的。即便比美国年轻人更努力，也不会比他们生活的更舒适和自信。在这样的实体环境下，谈技术积累，应用转化，企业家精神，就像明知无望，还让人上的理想主义。可是现在的年轻人，才不会为了“爱国“二字义无反顾，国家欠着我等年轻人多了，收割我等韭菜，还要我等为着看不到的未来埋单。 相比实业，在国内，服务业的发展，至少不会明显存在差距。因为人总是要吃喝拉撒被服务的，虽然服务质量、体验、管理等，不如发达经济体，但是服务业没准备国际化：开饭店的、洗衣房的、健身房的等等，没准备打入国际市场，总之还可以混个体面生活。 国家抱怨国内的小老板，小企业家，小富即安，不思进取，不能担当“百年老店”。说实话，心里都明白：有制度保障，企业家大富吗，保障企业百年长青吗？与其显山露水，被制度卡脖子，大家都不愿做被拍死的出头鸟。也大概是万一做大了，就赶紧换国籍的原因。 我不再抱怨企业家、商人和个体，什么样的社会体制孕育企业和个人。说国内企业无奸不商，老百姓缺乏仁心仁德，只是国家的问题。在对国内实业无望之后，大量实业人才流入金融证券等服务业，大量企业家沦为商人，投机买地，也是无奈之举。不知道还有没有明天，今天就做一回人吧。谁不爱惜一次的生命呢。 两个人都挣50万，在学校承包食堂的经理a和在大公司上班的工程师b，显然是不同的。 国内混久了，初心早就喂狗了。活着，才是中国人的最高哲学。 ps, 不吐槽，不快乐。you just can’t change your role in life, even you don’t like it sometimes, so enjoy it happy or unhappy. 个人技能进阶带团队，会有一个时间段特别兴奋，感觉每天都有很多进步；也有一个时间段，非常阻塞，看不清下一步。又在一个阻塞期，想到了底层实力。 底层实力做技术/工程应用，前期缺乏很高明的应用框架设计，大部分时候就是解决具体的问题。手段如何都不重要。大概是测试驱动的开发，不断调试，试错，直到找到一个通畅的解决。下次再碰到新问题，继续试错调整。这样的工作模式，也许不算agile。 目前阶段开发的几个层次： 熟悉项目，写简单的逻辑 使用成熟的库/框架（网络，前端，分布式） 掌握新编程思想(协程、异步、容器、微服务） 与此同时，对开发工具，生产环境(git, Docker)，应用场景的拓展逐渐成型。 产品导向缺乏底层实力，会容易被卡住。或者说缺乏产品导向，走着走着，不知道下一步在哪儿了。产品导向，需要一个人多任务，多人协调能力。 初创公司生存技术突破做ai等技术场景的初创公司，花大量的资源和人力，研究和实现新算法，提高算法的识别率，精度等。 产品入口相反，传统行业的创业，并没有领先技术和雄厚资本，总是从最不起眼的组装产品开始的。 资本壁垒一些规模效应的创业团队，投资人大量投入资本拼杀。]]></content>
  </entry>
  <entry>
    <title><![CDATA[npc wp planner in ADS simulation]]></title>
    <url>%2F2019%2F09%2F11%2Fnpc-wp-planner-in-ADS-simulation%2F</url>
    <content type="text"><![CDATA[backgrounda ADS simulation platform should have the three components: vehicle dynamics model for game engine(e.g. unReal, Unity3D) based ADS simulation platform, there has simple game-level vehicel dynamics(VD), and in engineering side, there are more precise VD, such as Carsim, CarMaker, and a few smaller ones. sensor models currently camera and Lidar model are pretty good, for mm Radar, which has a frequency around 24G hz or 77G hz, which is far more beyond the digital computer can sample, so to simulate Radar(or Radar model) is not that practicalable. as can find in famous simulator, e.g. lg-sim and Carla, they are puting a lot efforts in sensor models, which should be more useful in future. another good way of sensor model simulation, is to test and verify the vendor’s sensors. most time, the performance test from vendors are not fit well. and as the physical model of the sensors is not visible, to simualate as a black box is useful then. scenario describe PEGSUS project used OpenScenario xml language; in other case, the scenario can be described directly in Python. basically what need to define during scenario descibtion, includes: scenario static env scenario npc actors movement in more advanced solutions, the process of builing virtual envs directly start from dronze/cars images or cloud point images. what’s in this post is one part of npc movement control: lane change. lane_change_done eventat default lg-sim, there is a lane-change event, which describes when lane change happens; for a more precise npc control, here adds a lane-chagne-done event, basically to describe when the lane change is done. similar to lane-change event, here defines the lane-change-done event in both server and client side. Unity.transform.position vs local positionhere is the math part. Unity has plenty APIs to handle these, which is good to study in. dot product Vector3.Dot() cross product p2p distance Vector3.Distance() interpolate Mathf.Lerp() world 2 local transfer Transform.InverseFromPoint(position) camera matrix equations of lines waypoints described laneNPC agents are drived by waypoints embedded in the virtual roads, in lg-sim, waypoints is inside LaneSegmentBuilder. but in general the waypoints are not ideal: their distance is not unique their index in neighboring lanes doesn’t keep consistent they don’t care of curves of real roads for waypoints based planner, rather AI-based planner, e.g. lane-change planner, the processes are as following: get currentTarget in target(current) lane which is usually not pointed by the same currentIndex as the previous lane, so need to figure out the closeset wp from the waypoint list in current lane, and this closest wp should be in front of NPC (no consider NPC retrograde). keep currentTarget update during NPC (lane-change) operation, there is case when the currentTarget is behind NPC position, if it’s not expected, it’s always a error planner, leading NPC header turn over. so need to check currentTarget is always in front of NPC, if not, update currentTarget to next wp. 1) driving direction vs currentTarget2NPCposition driving direction in global dot product currentTarget2NPCposition should greater than zero, if not, update currentTarget 2) NPC in currentTarget local position NPC local position should always in negative axis, if not, update currentTarget. the trick here, can’t use currentTarget in nPC local position, as when NPC is head of currentTarget, NPC will be pointed in reverse, which makes the local coordinate reverse as well. but currentTarget is the fixed wp always. lane-change-done criteria ideally when the whole NPC occupied in the target lane, that’s when the lane-change operation done. but that’s never the reality. as then, if there is the next next lane, the NPC is partly in the next next lane, so can’t keep lane change in only one neighboring lane. but in reality, highway or express way, the vehicle can’t across two lane in one time. so to keep lane-change-done and done in just the next lane, the criteria is when the NPC position in Z or X direction projection to currentTarget X or Z direction projection is more than half of the lane width: Mathf.Abs(frontCenter.position.z - currentTarget.z) &lt; laneWidth/2.0 usually in game engine, NPC can be controlled by AI, will discuss later.]]></content>
      <tags>
        <tag>lgsvl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[a nested conditonal variable model in python]]></title>
    <url>%2F2019%2F09%2F06%2Fa-nested-conditonal-variable-model-in-python%2F</url>
    <content type="text"><![CDATA[background the work is derived from lg-sim/lgsvl/remote, the original remote function is listen message return from server on each command request, basically a async request-receive model. additionaly, we want lg-sim server to send every episode state info to client too, luckily, the server - client communicate is based on Websocket，which support server actively pushing message. so simple add a episode state info in server side and send it through websocket at every frame works. in conditional variable design, notify() won’t be hanged-up, while wait_for() can be hanged-up if no other thread call notify() at first. in Python class, to keep object status, it’s better to use class member variable, rather than object member variable, which can’t track its status when the object reset. (/) try finally the status update in try() package won’t be the final status . 12345678910111213141516 def process(type): status = False try: if(type=="a"): status = True finally: if(type == "b"): status = False print("cv.status ....\t", status) return statusdef main(): type_list = ["a", "a", "b", "b", "a", "a", "a"]; for _ in range(len(type_list)): print(process(type_list[_])) the client designin client, the message received is handeled in process(). by default, there is only one type of message, namely the event message, so only one conditional variable(CV) is needed to send notification to command() to actually deal with the recieved message. first we defined the new message type(episode message), as well as a new real-handel fun episode_tick(). then modifying remote::process() as: 123456789101112131415try: self.event_cv.acquire() self.data = json.loads(data) if check_message(self.data) == "episode_message" ： self.event_cv.release() self.event_cv_released_already = True with self.episode_cv: self.episode_cv.notify() else : self.event_cv.notify()finally: if self.event_cv_released_already: self.cv_released_already = False else: self.cv.release() will it dead-locked ?as both command() and episode_tick() have conditional variable wait_for() : 1234567public command(): with self.event_cv : self.event_cv.wait_for(lambda: self.data is not None)public episode_tick(): with self.episode_cv.wait_for(lambda: self.data is not None) so if notify() is not called in any situation, dead-locked happens, meaning the wait_for() will never return but suspended. remote.process() is running in another thread, rather than hte main sim thread, where run sim.process(). remote.process() used to accept the message, and sim.process() is the actual place to handle these received messages. the two threads run simutaneously. for any message received, if its type is event_message, then it triggers event_cv.notify(), so command() in sim::process() won’t dead block; to avoid episode_message dead locked, in sim::process() need to call episode_tick() only when the message type is episode_message, which can be checked by remote.event_cv_released_already == True, 123456789101112def sim::process(events): j = self.remote.command(events) while True: if self.remote.event_cv_released_already : self.remote.episode_tick() if j is None: return if "events" in j: self._process_events(j) j = self.remote.command（“continue")]]></content>
      <tags>
        <tag>python</tag>
        <tag>multithread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vehicle dynamics model in ADS simulation]]></title>
    <url>%2F2019%2F09%2F03%2Fvehicle-dynamics-model-in-ADS-simulation%2F</url>
    <content type="text"><![CDATA[VD backgroundusually to simualate vehicle dynamic(VD) system, either by physical model, e.g. pysical models of engine, gearbox, powertrain; or parameter model, which doesn’t take into the physical process of the dynamic system, but tracking the system’s input, output and fit their relationship with polynomail equation or functions with multi-pieces. traction force is derived from engine torque, which goes to gearbox(powertrain system) and then divided by radius of wheels, then distribute to wheels. traction torque air drag air lift force traction force $$ F(traction) = F(air drag) + F(air lift force) + F(tire drag) + acc * mass $$ in a detail way, the equation above should split into lateral state equation and longitudional state equation, if consider driver control module, which will give laterl control equation and longitudional control equation. brake torque and ABS systemABS(anti-block system) works in the situation, when driver input brake torque is larger than the max ground-tire torque can attached between tire and ground. once max ground-tire torque is achieved, namely the max fore-aft force T is achived, the traction direction traction slip angular decceleration will leap, this is the dead-blocking situation, and when it happens, the driver input brake torque is saturated. to avoid block situation happens, usually track decelleration of traction slip angular during brake torque increase, if the value of decelleration of slip traction angular is beyond a threshold value, ABS controller will trigger to decease brake torque. drive stabilitythe driving stability is mainly due to forces on tires, sepcially the lateral angular velocity derived from Lateral Force lateral controltaking driver as one element, the driveing system is a close-loop control system. the system works on a road situation: the driver pre-expect a driving path(predefined path) and operate the steering wheel to some certain angular the vehicle take a move with a real driving path(real path) the real path is not exactly fitted to the predefined path, leading the driver take an additional conpensation control longitudinal controlsimilar as lateral control VD in Unityany vehicle in Unity is a combination of: 4 wheels colliders and 1 car collider. WheelConllider1) AxleInfo AxleInfo represents the pair of wheels, so for 4-wheel vehicle, there are two AxleInfo objects. 12345678910struct AxleInfo &#123; WheelCollider left ; WheelCollider right; GameObject leftVisual ; GameObject rightVisual ; bool motor ; #enable movement of this wheel pair bool steering ; # enable rotation of this wheel pair float brakeBias = 0.5f; &#125; 2) core parameters wheel damping rate suspension distance Force apply point distance (where ground force act on wheel) suspension spring forwardSlip(slip angle), tire slip in the rolling(tractional) direction, which is used in calculating torque sidewaySlip, the lateral direction slip, which leads to stability issue. 3) visualization the WheelCollider GameObject is always fixed relative to the vehicle body, usually need to setup another visual GameObject to represent turn and roll. implementation from lg-sim 1234567891011void ApplyLocalPositionToVisuals(WheelCollider collider, GameObject visual) &#123; Transform visualWheel = visual.transform; Vector3 position; Quaternion rotation; collider.GetWorldPose(out position, out rotation); visualWheel.transform.position = position; visualWheel.transform.rotation = rotation; &#125; 4) WheelCollider.ConfigureVehicleSubsteps 1public void ConfigureVehicleSubsteps(float speedThreshold, int stepsBelowThreshold, int stepsAboveThreshold); Every time a fixed update happens, the vehicle simulation splits this fixed delta time into smaller sub-steps and calculates suspension and tire forces per each smaller delta. Then, it would sum up all resulting forces and torques, integrate them, and apply to the vehicle’s body. 5) WheelCollider.GetGroundHitreturn the ground collision data for the wheel, namely WheelHit wheel friction curvefor wheels’ forward(rolling) direction and sideways direction, first need to determine how much the tire is slipping, which is based on speed difference between the tire’s rubber and the road,then this slip is used to find out the tire force exerted on the contact point the wheel friction curve taks a measure of tire slip as an Input and give a force as output. The property of real tires is that for low slip they can exert high forces, since the rubber compensates for the slip by stretching. Later when the slip gets really high, the forces are reduced as the tire starts to slide or spin 1) AnimationCurveunity official store a collection of Keyframes that can be evaluated over time vehicleController() in lg-sim1) the controllable parameters: 123456currentGear currentRPMcurrentSpeed currentTorquecurrentInput steerInput 2) math interpolate function used Mathf.Lerp(a, b, t) a -&gt; the start value b -&gt; the end value t -&gt; the interpolation value between start and end 3) fixedUpdate() 1234567891011121314// cal trace force by rigidbodyrigidbody.AddForce(air_drag)rigidbody.AddForce(air_lift)rigidbody.AddForceAtPosition(tire_drag, act_position)// update current driving torquecurrentTorque = rpmCurve.Evalue(currentRPM / maxRPM) * gearRaion * AdjustedMaxTorque // apply torque // apply traction control// update speedcurrentSpeed = rigidbody.velocity.magnitude// update fuel infofuelLevel -= deltaConsumption// update engine temp// update turn signal light 4) ApplyTorque() 123456float torquePerWheel = accelInput * (currentTorque / numberofWheels) foreach(axle in axles): if(axle.left.motor): axle.left.motorTorque = torquePerWheel if(axle.right.motor): axle.right.motorTorque = torquePerWheel 5) TractionControl() 123456789101112TractionControl()&#123; AdjustTractionControlTorque(axle.hitLeft.forwardSlip)&#125; AdjustTractionControlTorque(forwardSlip)&#123; if(forwardSlip &gt; SlipLimit) tractionMaxTorque -= 10 else tractionMaxTorque += 10 &#125; in lg-sim, the VD model is still simple, as there is only traction/logitudional control. referadd equation in markdown wheelcollider doc whell collider official whellcollider tutorial]]></content>
      <tags>
        <tag>lgsvl</tag>
        <tag>vehicle dynamics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里-9]]></title>
    <url>%2F2019%2F08%2F31%2F%E6%9C%AA%E6%9D%A55%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C-9%2F</url>
    <content type="text"><![CDATA[低速发展和高附加值昨天看到一篇比较中欧(美)国家差异的帖子：北欧国家是如何保持高福利的。非常有意思的一个概念：高附加值，低发展速度的行业。 这些国家有一些传统强势行业。很有意思的是，印象中的传统行业，比如农业、畜牧业、纺织、手工业等等，跟高科技、现代化很远，似乎是非洲国家的土著人干的活儿。相比，在国内，互联网、技术创业、地产、金融、mba营销、国际贸易等才是时尚。 早年在农业大学，对以色列强大的现代农业是有所耳闻的。只是没有这种洞察力，现代化的传统行业与国民福利之间还有这样的隐形关系。 相比，美国的农业、畜牧业等传统行业很不错；美国也引领科技、金融这些时尚的行业。在美国生活的人民确是有了进退的余地。向左，可以退居大农村；向右，可以进军纽约、硅谷当金融、科技新贵。 国家的选择看似主动，也是现实的驱动。国家可以集中发力在互联网、金融、创业等方面做的很有声有色，但与此同时，传统的制造业，典型低发展速度，需要长期积累，国家发力也解不了。比如，汽车发动机、精密仪器、芯片制造等。 制造业在国内的标签是低附加值，资本不愿意进入，国家队发力也没气色。然而经过了这些长期积累的国家，比如欧洲小国瑞士、德国，制造业出口完全是高端制造的典范。而且马太效应显著，让这些欧洲国家不需要社会剧烈的经济变动，不需要老百姓担心行业变动、反倒一年还有20天带薪假。政府和人民都其乐融融，有时候看北欧人的生活，建筑、厨房、国家公园、办公环境、甚至监狱，简直像在童话里；报道一个小小的新闻，似乎都要惊动整个镇子上的人一样，这里简直就生活在像伊甸园。 而中国总是一片火热的场景，5年大力发展基建、5年大力吹捧全国创业、又5年搞工业互联网。国家没有消停，老百姓有人欢喜有人愁。 中国没有先发优势，在这些传统行业缺少长期积淀，强行进入既不不讨资本喜欢，也不讨人喜欢。比如报道，西安火箭研究员工资20万；大国重器的老焊工，无数荣誉奖章，一个月才一万多，在北京买不起房；国家队出动安利中小企业贷款，但银行就是宁愿带给亏损的国企，也不给需要钱的中小企业。 对于这些需要长期资本补血，短期回报低，才能形成壁垒的传统行业。在这个资本、市场竞争相对成熟的年代，确实比100年前，搞种植、搞养殖，或西门子、博世等制造业刚出道积累行业(非金钱资本)“资本”要困难的多。似乎印证了上周接触深圳汽车电子行业的感触，一个词：活着。根本谈不上行业积累。 可以说，北欧国家以及北美国家，因为上几代人的积累，给他们留下的行业遗产、社会制度遗产，才允许他们活的这么滋润。相比，中国没有上几代人的遗产，又进入了成熟资本市场的21世纪，所以，是压力也是机遇。去发掘新世纪的矿吧。 对这个大环境的从业者而言，选择农业、传统制造业，也确实不聪明。干着比人累的活儿，挣得比人少，社会还不待见。年轻人选择行业要识大局。相比，这个时代热的东西，确实该多关注。存在即合理。 工程师的心态国内有些企业家喜欢讲情怀，吹“工匠精神”, 结果把自己作死了。而华为这样的企业，可以拿高薪，也是加班拿命换的。能够心安理得的当一名工程师，不需要为生计、价值实现、社会待见操心的，只会在这些“低速、高附加值”的欧美公司。一个国家的企业能进化到这种形态，基本是社会稳定，企业稳定，developed，达到了一个最佳平衡点，再投资也不会增加收益，每个岗位都相对稳定，组织内部管理效能达到最优，系统的规范化远超过个人能力，所以每个员工心安理得，做好自己份内的事就好。剩下的时间，享受生活，或者出于兴趣爱好，搞点车库创业都不在话下。 相比，国内的市场还在巨变当中，没有一家企业已经进化到developed，企业组织系统还不成熟，个人能力还能发挥显著价值，那老老实实当工程师的，在还在发生资源重组系统里，就会被挤到最底层。所以，不要怪国内的年轻人心态不正。因为这个时代，传统技工不会长在国内，国内应该培养属于这个时代的阶层和行业。想跟欧美，比工匠精神，确实是拿短处跟人家的长处比。]]></content>
  </entry>
  <entry>
    <title><![CDATA[reactjs introduction]]></title>
    <url>%2F2019%2F08%2F30%2Freactjs-introduction%2F</url>
    <content type="text"><![CDATA[React is the front-end framework used in lg-sim WebUI(version 2019.07). as well for Cruise webviz, Uber visualization, Apollo, they all choose a web UI design, there is something great about web framework. what is Reactused to build complex UI from small and isolated pieces of code “components” 1234567class vehicleManager extends React.Component &#123; render() &#123; return &lt;html&gt;&lt;/html&gt; &#125; &#125; React will render the html on screen, and any changes in data will update and rerender. render() returns a description(React element) of what you want to see on the screen, React takes the descriptions and displays the result. build a react hello-world app12345npx create-react-app react-democd react-demonpm start index.jsindex.js is the traditional and actual entry point for all Node apps. in React, it tells what to render and where to render. componentscomponents works as a func, and props is the func’s paramters, the func will return a React element which then rendered in view components can be either class, derived from React.Component, which then has this.state and this.setState() ; or a function, which need use Hook to keep its state. the design advantage of components is obvious: to make components/modules reuseable. usually in route.js will define the logic switch to each component based on the HTTP request. setStatewhenever this.setState() takes an object or a function as its parameter, update it, and React rerender this component. when need to change a component state, setState() is the right way, rather than to use this.state = xx, which won’t register in React. Hookexample from stateHoook 12345678910111213141516import &#123; useState &#125; from 'react';function Example() &#123; // Declare a new state variable, which we'll call "count" const [count, setCount] = useState(0); return ( &lt;div&gt; &lt;p&gt;You clicked &#123;count&#125; times&lt;/p&gt; &lt;button onClick=&#123;() =&gt; setCount(count + 1)&#125;&gt; Click me &lt;/button&gt; &lt;/div&gt; );&#125; Hook is a speicial feature, used to share sth with React function. e.g. useState is a way to keep React state in function, which by default has no this.state in Hook way, even afer function() is executed, the function’s variable is not clear, but keep until next render. so basically Hook give a way to make function stateable. intial hook the only parameter pass in hook::useState(new Map()) is the initial state. useState(initState) return from useState it returns a pair [currentState, setStatefunc], e.g. [maps, setMaps], setStatefunc here is similar as this.setState in class component access state directly {currentState} update state {() =&gt; setStatefunc(currentState)} Context context gives the way to access data, not in the strict hierachy way. the top is the context Provider, and the is the context consumer, there can be multi consumers. createContextas in react context official doc 1const Context = React.createContext(defaultValue); during render, the component which subscribe this context will get the context content from its context provider and put in rendering, only when no avialable provider is found, defaultValue is used. 1&lt;Context.Provider value=&#123;/xx/&#125; &gt; whenever the value in Provider changes, all consumers will rerender. EventSourceEventSource is HTTP based one-way communication from server to client, which is lighter than Websocket eventsource vs websocket, also the message type is only txt in EventSource, while in Websocekt it can be either txt or binary stream. by default, when client received a [“/event”] message, will trigger onMessage(). but EveentSource allow to define user-speicial event type, e.g. VehicleDownload, so in client need to a new listener: 1myEventSource.addEventListener('VehicleDownload', (e)=&gt;handleVehicleEvents(e)) react routejs specialfirst class object a function is an instance of the Object type a function can have properties and has a link back to its constructor method can store a function in a variable pass a function as a parameter to another function return a function from another function promisearrow function]]></content>
      <tags>
        <tag>react</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nancyfx study]]></title>
    <url>%2F2019%2F08%2F30%2Fnancyfx-study%2F</url>
    <content type="text"><![CDATA[document, Nancy is used in lg-sim webUI 2019.07 version, pretty new staff for me. IntroductionNancy is a lightweight, low-ceremony framework for building HTTP based services on .NET and Mono. Nancy is designed to handle DELETE, GET, HEAD, OPTIONS, POST, PUT and PATCH request and provides a simple, elegant Domain Specific Language(DSL) for returning a response. build to run anywhereNancy was designed to not have any dependenceis on existing frameworks, it’s used pretty much wherever you want to. host in Nancy acts as an adaptor for a hosting environment, thus enabling Nancy to run on existing techs, such as ASP.NET, WCF. the bare minimum requires to build a Nancy service are the core framework and a host. helloworld serviceall module should be derived from NancyModule, and define a route handler. tips: always make the module public, so NancyFx can discover it. 12345678public class HelloWorld : NancyModule &#123; public HelloModule() &#123; Get[&quot;/&quot;] = parameters =&gt; &quot;Hello World&quot; ; &#125;&#125; exploring modulesmodule is where you define the logic, is the minimum requirement for any Nancy app. module should inherit from NancyModule, then define the behaviors, with routes and actions. modules are globally discoveredthe global discovery of modules will perform once and the information is then cached, so not expensive. Define routesto define a Route need to specify a Method + Pattern + Action + (optional) Condition 123456789public class VehicleModule : NancyModule&#123; public VehicleModule() &#123; Get[&quot;/vehicle&quot;] = _ =&gt; &#123; // do sth &#125;; &#125; &#125; or async run: 123456789public class VehicleModule : NancyModule&#123; public VehicleModule() &#123; Get[&quot;/vehicle&quot;, runAsnyc: true ] = async(_, token) =&gt; &#123; // do sth long and tedious &#125;; &#125; &#125; Methodmethod is the HTTP method used to access the resource, Nancy support: DELETE, GET, HEAD, OPTIONS, POST, PUT and PATCH. secret for selecting the right route to invokein case when two routes capture the same request, remember : the order in which modules are loaded are non-deterministic routes in a given module are discovered in the order in which they are defined if several possible matches found, the most specific match. root pathall pathes used in Nancy are relative to root path, which tell Nancy where its resources are stored on the file system, which is defined in IRootPathProvider static contentstatic content is things e.g. javascript files, css, images etc. Nancy uses a convention based approach to figure out what static content it is able to serve at runtime. Nancy supports the notion of having multiple conventions for static content and each convention is represented by a delegate with the signature Func&lt;NancyContext, string, Response&gt; the delegate accepts two parameters: the context of the current request and the application root path, the output of the delegate is a standard Nancy Response object or null, which means the convention has no static content. define your own conventions usign the bootstrapperlink View enginesview engine, takes a template and an optional model(the data) and outputs(usually) HTML to be rendered into the browser. in lg-sim, the view is rendered in nodejs. MVCmodel view controller understand controller a controller is reponsible for controlling the flow logic in the app. e.g. what reponse to send back to a user when a user makes a browser request. any public method in a controller is exposed as a controller action. understand view a view contains the HTML markup and content that is send to the browser. in general to return a view for a controller action, need to create a subfolder in the Views folder with the same name as the controller. understand model the model is anything not inside a controller or a view. e.g. validation logic, database access. the view should only contain logic related to generating the user interface. the controller should only contain the bare minimum of logic required to return the right view. C# anonymousc# programming guide 12345(input-parameters) =&gt; expression (input-parameters) =&gt; &#123;&lt;sequence-of-statements&gt;&#125;TestDelegate testD = (x) =&gt; &#123;(Console.WriteLine(x);&#125;; =&gt; is the Lambda operator, on its left is input paramters(if exist). Lambda expression/sequence is equal to a delegate class. delegate is a refer type, used to pass one function as paramter to another function. e.g. in event handle. Nancy in lg-sim WebUI/Assets/Scripts/Web a few steps to build Nancy server: Nancyhost.start() add NancyModule instance(where define route logic) a few other libs used : PetaPoco, a light-weight ORM(object relational mapper) framework in C# SQLite refermeet nancy nancy doc in chinese `]]></content>
      <tags>
        <tag>C#</tag>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里 (8).md]]></title>
    <url>%2F2019%2F08%2F24%2Fwhere-are-you-in-next-5-years-9%2F</url>
    <content type="text"><![CDATA[这一系列的思考来源两个事件：深圳汽车电子行业代表来司交流；与全球机器人大赛参展。 深圳的中小企业 vs 资本大户布谷鸟 智能驾驶座舱解决方案供应商。主要产品，车载显示屏，仪表屏和基于Android操作系统的上层应用。展示完，第一感受就是深圳华强北来了。 当年（2007年)，中国风靡深圳山赛手机。甚至多年以后，山赛智能手机成了中国发展的一个典型案例，为众人津津乐道： 眼看他起高楼，眼看他宴宾客，眼看他楼塌了。 这个产业最初蓬勃，但凡投资进去的人，都躺着赚钱，只是没能力在蓬勃发展期间为转型蓄力。后来政策、市场、甚至原材料的任何风吹草动，都足以摧毁它。 这个产业的问题放大了看，就是 华为/阿里 与 中兴/联想 的对比。国内的制造业小企业，最初都是代理、组装、贴标签、山赛货。如果碰到市场爆发，春风得意，赚的盆满钵满。但是，后续如何发展，创始人的vision就立判高下了。 一个选择是立足长远利益，未雨绸缪，生于有患。在打开了市场之后，立马投入产品和相关技术、供应链、品牌积累。可以接受眼前的低资金回报，选择了艰辛但持久的终成就行业头部的道路。 一个选择是只看赚钱。自己做产品积累，回钱周期太久，根本不考虑。资本趋利，大不了赚到钱，再转战下一片红海。这是势利的商人思路，他们对某一个具体行业不带任何感情，冷血。任何行业只是一个资本生长的土壤，不管是做手机、地产、保险、造车等等，都不是出于热爱，而是资本逐利。比如，宝能收购万科，就是冷血的资本挑战性情的行业中人。 但凡对行业还有所爱的，都难以忍受被资方强奸。所以，恒大站出来说要造车，我的第一反应，就是恒大要来强奸造车人。恒大没有对汽车的感情，只不过跟进资本进军汽车领域。资本原本是公司的生产要素之一，但是资本又最不具有公司界限约束，任何外部资本都能挑战公司自身。所以，如何让资本服务于行业公司？ 制造业创业回到布谷鸟，号称做车载计算平台，不自己做计算芯片，不自己做车载屏幕，不自己做车载定制Android操作系统，顶多开发几个上层app。这不又是一家华强北组装厂吗。创业还靠廉价组装竞争？相比，北上的创业公司，诸如硬件地平线、软件旷视等算是技术/境界高多了。 也不得不提本司。背靠大树，天然壁垒，但实际做的还是华强北的活儿。不同之处是，资本压力小，对技术储备有规划但没有环境和姿态的转变。会聘一两个外国人、一两个教授撑面儿，但这几位基本处于边缘，决策、项目规划都不考虑他们。估计其他家，也好不了哪里去。要不然，市场会有反馈的。 这样的团队/小公司里面，对想要拔高产品/技术见识的年轻人其实很艰难。因为打开市场，销售是关乎存活的，相比产品是自研的，还是贴牌的，有没有底层开发/设计积累都是次要的。 要去这种小团队做产品/技术，就需要能单挑担子的产品人/技术大牛。当然，付出与回报怎么权衡。给ceo待遇，似乎可以考虑。 传统制造业大公司里面的小团队，虽然没有自己去开辟市场的压力，大不了内销，但同时带来的问题，是积重的公司元老，对职业经理人，工程师文化，都是严重的挑战。 公司元老经常会看到，职业mba人在中国企业水土不服，或者回国的硅谷工程师对中国企业文化的抱怨。虽然，宏观数据国内企业似乎都很国际化了，底层的/微观的现代公司管理/制度上，却不是一代就能跟上国际的。比如，mbaer到中国公司，很难战胜资源/权利在握的元老们，按照现代管理办法去调整公司，那基本上就等着被边缘化，然后公司不养闲人，下一步就是踢出去。公司又回到原来的管理模式。老人们再次证明自己不瞎折腾，是公司的积重，继续升官发财。 工程师文化，不管是如google等互联网公司，还是如ford等汽车制造业公司，都比较明显。hr, manager, 甚至leader团队把自己归为工程师团队提供服务的，在衣食住行、工作环境、设备、交流、培训等等都照顾的很好。 虽然北美是更成熟的资本社会，他们的企业反而不那么见钱就撒网，没见过ford投资移动互联网，google要去收购造车厂，或者发展文化传媒业务的。他们的业务更倾向纵深，专注，在一个领域深耕，然后成为行业头部。这可能也是成熟资本市场的现代化公司该有的样子。 相比，国内的企业太草莽。国内的工程师只有听命于人。技术积累对绝大多数企业，都是可以谈谈但不会认真做的。这样的情况，当然长远是没前途的。一波政策、市场的红利之后，基本作死。 人口基数国内也有很多有情怀的创业人或中小企业主，他们也许并不甘与政策红利，也想认真做产品。但是，在这个社会没办法。 美国的创业环境，想想apple, google, facebook，cruise等等创业经历。首先他们自己没有生存压力，出于热爱开始的；创业阶段，市场/政策对他们的包容度很好，允许决策失败；然后，资本市场、专业人才的补充都有成熟的供应体系。 国内，一方面有廉价的人口红利，但对大部分创业公司，规模经济反而很难真正成为其产品的利好因素，除了资本玩家，比如，摩拜单车、瑞幸咖啡、yy连锁酒店，滴滴出行等等，这些玩家，根本不是在拼产品，而是资本。谁占有更多资本，谁就能笑到最后。 这样的创业环境，不能培养社会范围内更好的创业土壤，反而破坏了创业各方面的供应系统，包括创始人的初衷，专业人才队伍，资本法务系统建立等等。归结一个词：浮躁。 人口基数大，在政治家、资本家眼里是好词，但是对个体，就意味着就业竞争、服务质量差、人际关系不温存。 经历过大公司的年轻人在三四线小城市，有很多生活无忧的年轻人。他们大学毕业后，在大城市瞎晃了一年半载，找不到合适的企业，就打道回府了。家里有条件，在当地慢慢都会活的滋润。我是觉得，经历下一个现代化管理的大公司，也是个不错的体验。至少知道人类社会，最优秀的组织体系是怎么运作的。 当然，没有上升，一辈子在大公司的系统里打工，就有点无趣。不如回家悠哉悠哉。 世界机器人大会2019整体感受，小企业生存多艰。]]></content>
  </entry>
  <entry>
    <title><![CDATA[real-world env in ADS simulation]]></title>
    <url>%2F2019%2F08%2F20%2Freal-world-env-in-ADS-simulation%2F</url>
    <content type="text"><![CDATA[this topic is based on lg-sim, the advantage is plenty Unity3D resources. to drive planning design, simulation has to keep as real-world env as possible. for L3 and below, the high-fidelity is actually most about HD map, since in L3 ADS, sensors only cares about the road parternors on road, and planning module take hd map as input. so there is a need to build simulation envs based on hd map. osmin the Unity engine, all in the env(e.g. traffic light, road lanes, lane mark etc) are GameObjects. to describe a map info, one of most common used map format is OSM (another is opendrive, but no open parser in unity yet), however which is not born to used in hd map, but still is very flexible to extend to descripe all info, a hd map requires. e.g. lane id, lane width, lane mark type e.t.c the open-source tool OsmImporter is good enough to parse osm meta data into Unity GameObjects. for different map vendors, first to transfer their map format into the extended-osm format. and then can generate all map info related gameObjects in Unity env. that’s the main idea to create real-world env in simualtor. the end is to make a toolchain from map vendor input to Unity 3D env. waypointsin either lg-sim or carla, the npc vehicles in self-driving mode, is actually controlled by following waypoints in the simulator, and waypoints is generated during creating map in step above. carla has plenty client APIs to manage the waypoints, and then drive npcs. hd-map toolby default, both lg-sim and carla has a tool to create hd-map, that’s basically mark waypoints on the road in an existing map, which is not strong. carla later support Vector one to build in map more efficiently. L3+ virtual env generatorthere are plenty teams working on building/rendering virtual envs directly from sensor data, e.g Lidar cloud point or camera image, and plenty image-AI techs here, which of course gives better immersed experince. and the test task is mostly for perception, data-fusion modules, which is heavier in L3+]]></content>
      <tags>
        <tag>lgsvl</tag>
        <tag>simulation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[threading and websockets]]></title>
    <url>%2F2019%2F08%2F16%2Fthreading-and-websockets%2F</url>
    <content type="text"><![CDATA[threading.Thread123456789101112run()start()join([time])isAlive()getName()setName() to initialize a thread with: threadID, name, counter start() and run()start() once for each thread, run() will not spawn a separate thread, but run in current thread 1234567891011121314class myThread(threading.Thread): def __init__(self, *args, **kwargs): super(myThread, self).__init__(*args, **kwargs) def run(self): print("run ... from start()")if __name__ == "__main__": demo = myThread() demo.start() demo.join() lock objectsa primitive lock does not belongto a certain thread when locked. by default, when constructed, the lock is in unlocked state. acquire(), will set the lock to locked and return the lock immediately(atom operator); if current lock is locked, then acquire() blocks (the thread) untill the occuping-lock thread calls release(). if multi-thread blocked by acquire(), only one thread will get the lock when release() is called, but can’t sure which one from the suspended threads condition objects12345678910threading.Condition(lock=None)acquire()wait()notify()release() thread A aquire() the condition variable, to check if condition satification, if not, thread A wait; if satisfied, update the condition variable status and notify all other threads in waiting. websocketbackgroundwebocket is better than http when the server need actively push message to client, rather than waiting client request first. clientusually webSocket client has two methods: send() to send data to server; close() to close websocket connection, as well a few event callbacks: onopen(): triggered when websocket instance get connected onerror(), onclose(), onmessage(), which triggered when received data from server. when constructing a webSocket instance, a connection is built between server and client. 1234567891011121314151617// client connect to server var ws = new WebSocket('ws://localhost:8181');ws.onopen = function()&#123; ws.send("hello server");&#125;// if need run multi callbackws.addEventListener('open', function(event) &#123; ws.send('hello server'); &#125;); ws.onmessage = function(event)&#123; var data = event.data ;&#125;;ws.addEventListener("message", function(event)&#123; var data = event.data ;&#125;); ws.send('client message'); onmessage()whenever the server send data, onmessage() events get fired. server onOpen(), triggered when a new client-server connection setup onMessage(), triggered when message received onClose(), onError(), implementationin c#, there is websocket-sharp, in python is python-websockets, in js is nodejs-websocket. as well as websocket protocol is satisified, server can write using websocket-sharp, and client in python-websockets. an example is in lg-simulator. the message type in websocket can be json or binary, so there should be json parse in c#(SimpleJSON), python(json module) and js (JSON). refersending message to a client sent message from server to client python websockets nodjs websocket c# websocket]]></content>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unet intro]]></title>
    <url>%2F2019%2F08%2F16%2FUnet-intro%2F</url>
    <content type="text"><![CDATA[unity3d manual High Level API(HLAPI) HLAPI is a server authoritative system, triggered from UnityEngine.Networking authority host has the authority over all non-player GameObjects; Player GameObjects are a special case and treated as having “local authority”. local/client authority for npcmethod1: spawn the npc using NetworkServer.SpawnWithClientAuthoritymethod2: NetworkIdentity.AssignClientAuthority network context propertiesisServer()isClient()sLOcalPlayer()hasAuthority() networked GameObjectsmultiplayer games typically built using Scenes that contain a mix of networked GOs and regular GOs. networked GOs needs t obe synchronized across all users; non-networked GOs are either static obstacles or GOs don&apos;t need to synchronized across players networked GO is one which has a NetworkIdentiy component. beyond that, you need define what to syncronize. e.g. transform ,variables .. player GO NetworkBehavior class has a property: isLocalPlayer, each client player GO.isLocalPlayer == true, and invoke OnStartLOcalPlayer() Player GOs represent the player on the server, and has the ability to run comands from the player’s client. spawning GOsthe Network Manager can only spawn and synchronize GOs from registered prefabs, and these prefabs must have a NetworkIdentity component spawning GOs with client authorityNetworkServer.SpawnWithClientAuthority(go, NetworkConnection),for these objects, hasAuthority is true on this client and OnStartAuthority() is called on this client. Spawned with client authority must have LocalPlayerAuthority set to NetworkIdentity, state synchronization[SyncVars] synchronzed from server to client; if opposite, use [Commands] the state of SyncVars is applied to GO on clients before OnStartClient() called. engine and editor integration NetworkIdentity component for networked objects NetworkBehaviour for networked scripts configurable automatic synchronization of object transforms automatic snyc var build-in Internet servicesnetwork visibilityrelates to whether data should or not sent about the GOs to a particulr clinet. method1: add Network Proximity Checker component to networked GOmethod2: Scene GOs saved as part of a Scene, no runtime spawn actions and communicationmethod1: remote actions, call a method across networkmethod2: networking manager/behavior callbacksmethod3: LL network messages host migrationhost: a player whose game is acting as a server and a “local client”remote client: all other playersso when the host left, host need migrate to one remote client to keep the game alive how it works: enable host migration. so Unity will distribute the address of all peers to other peers. so when host left, one peer was selected to be the new host (heart-keeping) network discoveryallow players to find each other on a local area network(LAN) need component , in server mode, the Network Discovery sends broadcast message over the network o nthe specified port in Inspector; in client mode, the component listens for broadcast message on the specified port using transport Layer API (LL API)socket-based networking in the endnetwork is basic cornerstone in server-client applications, Unet is deprecated at this moment, but still many good network resource can take in charge. e.g. websockets, nodejs, and what behind these network protocol or languages, e.g. async/coroutines are charming as well.]]></content>
      <tags>
        <tag>unity3D</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python generator and asynico]]></title>
    <url>%2F2019%2F08%2F16%2Fpython-generator-and-asynico%2F</url>
    <content type="text"><![CDATA[python iteratorin python, iterator is any object that follows iterator protocol. which means should include method: __iter()__, next(), if no next element should raise StopIteration exception. take an example, dict and list have implemented __iter()__ and __getitem__() methods, but not implemented next() method. so they are iterable but not iterator. generatorto support delay operations, only return result when need, but not immediately return generator funcion (with yield) generator expression Python generator in other languages is called coroutines yieldyield is almost of return, but it pauses every step after executing the line with yield, and till call next() will continue from the next line of yield. 12345678while True: sim.run(timeout, cb)def cb(): a = 1 yield print(a) a += 1 coroutinesexample: 12345678def cor1(name): print("start cor1..name..", name) x = yield name print("send value", x)cor_ = cor1("zj")print("next return", next(cor_))print("send return", cor_.send(6)) to run coroutine, need first call next(), then send() is called. namely, if not call next() first, send() will wait, and never be called. the thing is, when define a python generator/coroutine, it never will run; only through await(), next() call first, which then trigger the generator/coroutine start. asyncioasyncronized IO, event-driven coroutine, so users can add async/await to time-consuming IO. event-loop event loop will always run, track events and enqueue them, when idle dequeue event and call event-handling() to deal with it. asyncio.Task awaitawait only decorate async/coroutine, which is a waitable object, it works to hold the current coroutine(async func A) and wait the other coroutine(async func B) to the end. 123456789async def funcB(): return 1async def funcA(): result = await funcB() return resultrun(funcA()) multi-task coroutines1234567891011121314151617181920212223242526272829303132333435363738loop.create_task()run_until_complete() #block the thread untill coroutine completedasyncio.sleep() #nonblock event-loop, with `await`, will return the control-priority to event-loop, at the end of sleep, control-priority will back to this coroutine asyncio.wait(), #nonblock event-loop, immeditialy return coroutine object, and add this corutine object to event-loop``` the following example: get the event-loop thread, add coroutine objct(task) in this event-loop, execute task till end.```pythonimport asyncioasync def cor1(name): print("executing: ", name) await asyncio.sleep(1) print("executed: ", name)loop = asyncio.get_event_loop()tasks = [cor1("zj_" + str(i)) for i in range(3)]wait_cor = asyncio.wait(tasks)loop.run_until_complete(wait_cor)loop.close()``` ### dynamically add coroutine```shell loop.call_soon_threadsafe() # add coroutines sequencially asyncio.run_coroutine_threadsafe() #add coroutines async add coroutine sequenciallyin this sample, main_thread(_loop) will sequencely run from begining to end, during running, there are two coroutines registered, when thread-safe, these two coroutines will be executed. the whole process actually looks like run in sequencialy 123456789101112131415161718192021222324252627import asynciofrom threading import Threaddef start_loop(loop): asyncio.set_event_loop(loop) loop.run_forever()def thread_(name): print("executing name:", name) return "return nam:" + name_loop = asyncio.new_event_loop()t = Thread(target=start_loop, args=(_loop,)) #is a thread or coroutine ?t.start()handle = _loop.call_soon_threadsafe(thread_, "zj")handle.cancel()_loop.call_soon_threadsafe(thread_, "zj2")print("main thread non blocking...")_loop.call_soon_threadsafe(thread_, "zj3")print("main thread on going...") add coroutines asyncin this way, add/register async coroutine objects to the event-loop and execute the coroutines when thead-safe 1234567891011future = asyncio.run_coroutine_threadsafe(thread_("zj"), _loop)print(future.result())asyncio.run_coroutine_threadsafe(thread_("zj2"), _loop)print("main thread non blocking...")asyncio.run_coroutine_threadsafe(thread_("zj3"), _loop)print("main thread on going...") async callback vs coroutinescompare callback and coroutines is hot topic in networking and web-js env.]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dns server in lan]]></title>
    <url>%2F2019%2F08%2F10%2Fdns-server-in-lan%2F</url>
    <content type="text"><![CDATA[DNS server in LANto use domain name(e.g. gitlab.com) in LAN rather than IP, it needs every local host machine to store all key-values:: host-IP. if the LAN has many host machines, it will be difficult to maintain. Setting up DNS server will help to automatically map the ip to domain or reverse in the LAN. bind912345678910111213141516171819202122232425262728293031323334353637383940apt-get install bind9 ``` ### /etc/bind/named.conf.local```shellzone "gitlab.com" &#123; type: master; file "/etc/bind/db.ip2gitlab.com" ;&#125;; zone "101.20.10.in-addr.arpa" &#123; type: master; file "/etc/bind/db.gitlab2ip.com" ;&#125;;``` ### /etc/bind/db.gitlab2ip.com [dns zone file format](https://help.dyn.com/how-to-format-a-zone-file/)gitlab2ip zone file is mapping from domain to ip, as following sample, it works like: www.$ORIGIN --&gt; 10.20.101.119 ```shell; command$TTL 6000;@ refer to current zone file; DNS-server-FDNQ notification-email$ORIGIN gitlab.com@ IN SOA server email ( 2 ; 1d ; 1h ; 5min ; )@ IN NS serverwww IN A 10.20.101.119server IN A 10.20.101.119 /etc/bind/db.ip2gitlab.comip2gitlab zone file is from ip to domain mapping, 1234567891011$TTL 6000$ORIGIN 101.20.10.in-addr.arpa@ IN SOA server. email. ( 2 ; 1d ; 1h ; 5min ; )@ IN NS server119 IN A www.gitlab.com119 IN A server.gitlab.com nslookupnslookup www.gitlab.com #dns forward (domain 2 ip) nslookup 10.20.101.119 #reverse (ip 2 domain) settingsif the DNS setted above(DNS-git) is the only DNS server in the LAN, then this DNS works like a gateway, to communicate by domain name, every local host talk to it first, to understand the domain name. but in a large size company LAN newtwork, there may already has a DNS server hosted at IT department (DNS-IT), with a fixed IP e.g. 10.10.101.101, and all localhost machines actually set DNS-IT as the default DNS. DNS-git will work as a sub-DNS server. Inside the small team, either every localhost change default DNS to DNS-git, then DNS-git become the sub-network server. if every localhost still keep DNS-IT, there is no way(?) to use DNS-git service in LAN, and even make conflicts, as the DNS-git localhost machine will listen on all TCP/IP ports, every new gitlab.com access request (input as IP address) will get an output as domain name, but the others can’t understand this domain-name… what happened with two DNS server in LAN ? how email worksMail User Agent(MUA), e.g. Outlook, Foxmail, used to receive and send emails. MUA is not directly sent emails to end users, but through Mail Transfer Agent(MTA), e.g. SendMail, Postfix. an email sent out from MUA will go through one or more MTA, finally reach to Mail Delivery Agent(MDA), the email then store in some database, e.g. mailbox the receiver then use MUA to review the email in the mailbox ps, one day work as a IT admin ….]]></content>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[design scenarios in ADS simulation]]></title>
    <url>%2F2019%2F07%2F25%2Fdesign-scenarios-in-ADS-simulation%2F</url>
    <content type="text"><![CDATA[to design scenarios in self-driving test, one reference is: a framework for automated driving system testable cases and sceanrios, most other scenario classifications have almost the same elements: ODD, e.g. road types, road surfaces; OEDR, namely object and event detection and response, e.g. static obstacles, other road actors, traffic signature, environment conditions, special zones; and failure mode behaviors. in general, test cases can be grouped as black box test, in which the scenario parterners’ behavior is not pre-defined or unpredictable, e.g. random traffic flow(npcs) scenario, or white box test, where the npcs behavior is pre-defined, e.g. following user-define routings. white box testing is helpful to support performance metrics; while black-box testing is helpful to verify the system completeness and robust. as for ADS test, there are a few chanllenges coming from: heuristics decision-making algorithms, deep-learning algorithms, which is not mathematically completed the test case completeness, as the number of tests required to achieve statistically significant to claim safe would be staggering undefined conditions or assumptions a sample test scenario set maybe looks like: ODD OEDR-obj OEDR-loc maneuver in rump static obstacles in front of current lane in rump static obstacles in front of targe lane in rump dynamic obstacles in front of current lane scenarios_runnercarla has offered an scenario engine, which is helpful to define scenarios by test_criteria and behaviors, of course the timeout as well. test_criteria, is like an underline boundary for the scenario to keep, if not, the scenario failed. e.g. max_speed_limitation, collision e.t.c. these are test criterias, no matter simualtion test or physical test, that have to follow the same criterias; in old ways, we always try to find a way to evaluate the simulation result, and thought this may be very complex, but as no clue to go complex further, simple criterias actually is good. even for reinforcement learning, the simple criterias is good enough for the agent to learn the drive policy. of course, I can expect there are some expert system about test criterias. For simulation itself, there has another metric to descibe how close the simulation itself to physical world, namley to performance how well the simulator is, which is beyond here. behaivor is a good way to describe the dynamic processing in the scenario. e.g. npc follow lane to next intersection, and stop in right lane, ego follow npc till the stop area. OpenScenario has similar ideas to descibe the dynamic. to support behaivor, the simulator should have the features to control ego and npc with the atomic behaviors, e.g. lane-follow, lane-change, stop at intersection e.t.c. in lg simulator, npc has simple AI routing and lane-following API, basically is limited to follow pre-defined behaviors; ego has only cruise-control, but external planner is avialable through ROS. for both test_criteria and behavior, carla has a few existing atomic elements, which is a good idea to build up complex scenarios. OpenScenarionot sure if this is a project still on-going, carla has interface and a few other open source parsers there, but as it is a standard in popular projects, e.g. PEGUS, should be worthy to take further study.]]></content>
      <tags>
        <tag>lgsvl</tag>
        <tag>simulation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[model based design sucks]]></title>
    <url>%2F2019%2F07%2F18%2Fmodel-based-design-sucks%2F</url>
    <content type="text"><![CDATA[AV development includes perception, sensor fusion, location &amp; mapping, decision-making &amp; control (or motion planning), embedded, simulation and maybe many system-glue software tools. L3 planning &amp; control is now expert-based decision system, which basically defines rules to make decision, where model based design(mbd) is a helper. Waymo mentioned their hybrid decision-making solution, basically Machine Learning(ML) will take a big part of the situations, but still space to allow rule-based solution to take priority. when consider to ML decision making, mdb will become less useful. why model-basedTraditional OEMs follow vehicle-level safety requirements(ASIL-D) to develop vehicle products and components, usually can be represented as the V style development, from user requirs, system design, implmenent to test verification. to go through the whole V process take a rather long time, e.g. for a new vehicle model, it means 2~5 years. Commercial hardware and software(mobile apps) products which has lower level safety requirements, however, can iterate in a quicker frequency. the safety requirements drive the product development in a very different way, compared to common Internet products, which include more straight-forward programming skills and software architecture mindset. but to satisfy the additional, or should say the priority safety requirements, how to organize the code is less important than how to verify the functions is to satisfy the safety. so there comes the model-based design, the most-highly feature of which is to support system test and verify at pre-product period. of course, model-based design should be easily to build up prototype and visualize the system, which is the second feature of mbd, working similar like a microsoft vision e.t.c thirdly, from design to product, is auto code generation. which means once the design is verified, you don’t need to go back to write code again, but directly generate code from the design graph. model-based design toolchain is already a whole eco-system, e.g. system design, auto code generator, test tools. and all these tools should be first verified by ASIL-D standard. Internet AI companies once thought it would be easy to take over this traditional development by Internet agile development, while the reality is they still depends on model-based design at first to verify the system, then back to implement code again, which should be more optimized than auto-generated ones, which is one drawbacks of mbd, as mbd is tool-depended, e.g. Matlab, if Matlab doesn’t support some most updated libs, then they are not in the auto-code, and most time Matlab is far behind the stable version of libs outside. what mbd can’t dombd is born to satisfy safety requirments in product development. so any non safety required product won’t use mbd. and by nature, mbd is good at turning mathematical expressions to system languages, and logical relations to state flows, so any non-articulatable system is difficult to represent in mdb languages. in vehicle product development, engine, powertrain, ECU, brake system, ADAS, L3 motion planning, e.t.c have depends heavily on mbd. but also we can predict, L3+ applications arise, with image, cloud point based object detection, data fusion, SLAM, AI-driven planning, IVI, V2X, will hybrid mbd with many Internet code style. industry experience: a metaphysicssome friends say mass-product-experience makes him more value than new birds. since industry experience is not transparent, as there is a no clear bar to test the ability/value of the enginer, unlike developers, who can valued by their product, or skills, also the same reason make these guys who stay long in the industry sounds more valued, and they have more likey went through one or many mass product experience. but at most, industry product depends e.g. vehicle, on teamwork, even the team lead can’t make it by himself, unlike developer, a top developer can make a huge difference, much valued than a team of ordinary ones.]]></content>
  </entry>
  <entry>
    <title><![CDATA[autosar sucks]]></title>
    <url>%2F2019%2F07%2F15%2Fautosar-sucks%2F</url>
    <content type="text"><![CDATA[what is AUTOSARbasically it’s an micro-service architecture for vehicle EE system. Each micro-service is called software component(swc), and it has uniform interfaces, while of which the implementation is varied. as the goal of AUTOSAR said: share on the standard (interface), compete in the implementation. the inter-connect of micro-services is through virtual function bus(vfb), which works as a gateway, guiding data flow from port A, from micro-serviceA to port B, from micro-serviceB the benefits of AUTOSAR is obvious, to design the interface at system level first, if any changed need, it can be updated quickly. after the system architecture is fixed, then go to the implementation details. refer input description software components(micro-services) description, only define the data flow, interface functions system, system topology(interconnection among ECUs, and available data buses, protocols etc) hardware, the available hardware(processors, sensors, actuators etc) system configurationused to distributes the software component descritpions to different ECU ECU configurationthe basic software(BST) and run-time environment(rte) of each ECU has been configured, this is based on the dedication of the application software components to each ECU. generation of executablesin this step to implement the software components, then build. this can be automated done by tool-chains. all steps up to now are supported by defining exchange formats(xml) and work methods. basic softwareeach swc has well-defined ports, either provider port(PPort) or request port(RPort), the swc interface can either be a client-server interface or sender-receiver interface. with a PPort, the swc will impelment data generation; with a RPort, the swc will implement data read. communication manager (ComM), is a resource mananger to encapsulates communication related basic software modules. the actual bus states are controlled by the corresponding bus state manager, e.g. CAN/FlexRay/Lin bus. when ComM request a specific commmunication mode from the state manager, it will map the communication mode to a special bus state. network management modules (NM) works in bus-sleep mode and only support broadcast communication. diagnostic communication manager(DCM), a common API for diagnostic services. CAN driver performs the hardware access and provides a hardware-independent API to upper layers; it can access hardware resources and converts the given information for transmission into a hardware specic format and triggers the transmission. runtime environment(RTE)between basic software to upper application softwares, I think it’s mostly vfb. application software componentsfor now, e.g. ADAS, traditional EE.]]></content>
      <tags>
        <tag>AUTOSAR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[play with ros]]></title>
    <url>%2F2019%2F07%2F13%2Fplay-with-ros%2F</url>
    <content type="text"><![CDATA[ros filesystem toolsfirst check ROS_PACKAGE_PATH, where defines all ROS packages that are within the directories. 1234567891011rospack find [package-name]rospack list roscd [package-name]``` take an example, to locate `rosbridge_websocket.launch` ```shell rospack find rosbridge* #rosbridge_serverroscd rosbridge_servercd launch another tool to view ros-launch: roslaunch-logs write a .launch filelaunch files, which uses XML format, usually make a directory named “launch” inside the workspace to organize all launch files, and it provides a convenient way to start up multiple nodes and master, it processs in a depth-first tarversal order. usually launch files can be put a launch folder under a ros node project. 123roslaunch package_name launch_file#or roslaunch /path/to/launch_file an sample launch file: 123&lt;launch&gt; &lt;node pkg="package_name" type=" " name=" " output=" " args=" " /&gt;&lt;/launch&gt; args can define either env variables or a command. node/type There must be a corresponding executable with the same name. rvizrviz can help to playback sensor rosbag at lab. and also the visualization tool in algorithm/simulation development. someone(at 2014) said Google’s self-driving simulation has used rviz: a sample with rviz to visualize rosbag info: 12345# terminal 1 roscore # terminal 2 rosbag play kitti.bag -l rosrun rviz rviz -f kitti-velodyne rosbagthe sensor ros node will collect data in rosbag during physical or vitual test, then playback rosbag to develop or verify the sensing algorithms. or use to build simulation scene. a few common commands, and also rosbag support interactive C++/Python APIs. 1234rosbag record #use to write a bag file wit contents on the specified topicsrosbag info #display the contents of bag files rosbag play #play back bag file in a time-synchronized fashion kitti datasetare we ready for autonoous driving? – the KITTI vision benchmark suite, which is a famous test dataset in self-driving sensing, prediction also mapping and SLAM algorithm development. there are a few benchmars includes: stereo, basically rebuild the 3D object from multi 2D images. optical flow, used to detect object movement(speed, direction) scene flow, include other 3D env info, and objects from optical flow depth visual odometry object detection object tracking road/lane detection semantic evaluation catkin packagecatkin is ros package build/manage tool. mkdir -p ~/catkin_ws/src cd ~/catkin_ws/src catkin_create_pgk demo std_msgs rviz cd demo mkdir launch cat "&lt;launch&gt; &lt;node name="demo" type="rviz" -d="ls `pwd`" /&gt; &lt;/launch&gt; " &gt; demo.launch cd ~/catkin_ws catkin_make --pkg demo # add catkin_ws to cat "export ROS_PACKAGE_PATH=$ROS_PACKAGE_PATH:/path/to/catkin_ws/" &gt;&gt; ~/.bashrc]]></content>
      <tags>
        <tag>ros</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[play with Docker swarm/compose]]></title>
    <url>%2F2019%2F07%2F12%2Fplay-with-Docker-swarm-compose%2F</url>
    <content type="text"><![CDATA[Docker swarmdocker swarm is Docker nature cluster manager, with built-in DNS service found mechanism, and load-balancing mechanism. compare to k8s, is a light-weight and easy goers. create a swarm cluster12345678910111213141516export MASTER_IP=192.168.0.1docker swarm init --advertise-addr $&#123;MASTER_IP&#125; --name masterdocker swarm join --token tokens $&#123;MASTER_IP&#125; --name worker1 docker node ls # demote/ promote nodeID as managerdocker node demote/promote nodeID# rm node docker node rm worker1# stop swarm mode docker swarm leave create service123456789docker service create service-name # scale servicedocker service scael SERVICE=replicas docker service rm docker service inspect create overlay network12345docker network create --subnet=192.168.0.0/24 -d overlay ppss-netdocker network rmdocker network connect network-name docker-node in swarm mode, there are three network created by default: bridge0, the default network docker_gwbridge, local bridge used to connect containers hosted in the same host ingress, is a overlay network used in the swarm cluster however, in swarm mode, the default network for service is bridge, to across physical host, services need go through overlay network. load balancingIngress load balancingexpose Docker service to external network env Internal load balancingswarm mode has build-in DNS Docker composedocker compose is a manage/build tool to create application, which combine a bunch of micro-services, each of which can be ran as a Docker container. the docker-compose.yml configure file has to include each micro-service Dockerfile, and the application running scripts. service startup orderthe services defined in docker-compose.yml is not necessary depended to each other, so each serice can up individually, but of course they can has based on each other. docker-compose.ymlbest practice build path to Dockerfile, can be absolute path or relative (to .yml) path. Compose will buid the image based on contextsub-choice under build, point to the Dockerfile image the image will be used, if not locally, will pull from hub (vs Dockerfile) containe_name volumes path to attached volumes, in the format HOST:CONTAINER[:access mode] network_mode same as docker run --network init privileged command override launch command when service contianer start environment set env variables, in the format ENV:valueif only ENV, the value will be derived from host machine runtime: nvidia to suuport nvidia-docker e.g. 1234567nvsmi: image: ubuntu:16.04 runtime: nvidia environment: - NVIDIA VISIBLE DEVICES=all command: nvidia-smi stdin_open std io aviable tty virtual terminal sample yml from projecta sample yml for web app: 123456789services: web: build: . links: - "db: database" db: image: postgres a sample yml for general app CI: 12345678910111213141516171819202122232425262728services: build: build: context: ./Dockerfile image: docker-image container_name: build_app volumes: - ./build_scripts: /root/build commands: /root/build/build.sh run: context: ./Dockerfile image: docker-iamge container_name: run_app volumes: - ./run_scripts:/root/run environments: -DISPLAY -ROS_MASTER network_mode: host runtime: nvidia command: /root/run/run.sh test: #TODO docker-machinedocker-machine is a tool to build virtial host when hosted in one physical host or among multi-physical hosts. ros-dockerin self-driving software stack, ros is often used. and there is a need to deploy ros in docker. next time.]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[play with Docker]]></title>
    <url>%2F2019%2F07%2F12%2Fplay-with-Docker%2F</url>
    <content type="text"><![CDATA[Docker networking bridge: the default network driver, only in standalone containers; best when have multiple containers to communicate on the same host machine. host: for standalone containers, the Docker host use the host machine’s networking directly; best when need no isolated from the host machine. overlay: connect multi Docker containers, enable swarm services to communicate with each other, no OS-level routing; best when need containers running on different host machines to communicate. none: disable all networking the network base is open source project libnetwork containers can communicate through hostname, or through DNS(the now Docker engine has default built-in DNS server), but in early days, can use external dns, e.g. Blowb to host docker images, either pull to Docker Hub, or to create a private cloud by ownCloud, or docker save/export tools: 1234docker save -o /path/to/save/file image | gzipscp *.tar.gz remote_user@remote_hostnamedocker load *.tar.gz running GUI apps in Dockerthere is a benchmark GUI/openGL test in Linux glxgears. to test the host machine support ssh or container based apps, we can first do the following test: 1234567891011121314151617181920sudo apt-get install mesa-utilsglxinfo glxgears``` Docker by default is bash/text based, but `nvidia-docker` is a gui-supported Docker engine, which requires Nvidia OpenGL drivers and Nvidia Gpus of course. since the gpu hardware version and the docker engine version, please check the compatability at first. ## remote hosted apps * configure master and worker nodes communication by setting IP address in the same domain, and setting the master node IP address as the gateway IP address for all worker nodes, basically the master node will work as the swticher.* install xserver-common, xserver-utils, as Ubuntu by deafult doesn't have X-server. ```shellmaster:~/ ssh -X user@workerworker:~/ DISPLAY=:0worker:~/ ./gui_app for docker containers, there is also authority property need take care. Dockfilewhen the Docker container starts, usually we want to auto start a shell process, so by CMD or ENTRYPOINT defined in the Dockfile. for the base images, e.g. Ubuntu, busybox, are used as the base for upper applications, usually will use CMD at the end of Dockerfile; but if the Docker container is specially for a certain application, it’s usually using ENTRYPOINT. the last line of Ubuntu 16.04 Dockerfile: CMD &apos;/bin/bash&apos; usually in one Dockerfile, there is only one CMD or ENTRYPOINT, and it’s better written in exec format: CMD [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] since in shell format: CMD exectuable, param1, param2 Docker will trigger /bin/sh first by default, if not define a shell. and this shell is always the first process in this Docker container, which sometimes is not what we expected. when using both CMD and ENTRYPOINT in one Dockfile, the output will looks like append/pipe CMD command after ENTRYPOINT command. a sample of Dockerfile: 123456789FROM &lt;image&gt;:[&lt;tag&gt;] [AS &lt;name&gt;]ADD [--chown=&lt;user&gt;:&lt;group&gt;] &lt;src&gt; ... &lt;dest&gt;ADD [" ", ... " "]COPY &lt;src&gt; ... &lt;dest&gt;RUN &lt;command&gt;VOLUME /mount/nameCMD ["executable", "param1", "param2"]CMD command par1 par2 from docker image to dockerfilewe can easily pull images from hub, but when we try to build some images directly, there is also way to get Dockfile from existing docker image: dfimage 1sudo docker pull chenzj/dfimage mount host volume to containereither we can define in Dockerfile by VOLUME, which is create a volume name in the base image, or during runtime, docker run -v /host/volume/path:/container/volume/path, to bind a host volume to current container.]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[where are you in next 5 years 8]]></title>
    <url>%2F2019%2F07%2F09%2Fwhere-are-you-in-next-5-years-8%2F</url>
    <content type="text"><![CDATA[工薪和自由职业者/创业/老板，退休以后会有什么不同。也许对于工薪阶层，退休的生活也会如上班的：说不上的无奈感，没有痛快酣畅的体验过人生 — 离开了根本不享受的工作，也谈不上享受生活。就像《肖生克的救赎》被放出去的老头，离开了监狱，也融不进社会了。 有个表哥创业了，做了一个健身品牌，小有所成，进入持续创业，第二份是乡政企业办公软件。两份业很不一样。甚至表哥说，这份业估计一年后就结束，生活还在继续，下一份业在哪里，现在根本想不到，但是也不担心没有。 认识一个姑娘，88年，澳洲留学读了两年mba就12年回国，用我的眼光看，之后就没正紧上过班，走走玩玩也在这些年学了瑜伽，今年(2019年)开了一家店，32岁了过的跟20岁出头的姑娘一样，到底是没心没肺，还是把生活过成了别人羡慕的样子。她并不符合我的价值观，但是难道这样的人生不值得吗？ 年初在深圳，被这里的年轻人着实震惊了。大公司（华为）加班到凌晨，年轻的生命就像路旁的热带植被在绽放和燃烧。只是大表哥说了句，是被洗脑了。 老妈在好几个城市工作做，做保姆、帮厨、家政，走到哪里都得到顾客的喜欢，走到哪里都可以有饭吃，根本不担心没有技能，找不到工作。像一个自由职业者。 打工和创业当老板的mindset，似乎是本质的不同。 创业者/自由职业者，总会有出路，生活处处都通达。 对比下，打工者的心态。就是处处被堵，操的心一点不少，把脑袋削尖了跟黑压压的长江后浪推前浪的年轻人比拼，担心技术上比不过行家，担心项目被各种原因取消了，担心行业遇冷，担心被老板穿小鞋，担心30岁还没有不可替代的核心，担心35岁要开始讨生活了，担心工资比年轻人高容易被开，也开始担心身体健康、家人健康等等。 采取的解决办法也是围绕着这些压力了，人生没有朝向，谈不上洒脱和享受。 生活不止眼前的苟且，打工者真是委屈了心，把路走窄了，反而觉得这是唯一的出路。打工解决不了焦虑，必须转变。这世上，除了生死，都是小事。所以无所谓待业；找不到真正实现价值的事业，宁可像无业游民一样活着。 民企打工没有完善的制度，好的方面就是可以立山头，只要说动了领导，技术上可以大胆尝试，当然并不一定能得到支撑。 没有稳定的做产品的氛围，所以即便立了项目，也不一定能看到项目落地。对产品开发人员，就是不利于积淀。 这样的氛围下，见风使舵就是生存哲学。 可以联想到更广大的中小民企，更缺乏完善的产品流程和考评体系，老板一个人的话语权太大，打工者基本没有话语权。 出来自己干的人，哪些不同的品质？首先，压力的来源不该是跟成千上万人挤独木桥。一切可以明确定义的职位，比如，程序员、会计、工程师、个体网商，都挤满了人。而一旦陷入了这种思维，思考就会局限在削尖脑袋挤到这个行业/职位的头部。付出的代价/成本非常不成比例。简言之，就是洗脑了。 打工只适合初期的资本积累。所以，选择做舞蹈老师、瑜伽教练，咖啡馆老板的，慢慢都会生活和工作双赢，退休了也不愁不知道怎么经营生活。而单一大公司打工的，可能初期会在工作上得意，但是慢慢的生活和工作都会失去，而且退休了根本不会打理生活。 这些愿意出来自己干的人，更珍惜生活吧。所以不能接受将精力埋没在日复一日的工作中。这些自己出来干的人，都是被逼，当初没机会选择有保险的工作，只能自己走出一条道儿来。 刻意要避免打工，估计也是自找麻烦。所以平常心。 做技术的氛围到底什么环境/氛围适合做技术？北美的工作环境，相比国内，算是无忧无虑了，虽然有讨厌的印度人，但是如果有心总是有钱有时间捣鼓技术。不过反而，普通人在这样的环境下，是没有表现很强的科研热情。国内相比，待遇，工作的可爱度低，周围的大牛少了，但是反倒人因为生活和人的竞争感，反而会想多学点。 另一方面，国内的资本家似乎更缺乏对行业的敬畏心，资本家对这个行业就是个格外挣钱工具的心理，当然不会真正给这个行业带来真正伟大的技术推动。比如，当宝能系，恒大都砸出一叠钱说要造车，网上大张旗鼓的招聘，按照自动驾驶的各个模块：感知算法，运动规划，决策控制，地图定位开始招人的时候。一方面是哭笑不得，一方面是无奈，觉得身为工程师只是个棋子罢了，被一个工具/算法/模块给定义了。 相比这些只有钱的资本家的嘴脸，虽然汽车厂背后也站着资本家，似乎对汽车行业本身也更关心。当然，在中国，总是要面对钱，落地的现实。美国人可以谈vision, 3，5年不出产品，中国的资本市场基本不允许出现。所以，即使在国内的车厂做技术/研究，也是被量产推着。所以，整体氛围是浮躁，也就没办法专心下去]]></content>
  </entry>
  <entry>
    <title><![CDATA[computing chips in AV]]></title>
    <url>%2F2019%2F06%2F23%2Fcomputing-chips-in-AV%2F</url>
    <content type="text"><![CDATA[chips requirements in vehicleBosch : BMW: the next-generation vehicle EE platform can be easily modulized based on the topology of network composed by domain controllers and in-vehicle Ethernets. take an example with Singulato iS6, which has five domains: smart driving, powertrain, chassis, smart body, smart seat. each domain need support by a domain controller unit(DCU), the core of which is a powerful computing chip, which is usually more powerful than traditional ECUs. in average, L2 requires computing power about 10TOPS , L3 needs 60TOPS, L4 needs 100TOPS. computing chips product MDC600 Driver PX Pegasus EyeQ 4 BlueBox R-car H3 Journey2.0 Huawei Nvidia Mobileye NXP Renesas Horizon TOPS 352 320 2.5 10 main cores 8 * 晟腾310 16 ARM VMP LS2084A 4*Arm/A57 BPU2.0 other cores Ascend 310 2 TensorCore GPU S32V234 4*Arm/A53 FPGA AV-level L3+ L5 L3 L4(target) IVI L3 Camera support 16 10 8 8 8 4 Lidar support 8 6 function safety ASIL-D ASIL-B ASIL-D ASIL-D(target) ASIL-B ASIL-B products timelineTier1s/Tier2s timeline 2018 2019 2020 2021 Aptive level3 level4 Bosch level2 level3 level4 Conti level2 level3+ Autoliv level2 level3 level4 Intel level3 level4+ Nvidia level3 level4+ Chinese OEMs timeline 2019 2020 2021 2022+ changan level4 FAW level4 GAC level3 level5 Geely level3 level5 GWM level3 level4 SAIC level3 xiaoPeng level3 WeiMa level3 Nio level2 level4+ global OEMs timeline 2018 2019 2020 2021 2022+ Ford level2 level4 GM level2 level4 Fiat-Crysler level3+ Audi level2 level4 Mercedze level2 level3 level4 Toyota level2 level3 level4 Honda level3 level4 referenceMatrix 1.0 the five chip vendors from GPU to ASIC hauwei and the others global chips vendors L4 AI chips the evolution of EyeQ Mobileye tech NXP bluebox R-car H3 soc NXP function safety horizontal AI matrix2.0 cars, mobility, chip-to-city design and the Iphone4 2018-2019 汽车域控制器产业研究报告 汽车电子演化 Global L3 self-driving vehicle market insights 2019 self-driving car research report]]></content>
  </entry>
  <entry>
    <title><![CDATA[Lidar in AV]]></title>
    <url>%2F2019%2F06%2F22%2FLidar-in-AV%2F</url>
    <content type="text"><![CDATA[science, religion, music, universe as well as other sources of beauty, are what we humans should look for. – zj operational theorya pulse of light is emitted and the precise time is recorded. the reflection of that pulse is detected and the precise time is recorded. using the constant speed of light and the delay can convert into distance, with the known position and orientation of the sensor, the xyz position of the reflective surface can be calculated. components laser scanner/emitter and laser detector high-precision clock GPS and GPS ground station record xyz of the scanner IMU record angular orientation of the scanner field of view(FOV)azimuth with fixed vertical angle resolution, the neighboring laser emmiter-detector pair will create concentric circle, the distance between two neighboring concentric circle will grow with the distance from detected objects to Lidar. light source950nm wavelength producer is Si-based, which makes it cheaper than 1550nm, the InGaAs based, making it safer to human eyes as 950nm can burn retina and powerful. 1550nm is easier to be absorbed by water than 950nm, which makes it performance better in rainy days. the laser source emiss lines of pulse every frame, and a few photon return back to Photodetector(光电探测器), there are lots of env photons(noise), we can use narrow-band-filter to tick off some env photons, but not all of them, since the solar radiation is in the range from 905nm 50 1550nm. solid-statethere are two ways ongoing: MEMS based, phased array tech(相位阵列）. MEMS tech is using a micro scaning mirror, either rotate or vibrate to control laser direction. the drawback of micro-mirror is the in the process of relection, lots of laser energy is lost. phased array tech(Quanergy) integerated multi micro laser emission into one socket to control laser direction. and the drawback at this moment is the short detection distance. the traditional mechanical design Lidar(Velodye) usually has multi light emitters as well as multi corresponded light detectors. while SS-Lidar depends only on one single light emitter and the scanning mirror to control emission direction, which makes it cheaper. for example, each pair of mechanical emitter-detector cost 200 us dollar, so a 64 lines product will cost about 12800 us dollar, compared to MEMS socket about 200 us dollar each. detection distance(dd) &amp; angle resolution(ar)detection distacne with 10 % reflectivity vertical angle range(var) vertical angle resolution(va_res) company product dd(m) var va_res channels Hesai Pandar40 200 23&deg; 0.33&deg; 40 Robo sense RS-Lidar-32 200 40&deg; 0.33&deg; 32 Velodyne HDL-32e 100 41.3&deg; 32 Quanergy M8 150 20&deg; 32 Ibeo NSH_32 80 16&deg; 0.2&deg; 32 InnoVusion Cheetah 200 40&deg; 0.13&deg; 300 env effectswhat about weather effects? e.g. snow, dust, rain; what about env effects? e.g. temperature, system vibration. fusion with camerathe speed of productivizationwhen I first heard about InnoVusion, the founder Bao Junwei who was working at Baidu AI, then had the idea to produce Lidar around 2015, then he left Baidu and started InnoVusion, at the end of 2016, their first product Cheetah was born. this process is really speedy, one thought is the drive force either by capital market or industry needs is becoming so fast that every good chance from idea to product is becoming shorter in time; the other thought, only these highly effective persons will survive in this fast-iteration world. some other founders stories are here : the AI masters who left from Baidu referenceIbeo Next 3D SS-Lidar Innovusion Cheetah Lidar Velodyne HDL_32e product manual]]></content>
  </entry>
  <entry>
    <title><![CDATA[principles of GNSS positioning]]></title>
    <url>%2F2019%2F06%2F17%2Fprinciples-of-GNSS-positioning%2F</url>
    <content type="text"><![CDATA[novatel introduction GNSS architecturea) space segment the GNSS satellites, each of which broadcasts a signal that identifies ti and provides its time, orbit and status. b) control segment a ground-based network of master control stations, data uploading stations adn monitor stations. in case of GPS, 2 master control stations(one primary and one backup), 4 data uploading stations, and 16 monitor stations c) user segment the user equipment that process the received signals. GNSS propagationthe layer of atmoshpere that most influcences the transmission of GPS signals is the ionosphere(电离层), ionoshperic delays are frequency dependent; and the other layer is troposphere(平流层), whose delay is a function of local temperature, pressure and relative humidity. some singal energy is reflected on the way to the receiver, called “multipath propagation”. Antennaeach GNSS constellation has its own signal frequencies and bandwidths, an antenan must cover the signal frequencies and bandwidth. antenna gain is defined as the relative measure of an antenna’s ability to direct or concentrate radio frequency energy in a particular direction or pattern. A minimum gain is required to achieve a minimum carrier : power-noise-ratio to track GNSS satellites. GNSS error sourcescontributing sources error range satellite clocks +- 2m orbit errors +-2.5m inospheric delays +-5m tropospheric delays +-0.5m receiver noies +-0.3m multi path +-1m Resolving errorsmulti-constellation &amp; multi-frequencymulti-frequency is the most effective way to remove ionospheric error, by comparing the delays of two GNSS signals, L1 &amp; L2, the receiver can correct for the impact of ionospheric errors. multi-constellation has benefits: reduce signal acquisition time, improve position and time accuracy. D-GNSSin differential GNSS(D-GNSS), the position of a fixed GNSS receiver, refered as a base station, which sends the atmospheric delay related errors to receivers, which incorporate the corrections into their positoin calculations. differential positiong requires a data link betwen the base station and rovers, if corrections need to be applied in real-time. and D-GNSS works very well with base station-to-rover separations of up to 10km. Real time kinematic(RTK)it uses measurements of the phase of the signal’s carrier wave, in addition to the information content of the signal and relies on a single fixed reference station to provide real-time corrections, up to centimetre-level accuracy. the range to a satellite is calculated by multiplying the carrier wavelength times the number of whole cycles between the satellite and the rover and adding the phase difference. the results in an error equal to the error in the estimated number of cycles times the wavelength, so-called integer ambiguity search, which is 19cm for L1 signal. Precise Point Positioning(PPP)PPP solution depends on GNSS satellite clock and orbit corrections, generated from a network of global reference stations. GNSS + IMUthe external reference can quite effectively be provided by GNSS, and GNSS provides an absolute set of coordinates that can be used as the initial start point, as well, GNSS provides continuous positions adn velocities thereafter which are used to update the IMU/INS filter estimates. for additional combined sensors, such as odometers, cameras vision. challenges of GNSS in AVtalk from iMorpheus.ai 1) antenna 2) multipath mitigation 3) multi-band, multi-constellation signals 4) integrated navigation (camera )]]></content>
  </entry>
  <entry>
    <title><![CDATA[Manhanton SC review]]></title>
    <url>%2F2019%2F06%2F12%2FManhanton-SC-review%2F</url>
    <content type="text"><![CDATA[conjunctionsthe seven conjunctions can used to connect two independent clauses: For, And, Nor, But, Or, Yet, So comma only cant connect two sentences; but can connect two independent clauses using a semicolon(;) semicolon is often followed by a transition expression, (however, therefore, in addition), but these expression are not conjunctions, so must use semicolons, not commas to join. noun modifiersin the format: prepositon, part participle, presetn participle without commas. put the noun and its modifier as close together as possible “ comma which” is a nonessential modifier relative pronounswhich, cant modify people, who/ whom, must modify people whose, can modify either people or things where, can modify a noun place, can’t modify a metaphorical place, such as situation, case … when, can modify a noun event or time Noun Modifier markersany -ing that are not verbs and not separated from the rest of the sentence by comma will either be a noun, or a modifier of another noun. any “comma -ing” is adverbial modifiers adverbial modifierin the format of: prepositional phrase, present participle with commas , past participle with commas the adverbial modifier must modify a certain verb or clause at a right position, not structurally closer to another verb or clause participle modifierswhen using particples, the information present earlier in the sentence leads to or results in the information presented later in the sentence . subordinatorssubordernate clause provides additional info about the main clause. common subordinatetor markers: although, before, unless, because, that, so that, if, yet, after, while, since, when. and using only one connected word per “connection” . . which vs -ingwhenever using which, must refer to a noun, can’t modify a whole clause. so if need to modify the whole clause, use an adverbial modifier(either -ing, or past) quantitycountable modifiers: many, few, fewer, fewest, number of , numerous uncountable modifiers: much, little, less, least, amount of , great more, enough, all works with both countable and uncountable. parallelismcomparable sentence parts must be structurally and logically similar comparisoncomparison are a subset of parallelism, which requires parallelism between two elements, but also require the two compared items are fundamentally the same type of thing like, unlike, as, than, as adj as, different from, in contrast to/with like vs aslike is used to compare nouns, pronouns or noun phrase, never put a clause or prepositional phrase after like. as can used to compare two clauses.]]></content>
  </entry>
  <entry>
    <title><![CDATA[how simulation helps for autonomous vehicle]]></title>
    <url>%2F2019%2F06%2F06%2Fhow-simulation-helps-for-autonomous-vehicle%2F</url>
    <content type="text"><![CDATA[prefarerecently joined another Chinese auto OEM, still work in simulation platform for autonomous vehicle. this experience is a different pratice from GM or Ford. at first, it’s the mindset update, once for a long time, or even take for granted that I was looking for to be treated greatly, like in NA companies, what the company can offer me. it’s about freedom and responsibility. To have the freedom/right to make a difference in project direction, contents, and methods to achieve sth, first prove that I can take it, which is responsibility. “ you can you up” that kind of phylosphy is very common and actually I’d love it in some point. beta versionthere are ways to make a virtual simulator. traditional tool vendors include Matlab/Simulink, Prescan, VTD, ANSYS e.t.c. and as the beta version of simulator tool-chain, they are common in most OEMs in China. while have to say, in China, cause most software tools, from engineering tools to modern HR managment tools to product managment tools are kind of in practice process in Chinese companies as I can see, rather than as a matured segment pluged-in the company’s DNA. e.g. the product development tools are pretty in trial and error, occasionaly some PLM vendors come to introduce their products. and actually except the CAD/CAE tools, which were introduced in China since 80’s, and which sounds matured right now, the simulation tools for autonomous vehicle is pretty new-things, from map, sensor to all kinds of internal algorithms. the beta version of simulation tools has its DNAs as well. first they are charged by license fee, which is a pretty old way like IBM days. not many modern Internet mobility companies live in this way anymore. secondly, they can’t update frequently, once or twice a year with a new version of the product is common, which maybe OK in CAD/CAE tools, since the engineering analysis process are pretty matured and few exceptional requirements jump out in daily work, and there update is sometime by the vendor itself, which is not a required update, maybe optimized the algorithms, maybe added a third-party function, and sometime is from the users conference, usually the tool vendor will organize this kind of user conference to group users together, one way to get some connections, as well as to get some end-user requirements to fix the next version product, which is also a drawback compared to open source tool community, in where the user is more active. thirdly, the beta version tools are excluding the users someway, who can’t actually manage the tool as part of its whole functions. assuming simulation will be the key to make a difference in future auto product, then this excluding is unacceptable. on the other side, if autonomous simulation tool are at most aided to develop new product, then it becomes a piece of chicken neck either to take or to throw. from a product view, what is the role of simulation tool or as a product itself, where it is blooming point alpha versionwhy simulation tool become a role in autonomous, except the traditional vendors, mostly coming from Internet companies, e.g. Google, Uber, Baidu, Tecent etc. they are pretty strong to make a software product, including the simulation software in auto industry. interesting, these Internet companines never go to make a CAD/CAE software, but they actually prefer to support the cloud/HPC infrastructures for CAD/CAE simulation. these big mobility companines make their simulation tools now and well cowork with their existing IT development methodlogy, software product ideas as well as all ICT infrastures, which make these tools sounds huge great, and which also make themselves as the core role, rather than the OEMs, few are free or not yet open-sourced. and there are a few popular open source simulation tools, e.g. Carla, Airsim, LG simulator e.t.c. I actually get a chance with Carla at GM research team, which even now still great, but does’t study deeper, and for me it is a great tool for AI training, which should be the key role of simulation tool in future, since L3 or below, really cares little environment info, and its rule-based decision has no need for a large chunk of virtual test. that’s also a difference between Chinese and NA team, NA teams have the trend to invest in new tech even without immeditaly money back, but Chinese companies would prefer in immediate invest. lg simulatorUnreal never tried, but Unity looks pretty easy to use, while still need sometime to get familiar with the editor and play with scenes. lg simulator has give a great reference to build the virtual city and make autonomous vehicles running. the real problem actually for most team is how to make this tool useful in product development. we can think about the tool strucutre itself, like cloud running features, implement measure/verified methods, and build scenes database as we can think about or from the sort of system engineering port. as for a L3 or below usage, the whole meaning of simualtion is not driven, but at most additional support of product development, as I can see. so what simulation for ? L4+, which leads to AI, or data driven product development! from this point, the simlation itself is not the core, but the AI, so think about that in career development.]]></content>
      <tags>
        <tag>simulation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[robust control theory]]></title>
    <url>%2F2019%2F05%2F10%2Frobust-control-theory%2F</url>
    <content type="text"><![CDATA[this is a review from cmu refer state variable methodany Nth order differential equation describing a control system could be reduced to N 1st order equations, these equations could be arranged in the form of matrix equations. define x as system state, y as output, u as input: modern control methods(ODEs) can handle multiple-input-multiple-outputs, and they can be optimized, and they allow to design performance and cost model. effects of uncertaintyobservabilitythe ability to observe all of the parameters or state variables in the system controllabilitythe ability to move a system from any given state to any desired state stabilitythe bounded response to any bounded input robust control theory might be stated as a worst-case analysis, to bound the uncertantiy. metircshow to model the behavior of the test system is one most difficult challenge in design a good control system. adaptive controlset up observers for each significant state variable. at each iteration loop, the system learns about the changes in the system parameters, and getting closer to the desired. while the method may suffer from convergence issues H2 or H-infinityH2 control seeks to bound the power gain of the system, H-infinity seeks to bound the energy gain of the system. gains in power or energy indicate operation of the system near a pole in the transfer function. parameter estimationby establishing boundaries in the frequency domain that cannot be crossed to maintain stability. Lyapanovthe only universal tech for assessing non-linear systems, the method focus on stability. Lyapanov functions are constructed, which are described as energy-like functions, to model the behavior of real system.]]></content>
      <tags>
        <tag>control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Carmaker 8.0]]></title>
    <url>%2F2019%2F05%2F10%2FCarmaker-8-0%2F</url>
    <content type="text"><![CDATA[this is from IPG open house Shang Hai scenario generation taskdata recordtracking vehicles, roads, tobstacles obj: lane, road, barries, GPS input, vehicle position/orientation, fixed ID, type the goal of recoding is for road building, which will be used in replay. road buildGPS input + lane mark info + vehicle location –&gt; vehicle trajectory replayrun config, input as tranversal and longitudial position traffic vehicle location, speed rearrangeinput as : traffic vehile info + ego info , list of traffic vehicle info traffic vehicle manage: 1) manuevor control: free move 2) spawn control: lati + longi --&gt; 23 cases 3) support external plugins + manuevor trigger Synthetic Scenariojunction assistant road type + traffic rules + scenario –&gt; support road topology modification support different envs: day of time, weather, scenario editor to support opendrive import standardizationPEGASUS + ASAM simulation standards roads, scenarios, simulation interfaces Opendrive –&gt; road topology opENScenario –&gt; maneuver &amp; anction abstract definitions Open simulation interface –&gt; interface developed for PEGASUS limitationspre-define route for vehicle ? the ego car has AI maneuvor ? Vitual Prototypeincluding gearbox loss mode, gas mode, through look-up table including hybrid powertrain architectures: automatic gearbox + parallel hybrid including powertrain masses(engine, tank, gearbox, battery, motor) including trailer data set generator including damping top mount Simulation testsupport:: ADAS/AD, POWERTRAIN, Vehicle Dynamics steering system visual casefor less steering will overall comfort and vehicle dynamics reference measurements(steering-in-loop simulator) -&gt; model parameter id + softare + ECU integration –&gt; parameterization &amp; validation -&gt; training how the steering system worksopen loop to get mechanical characteristics(stiffness, friction..) system performance with or without EPS 1) ideal(basis) model vs physical model how to cowok the physical model with autopilot control model ? test bedto support electrification, durability, balancing, driveability, powertain caillbratio, connected powertrain AI training with synthetic scenariodecion making trajectory planning image perception q: how to make sure AI robost ? –&gt; what CarMaker can do for AI? 1) obj annotation (vehicles, pedestrains) –&gt; auto annotation 2) semantic segmentation e.g. IPG Movier for auto semantic segmentation Q: what’s the hardware for ? Cloud &amp; CPU/GPU for Parallelizationq: how to parallel in docker ? 1) test case in each CPUs 2) even for single test run(with multi sensors, multi cars ) resources &amp; distribution CPU: vehile model, drivel model, envs, ideal sensors GPU; visual, camera RIS, radar ris, lidar rs Test run in prallelsensor setup(10 ultra, 5 Radar, 1 Lidar, 1 Camera) host pc (with test manager) + 4 virtual machines output: key figures, reports, statistics, queries open archi for scalable processing( on-premise and cloud) big data anaysis with DaSense by NorCom how it works ? external scheduler mananger, PBS HPC light to support local PC parallel new features in 8.0virtual test driving 8.0 simulink lib (through Simscape) Scenario Editor: vege geenration, animated 3D objs, new models(vehicles, trailers, trucks, bus, buildings, houses, street furniture, pedestrains) visulize road surfaces .. ipg movie fisheye distortion from external file new sensor models(Lidar RSI) q: what’s the difference of open source tool vs commericial ? Lidar RSIIdeal perfect world –&gt; ground truth HiFi –&gt; false positives &amp; negatives raw data –&gt; RSI supporting Lidar type: moving laser &amp; photot diode moving mirrors solid state flash input features : Laser beam, including custom beam pattern, Raytracing rays Scene Interaction, including atmoshpere attenuation, color or material or surface or transparent dependency detection, including threashold, multiple echoes per beam, separability output features: sending &amp; receiving direction of every beam light intensity of every beam time &amp; lenght of light pluse width number of interactions User Case : Nio Pilotby sun peng casesinter-city, parking, closed space, crowded space sensors: 3 front camera, 4 surround camera, 4 mm RADARS, 12 Ultra, 1 driver monitor camera higway pilot in June perception: camera, radar, ult, hd map, location planning : path planning, maneuvor decsion, system control cloud &amp; AI simulation usage FDS -&gt; cases -&gt; SIL platform –&gt; cases -&gt; regression test, abstraction &amp; instantiation ; scene reconsturction(in-house) / close loop SIL ; traffic model training(to do) integration -&gt; HIL what about vd ? –&gt; co-work with simulation and physical test, the cover percentage of simulation is about 80%, the left is from data platformupload nodes -&gt; cloud med usa API server -&gt; fleet mgmt log stash –&gt; elastic search –&gt; Kibana &amp; Admin (tensn and spark ) I think they are collecting data, and this data for scene building and simulation usage in future data visulazationHIL lane model simualtion on HIL fusion simulation on HIL automation testjenkins master –&gt; jenkins slave (agent IPG) –&gt; cloud goalsimulation server &lt;—&gt; data center parallel sim core + simulation monitor (data exchange service) data processing + labelling + case management + traffic model training (replay, SIL, REMODEL, Visuliazation )]]></content>
      <tags>
        <tag>simulation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PID control]]></title>
    <url>%2F2019%2F05%2F04%2FPID-control%2F</url>
    <content type="text"><![CDATA[closed loop systemset point is the desired or command value for the process variable. at any given moment, the difference between the process variable and the set point is used by the control system algorithm to determine the desired actuator output to drive the system. closed loop system, the process of reading sensors to provide constant feedback and calculating the desired actuator output is repeated continuously and at a fixed loop rate. control system performance is measured by applying a step fuction as the set point command variable, and then measuring the response of the process variable. rise time is the amount of time that the system takes to go from 10% to 90% of the steady-state/final. percent overshoot is the amout that the process variable overshoots the final value, expressed as a percentage of the final value. settling time is the time required for the process variable to settle to within a certain percentage(5%) of the finla value steady state error is the final difference between the process variable and set point disturbance rejection is the measure of how well the control system is able to overcome the effects of disturbances. often there is a disturbance in the system that affects the process variables or the measurements of these variables, it’s important to design a control system that performs satisfactorily during the worst case conditions. nonlinear system , in which the control parameters that produe a desired response at one operating point might not produce a satisfactory response at another operating point. deadtime is the delay between when a process variable changes, and when that change can be observed. loop cycle the interval of time between calls to a control system, system that change quickly or have complex behavior requires faster control loop rates. PID theoryproportional responseerror the difference between the set point and the process variable. the proportional gain(K_g) determins the ratio of the output response to the error signal. e.g. the error term has a magnitude of 10, and the K_g is 5, then the proportional response is 50. increasing K_g will increase the speed of the control system response, but if K_g is too large, the process variable will oscillate(why?) integral responsethe integral component sums the error term over time. the result is that even a small error term will cause the integral component to increase slowly. the integral response will continually increase unless the error is zero, so the effect is to driven the steady-state-zero to zero. integral windup when integral action saturates, still without the controller driving the error signal toward zero derivative responsethe response is portortional to the rate of change of the process variable. increasing the derivative time will cause the control sytem to react more strongly to changes in the error, and react more quickly. in practice, most control system use very small derivative time, since the derivative response is highly sensitive to noise. turningwhich is the process of setting the optimal gains of P, I, D to get an ideal response. trial and errorset I, D as zero, and increas P gain. Once P has been set to obtain a desired fast response, I starts to increase to stop the oscillatins to achieve a minimal steady state error. once P and I have been set, the D is increased untill the loop is acceptably quick to its set point. filteringassume a sinusoidla noise with frequency w, the direvative is: so in practice it’s necessary to limit the high frequency gain of the derivative term, either by adding a low pass filtering of the control signal, or implement the derivative term in a cut-off way. Udacity Self driving Car project 9 int main() { uWS::Hub h ; PID pid ; pid.Init(pinit, iinit, dinit); h.onMessage( // cte, the error { double diff = fabs(pid.p_error - cte) ; if( diff &gt; 0.1 &amp;&amp; diff &lt; 0.2) thr = 0.0; else if( diff &gt; 0.2 &amp;&amp; speed &gt; 30) thr = -0.2; pid.UpdateError(cte, dt); steer_value = -pid.TotalError(speed); } } void PID::UpdateError(double cte, double dt) { d_error = (cte - p_error) /dt ; p_error = cte ; i_error = integral(cte * dt); } double PID::TotalError(double speed) { return (Kp - 0.0032 * speed) * p_error + Ki * i_error + (Kd + 0.002 * speed) * d_error ; } in real self driving control system, usually divide as latitual and longitual control. and in different scenarios, there are plenty of PID controls. in the future will expose: highway autopilot, auto parking, urban L4 referencePID theory explained PID control from Caltech udacity pid control]]></content>
      <tags>
        <tag>control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apollo 2.0 Localization 源码]]></title>
    <url>%2F2019%2F05%2F03%2FApollo-2-0-Localization-%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[there are two localization methods: RTK and multi-sensor fusion(MSF). RTK using GPS and IMU inputs, MSF using GPS, IMU and Lidar sensor and HD map as inputs. the output is the localization estimated object instance. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657Status Localization::Start()&#123; localization_ = localization_factory_.CreateObject(cofig_.type()); localization_-&gt;Start(); return Status::OK();&#125;Status RTKLocalization::Start()&#123; AdapterManager::Init(FLAGS_rtk_adapter_config_file); timer_ = AdapterManager::CreateTimer(ros::Duration(duration), &amp;RTKLocalization::OnTimer, this); AdapterManager::GetGps(); AdapterManager::GetImu(); tf2_broadcaster_ = new tf2_ros::TransformBroadcaster() ; return Status::OK();&#125;void RTKLocalization::OnTimer(const ros::TimerEvent &amp;event)&#123; AdapterManager::Observe(); PublishLocalization(); RunWatchDog(); &#125;void RTKLocalization::PublishLocalization()&#123; LocalizationEstimate localization ; PrepareLocalizationMsg(&amp;localization); AdapterManager::PublishLocalization(localization); PublishPoseBroadcastTF(localization);&#125;void RTKLocalization::PrepareLocalizationMsg(LocalizationEstimate *localization)&#123; const auto &amp;gps_msg = AdapterManager::GetGps()-&gt;GetLatestObserved(); Imu imu_msg = AdapterManager::GetImu()-&gt;GetLatestObserved(); ComposeLocalizationMsg(gps_msg, imu_msg, localization);&#125;void RTKLocalization::ComposeLocalizationMsg(const localization::Gps&amp; gps_msg, const localization::Imu &amp;img_msg, LocalizationEstimate* localization)&#123; // add header // set measurement time // combine gps and imu auto mutable_pose = localization-&gt;mutable_pose(); if(gps_msg.has_localization())&#123; const auto &amp;pose = gps_msg.localization(); if(pose.has_position())&#123; // update mutable_pose &#125;; if(pose.has_orientation()) &#123; //update mutable orientation &#125;; if(pose.has_linear_velocity())&#123;&#125;; &#125; if(imu.has_linear_acceleration())&#123; //update mutable_pose acc &#125;; if(imu.has_angular_velocity())&#123;&#125;; if(imu.has_euler_angles())&#123;&#125;;&#125; MSFvehicle localization based on multi-sensor fusion 12345678910111213141516171819202122232425class MSFLocalization &#123; void InitParams(); void OnPointCloud(const sensor_msgs::PointCloud2 &amp;message); void OnRawImu(const drivers::gnss::Imu &amp;imu_msg); void OnGnssRtkObs(const EpochObservation &amp;raw_obs_msg); // ... void PublishPoseBroadcastTF(const LocalizationEstimate&amp; localization);&#125;Status MSFLocalization::Start()&#123; AdapterManager::Init(FLAGS_msf_adapter_config_file); Status &amp;&amp;status = Init(); AdapterManager::GetRawImu(); AdapterManager::ADDRawImuCallback(&amp;MSFLocalization::OnRawImu, this); AdapterManager::GetPointCloud(); AdapterManager::AddPointCloudCallback(&amp;MSFLocalization::OnPointCloud, this); // ... return Status::OK();&#125; there is addtional local_map module, which will be review later.]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apollo 2.0 Prediction源码]]></title>
    <url>%2F2019%2F05%2F03%2FApollo-2-0-Prediction%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[Predictionprediction inputs are: obstacles from perception module nad localization from localization module. outputs are obstacles will predicted trajectories. there are three classes in prediction modules: * container store input dat from subscribed channelds, e.g. perception obstacles, vehicle localization * evalutor predicts paths and speed separately for any given obstacles * predictor generate predicted trajectories for obstacles. e.g. lane sequence(obstacle moves following the lanes), free movement, regional moves(move in a possbile region) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// 模块入口APOLLO_MAIN(apollo::prediction::Prediction);Status Prediction::Init()&#123; predicition_conf_.Clear(); adapter_conf_.Clear();common::util::GetProtoFromFile(FLAGS_prediction_adapter_config_filename, &amp;adapter_conf_); //Initial managers AdapterManager::Init(adapter_conf_); ContainerManager::instance()-&gt;Init(adapter_conf_); EvaluatorManager::instance()-&gt;Init(prediction_conf_); PredictorManager::instance()-&gt;Init(prediction_conf_); AdapterManager::GetLocalization(); AdapterManager::GetPerceptionObstacles(); AdapterManger::AddPerceptionObstaclesCallback(&amp;Prediction::RunOnce, this); AdapterManger::AddLocalizationCallback(&amp;Prediction::OnLocalization, this);AdapterManger::AddPlanningCallback(&amp;Prediction::OnPlanning, this); return Status::OK();&#125;void Prediction::RunOnce(const PerceptionObstacles&amp; perception_obstacles)&#123; ObstaclesContainer* obstacles_container = dynamic_cast&lt;ObstaclesContainer*&gt;(ContainerManager::instance()-&gt;GetContainer(AdapterConfig::PERCEPTION_OBSTACLES)); obstacles_container-&gt;Insert(perception_obstacles); EvaluatorManager::instace()-&gt;Run(perception_obstacles); PredictorManager::instance()-&gt;Run(perception_obstacles); auto prediction_obstacles = PredictorManager::instance()-&gt;prediction_obstacles(); for(auto const&amp; prediction_obstacle : prediction_obstacles.prediction_obstalces())&#123; for(auto const&amp; trajectory:prediction_obstacle.trajectory()) &#123; for(auto const&amp; trajectory_point : trajectory.trajectory_point()) &#123; if(!IsValidTrajectoryPoint(trajectory_point))&#123; return ; &#125; &#125; &#125; Publish(&amp;prediction_obstacles); &#125;void Prediction::OnLocalization(const LocalizationEstimate&amp; localization, ObstaclesContainer* obstacles_container)void Prediction::OnPlanning(const planning::ADCTrajectory&amp; adc_trajectory, ADCTrajectoryContainer* adc_trajectory_container) Prediction interface has defined: RunOnce() and Pulish(). the Prediction class has defined listener’s callback: OnLocalization, OnPlanning, and Start(), Stop() and implemented interface functions. For both EvaluatorManager and PredictorManager, there are Init() and Run() functions, and there are bunch of apis from container class. Evaluator1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859void EvaluatorManager::Init(const PredictionConf&amp; config)&#123; //... switch(config.obstacle_type())&#123; case PerceptionObstacle::VEHICLE :&#123; vehicle_on_lane_evaluator_ = obstalce_conf.evaluator_type(); break; case PerceptionObstacle::BICYCLE:&#123; cyclist_on_lane_evaluator_ = obstlce_conf.evluator_type(); break; case PerceptionObstacle::PEDESTRAIN:&#123; break; &#125; case PerceptionObstacle::UNKNOWN:&#123; default_on_lane_evaluator_ = obstacle_conf.evalutor_type(); break; &#125; &#125;&#125;Evaluator* EvaluatorManager::Run(const perception::PerceptionObstacles&amp; perception_obstacles)&#123; ObstaclesContainer* container = dynamic_cast&lt;ObstaclesContainer*&gt;(); Evaluator* evaluator = nullptr ; for(const auto&amp; perception_obstacle : perception_obstacles.perception_obstalce()) &#123; int id = perception_obstalce.id(); Obstacle* obstacle = container-&gt;GetObstalce(id); switch(perception_obstalce.type()) &#123; case PerceptionObstacle::VEHICLE: &#123; if(obstacle-&gt;IsOnLane())&#123; evaluator = GetEvaluator(vehicle_on_lane_evaluator_); &#125; break; &#125; case PerceptionObstacle::BICYCLE:&#123; if(obstacle-&gt;IsOnLane())&#123; evaluator = GetEvaluator(cyclist_on_lane_evaluator_); &#125; break; &#125; // ... if(evaluator != nullptr) &#123; evaluator-&gt;Evaluate(obstacle); &#125; &#125; &#125;&#125; and there are a few different evaluators: (multilayer perception approach) MLP and RNN(deep neural network), both will discuss in details in future. Predictorpredictor will generate trajectories, a few apis: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172virtual void predictor::Predict(Obstacle* obstacle) = 0;void predictor::TrimTrajectories(const Obstacle, const ADCTrajectoryContainer* );static predictor::Trajectory GenerateTrajectory(const std::vector&lt;apollo::common::TrajectoryPoint&gt;&amp; points) ;void PredictorManager::Init(const PredictionConf&amp; config)&#123; // ... switch(obstacle_conf.obstacle_type()) &#123; case PerceptionObstacle::VEHICLE: &#123; if(obstacle_conf.obstacle_status() == ObstacleConfg::ON_LANE)&#123; vehicle_on_lane_predictor_ = obstacle_conf.predictor_type(); &#125;else if(obstacle_conf.obstacle_status() == ObstacleConf::OFF_LANE)&#123; vehicle_off_lane_predictor_ = obstacle_conf.predictor_type(); &#125; &#125; break; // ... &#125; void PredictorManager::Run(const PerceptionObstacles&amp; perception_obstacles)&#123; ObstaclesContainer* obstacles_container = dynamic_cast&lt;ObstaclesContainer*&gt;(AdapterConfig::PERCEPTION_OBSTACLES) ; ADCTrajectoryContainer *adc_trajectory_container = dynamic_cast&lt;ADCTrajectoryContainer*&gt;(AdapterConfig::PLANNING_TRAJECTORY); Predictor* predictor = nullptr ; for(const auto* perception_obstacle : perception_obstacles.perception_obstacle()) &#123; int id = perception_obstacle.id(); PredictionObstacle prediction_obstacle ; Obstacle* obstacle = obstacle_container-&gt;GetObstacle(id); if(obstacle != nullptr) &#123; switch(perception_obstacle.type()) &#123; case PerceptionObstacle::VEHICLE: &#123; if(obstacle-&gt;IsOnLane())&#123; predictor = GetPredictor(vehicle_on_lane_predictor_); &#125;else&#123; predictor = GetPredictor(vehicle_off_lane_predictor_); &#125; break; &#125; // ... &#125; if(predictor != nullptr) &#123; predictor-&gt;Predict(obstacle); if(obstacle-&gt;type() == PerceptionObstacle::VEHICLE)&#123; predictor-&gt;TrimTrajectories(obstacle, adc_trajectory_container); &#125; for(const auto&amp; trajectory : predictor-&gt;trajectories()) &#123; prediction_obstacle.add_trajectory()-&gt;CopyFrom(trajectory); &#125; &#125; &#125; prediction_obstacle.mutable_perception_obstacle()-&gt;CopyFrom(perception_obstacle); prediction_obstacles_.add_prediction_obstacle()-&gt;CopyFrom(prediction_obstacle); &#125; the predictor has a few types:: regional based, free move, lane sequence e.t.c, which defined as subclasses, will discuss more in future. Containercontainer manager class has a few subclass as adc_trajctory container, obstacles contianer and pose constainer, which should be discussed in details later. Adapter1234567891011121314151617181920212223242526272829// in adapter_manager.cc case AdapterConfig::LOCALIZATION: EnableLocalization(FLAGS_localization_topic, config);case AdatperConfig::PERCEPTION_OBSTACLES: EnablePerceptionObstacles(FLAGS_perception_obstacle_topic, config); // in messsage_adapters.h using PerceptionObstaclesAdapter = Adapter&lt;perception::PerceptionObstacles&gt;; using LocalizationAdapter = Adapter&lt;apollo::localization::LocalizationEstimate&gt;;// in adapter_manager.h static voi Enable##name()&#123; instance()-&gt;InternalEnable##name(topic_name, config);&#125;; static name##Adapter *Get##name()&#123; return instance()-&gt;InternaleGet##name();&#125;static void Add##name##Callback(name##Adapter::Callback callback)&#123; instance()-&gt;name##_-&gt;AddCallback(callback);&#125;template&lt;class T&gt; staic void Add##name##Callback(void(T:: *fp)(const name##Adapter::DataType &amp;data), T* obj)&#123; Add##name##Callback(std::bind(fp, obj, std::placeholders::_1));&#125;]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apollo2.0 Control 源码 (3)]]></title>
    <url>%2F2019%2F05%2F01%2FApollo2-0-Control-%E6%BA%90%E7%A0%81-3%2F</url>
    <content type="text"><![CDATA[the input of Apollo control module includes: chassis info, localization info, and planning info ,the output is steering angle, acc, throttle. 12345678910111213141516171819202122232425262728293031323334// 模块入口#define APOLLO_MAIN(APP) int main(int argc, char **argv) &#123; google::InitGoogleLogging(argv[0]); google::ParseCommandLineFlags(&amp;argc, &amp;argv, true); signal(SIGINT, apollo::common::apollo_app_sigint_handler); APP apollo_app_; ros::init(argc, argv, apollo_app_.Name()); apollo_app_.Spin(); //check previous blog(1) return 0; &#125; Status Control::Init()&#123; init_time_ = Clock::NowInSeconds(); common::util::GetProtoFromFile(FLAGS_control_conf_file, &amp;control_conf_); AdapterManager::Init(FLAGS_control_adapter_config_filename); common::monitor::MonitorLogBuffer buffer(&amp;monitor_logger_); controller_agent_.Init(&amp;control_conf_); AdapterManager::GetLocalization(); AdapterManager::GetChassis(); AdapterManager::GetPlanning(); AdpaterManager::GetControlCommand(); AdapterManager::GetMonitor(); AdapterManager::AddMonitorCallback(&amp;Control::OnMonitor, this); return Status::OK();&#125; conf_file: /modules/control/conf/lincoln.pd/txt, which is derieved from /modules/calibration/data/mkz8/calibration_table.pd.txt the topics in control modules are : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125config &#123; type: LOCALIZATION mode: RECEIVE_ONLY&#125;config &#123; type: PLANNING_TRAJECTORY mode: RECEIVE_ONLY&#125;config &#123; type: CHASSIS mode: RECEIVE_ONLY&#125;config &#123; type: CONTROL_COMMAND mode: PUBLISH_ONLY&#125;config &#123; type: MONITOR mode: DUPLEX&#125; ``` basically, control module will receive topic about /localization, /planning, /chassis, and publish /control_command the controller_agent is an interface class, so can support user defined controller algorithms, which only need to configure through `control_conf` ```cStatus ControllerAgent::Init(const ControlConf* control_conf)&#123; RegisterControllers(control_conf); InitializeConf(control_conf); for(auto &amp;controller : controller_list_) &#123; if(controller == NULL || !controller-&gt;Init(control_conf_).ok()) &#123; return Status(ErrorCode); &#125; &#125; return Status::OK();&#125;void ControllerAgent::RegisterControllers(const ControlConf *control_conf)&#123; for(auto active_controller : control_conf-&gt;active_controllers()) &#123; switch(active_controller)&#123; case ControlConf::MPC_CONTROLLER: controller_factory_.Register( ControlConf::MPC_CONTROLLER, []()-&gt;Controller * &#123;return new MPCController();&#125;); break; //case LAT_CONTROLLER //case LON_CONTROLLER &#125; &#125; &#125; Status Control::Start()&#123; //sleep for advertised channel to ready std::this_thread::sleep_for(std::chrono::millisecons(1000)); timer_ = AdapterManager::CreateTimer(ros::Duration(control_conf_.control_period()), &amp;Control::OnTimer, this); common::monitor::MonitorLogBuffer buffer(&amp;monitor_logger_); return Status::OK(); &#125; void Control::OnTimer(const ros::TimerEvent &amp;) &#123; double start_timestamp = Clock::NowInSeconds(); ControlCommand control_command ; Status status = ProduceControlCommand(&amp;control_command); double end_timestamp = Clock::NowInSeconds(); status.Save(control_command.mutable_header()-&gt;mutable_status()); SendCmd(&amp;control_command); &#125; Status Control::ProduceControlCommand(ControlCommand *control_command) &#123; Status status = CheckInput(); Status status_ts = CheckTimestamp(); Status status_compute = controller_agent_.ComputeControlCommand( &amp;localization_, &amp;chassis_, &amp;trajectory_, control_command); return status; &#125; Status ControllerAgent::ComputeControlCommand( const localization::LocalizationEstimate *localization, cosnt canbus::Chassis *chassis, const planning::ADCTrajectory *trajectory, control::ControlCommand *cmd)&#123; for(auto &amp;controller : controller_list_) &#123; controller-&gt;ComputeControlCommand(localization, chassis, trajectory, cmd) ; &#125; return status::OK(); &#125; ControllerAgent::ComputeCmd() is the interface, the real control cmd will be computed inside each specified controller modes. there are few controller modes: MPC, Lattice e.g in Apollo. in next few days continue work on localization, prediction, and decision modules in Apollo 2.0]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apollo2.0 Routing源码(2)]]></title>
    <url>%2F2019%2F05%2F01%2FApollo2-0-Routing%E6%BA%90%E7%A0%81-2%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// 模块入口APOLLO_MAIN(apollo::routing::Routing) int main(int argc, char** argv)&#123; google::InitGoogleLogging(arg[0]) ; google::ParseCommandLineFlags(&amp;argc, &amp;argv, true); signal(SIGINT, apollo::common::apollo_app_sigint_handler); apollo::routing::Routing apollo_app_ ; ros::init(argc, argv, apollo_app_.Name()); apollo_app_.Spin(); return 0;&#125;``` `appollo_app_.Spin()` can be found [here](https://zjli2013.github.io/2019/04/28/apollo-planning-源码/)```c apollo::common:Status Routing::Init()&#123; const auto routing_map_file = apollo::hdmap::RoutingMapFile() ; navigator_ptr_.reset(new Navigator(routing_map_file)) ; common::util::GetProtoFromFile(FLAGS_routing_conf_file, &amp;routing_conf_); hdmap_ = apollo::hdmap::HDMapUtil::BaseMapPtr(); AdapterManager::Init(FLAGS_routing_adapter_config_filename); AdapterManager::AddRoutingRequestCallback(&amp;Routing::OnRoutingRequest, this); return apollo::common::Status::OK();&#125;/* DEFINE_string(routing_adapter_config_filename, "modules/routing/conf/adapter.conf", "the adapter config filename")*/void AdapterManager::Init()&#123; //... for(const auto &amp;config :: configs.config()) &#123; case AdapterConfig::ROUTING_REQUEST : EnableRoutingRequest(FLAGS_routing_request_topic, config); break; case AdapterConfig::ROUTING_RESPONSE: EnableRoutingResponse(FLAGS_routing_response_topic, config); break; case AdapterConfig::ROUTING_MONITOR: EnableMonitor(FLAGS_monitor_topic, config); break; //... &#125;&#125; where define EnableRoutingRequest() ? 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//apollo/modules/common/adapters/adapter_manager.h #define REGISTER_ADAPTER(name) static void Enable##name(const std::string &amp;topic_name, const AdapterConfig &amp;config) &#123; instance()-&gt;InternalEnable##name(topic_name, config); &#125; template&lt;class T&gt; static void Add##name##Callback( void(T::*fp)(const name##Adapter::DataType &amp;data), T *obj)&#123; Add##name##Callback(std::bind(fp, obj, std::placeholders::_1)); &#125; tempalate&lt;class T&gt; static void Add##name##Callback(void (T::*fp)(const name##Adapter::DataType &amp;data))&#123; Add##name##Callback(fp); &#125; // apollo/modules/common/adapters/message_adapters.husing RoutingRequestAdapter = Adapter&lt;routing::RoutingRequest&gt; ;using RoutingResponseAdapter = Adapter&lt;routing::RoutingResponse&gt;;// apollo/moduels/common/adapters/adapter.h typedef typename std::function&lt;void(const D&amp;)&gt; Callback ;// apollo/modules/common/adapters/adapter.h template &lt;class D&gt; void Adapter&lt;D&gt;:OnReceive(const D&amp; message)&#123; last_receive_time_ = apollo::common::time::Clock::NowInSeconds(); EnqueueData(message); FireCallbacks(message);&#125;void AddCallback(Callback callback)&#123; receive_callbacks_.push_back(callback);&#125;tempplate&lt;class D&gt; void Adapter&lt;D&gt;::FireCallbacks(const D&amp; data)&#123; for(const auto&amp; callback : receive_callbacks_) &#123; callback(data); &#125;&#125; Dreamview and Planning modules have message publish API. e.g. 1234567891011121314151617SimulationWorldUpdater::(WebSocketHandler *websocket, SimControl *sim_control, const MapSerivce *map_service, bool routing_from_file) : sim_world_service_(map_service, routing_from_file), map_service_(map_service), websocket_(websocket),sim_control_(sim_control)&#123; // ... websocket_-&gt;RegisterMessageHandler("SendRoutingRequest", [this][cosnt Json &amp;json, WebSocketHandler::Connection *conn) &#123; RoutingRequest routing_request, bool succed = ConstructRoutingRequest(json, &amp;routing_request); if(succed)&#123; AdapterManager::FillRoutingRequestHeader(FLAGS_dreamview_module_name, &amp;routing_request); AdapterManager::PublishRoutingRequest(routing_request); &#125; &#125; ApapterManager class is used to make sure the connection among each module in ROS message type. Routing class has GPS and IMU input and generate routing and velocity info as output. Navigator class is using A start algorthm with hd map, start point and end point as input to generate a navigation route.]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里(7)]]></title>
    <url>%2F2019%2F04%2F30%2F%E6%9C%AA%E6%9D%A55%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C-7%2F</url>
    <content type="text"><![CDATA[噱头团队必须在有危机意识的企业中成长。靠投资活的团队，往往只展现其繁荣的一面，对内对外；而实际公司的产品、市场定位，甚至内部员工都不知情。对员工缺乏诚实，对市场客户也不会诚实。 民企文化近距离观察了一家民企（长城汽车），意识到民企都不容易迈过国际化的坎儿。为什么需要国际化？因为资本市场是赢者通吃。行业内国际企业进入，本土行业要想生存，必须主动走出去。 民企的老总个人印记太深。集权的管理问题，符合中国文化传统，但与现代管理理念相差甚远。强调军事化管理的集体制，是无法调动个人主观积极性的，对于底层车间工人可能有效，但如此又会造成企业管理上的双轨制，产生内部紧张。另外，集体制会助长一些骄横的个人气息，狭隘自大，而不利于个人内在品质的培养。一个代表先进生产关系和生产力的团队，是不会容忍形式主义的。 国内不错的企业要么狼性，要么艰苦奋斗。其实鼓励创业氛围没有问题，但是文化上很容易“右倾”，把规则理解的太死。 另外强调艰苦奋斗又不集权的华为，给员工持股，也许才是现代优秀企业该有的特征。把员工当作“合伙人“，自然调动了员工的积极性，而不是为某个老板打工。 回国前的想法是参与一个团队的成长，而不是在一家公司打工。所以除了物质利益，现在人更渴求在工作中的身份认同：”合伙人“。 套路国内的套路：先放话。 改革开放前，国家层面严禁私有制，结果江浙的小农户搞了“分田到户”私有承包，后来却全国推广了。面对新情况，国家领导也在摸索，但又要给广大普通人一致的声音。所以先放话。在日后的实践中，慢慢修正，甚至会产生与放话的内容完全相反的实践。至于普通人，如果把放话的内容听的太真，跟领导较劲儿，就是不懂套路。 有些企业做了匪夷所思的规定，还名正言顺地称为“企业文化”，对于明显不符合人情逻辑的条例，也就是这类“放话”，企业领导并不知道怎么管理，员工明白就好，该怎么来还是怎么来；但如果因此想挑战企业领导的规定，就是不懂套路。 提拔 or 压制国内有个说法叫”站错队“：不怨能力，是没跟对人。在美国职场，管理层都至少表现比较”亲民“，另外工薪层也基本生活无忧，所以两者相安，比较容易相信对方；相对，国内的职场还没有成熟的系统，就会有”站错队“的风险。另外大家都有生活压力，难免成了隐性竞争对手，互不信任，所以国内的职场被压制可能多过受提拔。这当然是陋习。 偶然看了密西根的地图，一股亲切感就涌上来。在美国的大环境会把善意当作默认的配置，工作生活上有意无意都会受到他人的帮助或有意无意地帮助别人。善意比较容易表达出来；相比，国内的环境，职场上、生活上，都有一些压制感。 比如生活上的压抑感。在北美的同学都嘴上说，回国好啊，吃的多么多么好。实际上，外面的东西都不敢吃，忌讳比如肉干净吗、油干净吗、放了不该放的调料吗。办点事，老是担心哪里被骗了都不知道。社会系统不成熟，就会有这样的隐形成本。 写在最后写完上面的内容4天后，才有机会再次打开。工作到没时间读书是对人最大的消耗。希望自己尽快适应国内工作生活节奏，而又不受制于这样的工作生活状态。 某个人的回忆 ps 军训的时候，每天跑8km，站军姿一个半小时，在这样的环境下反而更激发我去思考，如何把高标准习成标配。 管理之路我以为自己是细节导向的，国内这个工作环境会占据太多个人时间，叫人没时间思考大方向的问题。高标准成为标配吧。 越来越看到技术是平的，相比，工作流程，系统建设才是企业愿意花钱的地方。写程序、推公式、做运营、甚至做基础研究，到一定阶段都是极易取代的。“单腿走路”是high risk的。相比，任何一个领域的产品人，至少有一条粗腿，同时还有很多不错的小腿。把各个环节、各个岗位有机组合起来的，就是管理。而在工作中需要有意培养系统（管理）意识。 现在的民企，管理水平还是很差的。文化层面上，虽然在推一些流程，但是也有一些制度性的限制。一方面，一边高举打破旧思维搞创新，一边又做很多行为细节上的约束，在员工心理建立墙。工作流程的建立，不是做几次讲座、参加几个培训，领导提倡几次，不是喊出来的，而是像培养一个工作习惯。需要反复练习，慢慢融入到企业文化，变成员工自然而然的行为。 日常层面上，任务管理、时间管理都比较缺乏。老板会希望员工加班，但并没有定义很好的工作内容，即缺乏任务管理，缺乏日常的考核机制。然后责任不明晰，缺乏有效的工作模式，比如出现一个问题，一伙儿人都围上来了。或者一个会议讨论停不下来。当然，大的层面还是工作的流程没有建立好。 今后的工作，要经常跳出来思考系统建设，而不要跟年轻人拼体力上。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Apollo 2.0 Planning 源码(1)]]></title>
    <url>%2F2019%2F04%2F28%2Fapollo-planning-%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104// 模块入口APOLLO_MAIN(apollo::planning::Planning)int main(int argc, char **argv)&#123; google::InitGoogleLogging(argv[0]); google::ParseCommandLineFlags(&amp;argc, &amp;argv, true); signal(SIGINT, apollo::common::apollo_app_sigint_handler); apollo::planning::Planning apollo_app_ ; ros::init(argc, argv, apollo_app_.Name()); apollo_app_.Spin(); return 0;&#125;///////////////////////////////////////////////////////int ApolloApp::Spin()&#123; ros::AsyncSpinner spinner(callback_thread_num_); auto status = Init(); status = Start(); spinner.start(); ros::waitForShutdown(); Stop(); return 0;&#125;///////////////////////////////////////////////////////Status Planning::Init()&#123; hdmap_ = apollo::hdmap::HDMapUtil::BaseMapPtr(); apollo::common::util::GetProtoFromFile(FLAGS_planning_config_file, &amp;config); if(!AdapterManager::Initialized())&#123; AdaapterManager::Init(FLAGS_planning_adapter_config_filename); &#125; AdapterManager::GetLocalization(); AdapterManager::GetChassis(); AdapterManager::GetRoutingResponse(); AdapterManager::GetRoutingRequest(); if(FLAGS_enable_prediction) AdapterManager::GetPrediction(); if(FLAGS_enable_traffic_light) AdapterManager::GetTrafficLightDetection(); ReferenceLineProvider::instance()-&gt;Init(hdmap_, config_.qp_spline_reference_line_smoother_config()); RegisterPlanners(); planner_ = planner_factory_.CreateObject(config_.planner_type()); return planner_-&gt;Init(config_);&#125;/* DEFINE_string(planning_adapter_config_filename, "modules/planning/conf/adapter.conf", "The adapter configuration file")*////////////////////////////////////////////////////////////////void AdapterManager::Init(const std::string &amp;adapter_config_filename)&#123; AdapterManagerConfig configs; util::GetProtoFromFile(adapter_config_filename, &amp;configs); Init(configs);&#125;//////////////////////////////////////////////////////////////void AdapterManager::Init(const AdapterManagerConfig&amp; configs)&#123; if(Initialized()) return; instance()-&gt;initialized_ = true; if(configs.is_ros())&#123; instance()-&gt;node_handle_.reset(new ros::NodeHandle()); &#125; for(const auto &amp;config : configs.config()) &#123; case AdapterConfig::CHASSIS: EnableChassis(FLAGS_chassis_topic, config); break; case AdapterConfig::LOCALIZATION: EnableLocalization(FLAGS_localization_topic, config); break; // ... &#125; &#125; /* DEFINE_string 宏 FLAGS_chassis_topic -&gt; /apollo/canbus/chassis FLAGS_localization_topic -&gt; /apollo/localization/pose*/ where is EnableChassis ? in /apollo/modules/common/adapters/message_adapters.h 12345using ChassisAdapter=Adapter&lt;::apollo::canbus::Chassis&gt;;using GpsAdapter = Adapter&lt;apollo::localization::Gps&gt;;using PlanningAdapter = Adapter&lt;planning::ADCTrajectory&gt;; in /apollo/modules/common/adapters/adapter_manager.h 1234#define REGISTER_ADAPTER(name) public static void Enable##name(const std::string &amp;topic_name, const AdapterConfig&amp; config) &#123; &#125; continue tomorrow]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动驾驶职位介绍]]></title>
    <url>%2F2019%2F04%2F08%2F%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%81%8C%E4%BD%8D%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[光庭科技（武汉）智能网联汽车车载终端产品，汽车IT公司，自主研发、产品设计、制造和销售。e.g. 自动驾驶控制器，远程无线通信终端，立体相机、地图传感器、”光谷梦4.0”自动驾驶系统 职位：感知算法工程师 要求：图像/点云目标检测、跟踪；slam算法；Camera/Lidar/Radar传感器融合算法；系统集成和调试 加分项：ROS开发经验，视觉SLAM算法，深度学习算法 职位：控制算法工程师 要求：控制系统设计、信号处理、动态系统建模； 时域/频域控制器设计和稳定性分析；汽车速度和方向控制 职位：测试主管 要求： 产品测试方案、管理。软件测试及自动化测试工具 华砺智行（武汉）智能基础设施平台，为智能驾驶、智慧城市提供软硬件解决方案。产品：智能网联汽车终端、交通行业、无线通信、云平台、交通应用等 天迈科技（郑州）城市公交运营、管理及服务提供综合解决方案。车联网产品：智能公交调度系统、远程监控系统、智能公交收银系统、充电运营管理。 职位：视频算法工程师 要求： 驾驶员行为状态检测算法，机器学习检测效果 赢彻科技物流 纵目科技环视ADAS解决方案 职位： 车身控制算法工程师 要求： 车身横向、侧向控制算法设计、仿真、测试。自动控制理论，车辆底盘控制，车身动力学，Carsim, Simulink 西井科技（上海）职位： 仿真平台开发工程师 要求： 设计仿真平台架构，传感器仿真模型，3D物理引擎(unreal, unity)，熟悉开源仿真平台(carla, airsim)，构建场景环境 小鹏汽车（广州）职位： SLAM专家 要求： 视觉定位算法；与感知、地图模块结合做3d视觉系统； 职位：HD Map工程师 要求： 大规模多层高精地图框架，地图API, 地图数据质量评估，地图数据格式 职位：运动控制算法专家 要求：转向、制动、动力系统的主动控制，对底盘、执行器的控制需求；车道保持、自适应巡航、自动变道等功能运动控制，现场调试 职位：规划与控制专家 要求：算法开发、测试，提出硬件设计和集成要求 职位： 雷达算法工程师 要求： 算法仿真验证、数据分析、算法嵌入式平台实现、维护 职位：计算机视觉 要求：场景元素识别；道路、停车场环境寓意分割；算法验证 职位：传感器融合 要求： 毫米波、超声波、摄像头、激光雷达测试开发； 感知数据处理、实时地图构建及应用；目标实时跟踪与预测 职位：项目经理 要求： l2-l4产品解决方案项目管理 宇通汽车（深圳）职位： 控制工程师 要求： 轨迹跟踪、车辆控制、 仿真优化 职位：行为决策工程师 要求：行为决策算法开发，碰撞预测、行为预测、驾驶经验库、交通安全规则库等，基于强化学习的跟踪(?) 职位：首席工程师 要求：自驾客车架构规划、设计；核心算法开发和测试；自动驾驶技术跟踪。熟悉人工智能、机器视觉、深度学习、高精地图、定位等， 熟悉滤波算法、轨迹规划 纽励科技职位：ADAS产品经理 要求：产品客户需求调研、项目收集整理， 与主机厂沟通技术方案，产品改进。熟悉AEB, ACC, LKA, FCW, 自动泊车，熟悉汽车电子软硬件设计、嵌入式开发标准，熟悉传感器 光束汽车职位： 控制算法 华人运通职位： 控制工程师 要求： 运动控制算法的设计开发、仿真、优化，测试； 基于驾驶员操作数据的车辆运动控制算法 恒大汽车自行开发，现有汽车平台及产品 职位： 仿真验证经理 要求：汽车电子网络、诊断开发测试、 研发中心结构： 造型中心，动力总成中心， 车联网中心，整车工程中心，自动驾驶中心 上海电气轨道公交智能系统 职位： 控制算法工程师 要求：车辆控制实时数据采集、理论模型、系统仿真和实测验证算法 Magna Steyr 麦格纳斯太尔汽车技术职位： L4 ADAS电子专家 要求： ADAS SE团队 职位： GNSS测试验证首席工程师 牧月科技职位：仿真系统工程师 要求： 负责仿真系统开发、测试，及各种传感器的仿真软件库，利用已有数据构建仿真场景；开发自动化仿真分析平台 景驰科技（文远知行）职位： 仿真算法工程师 要求： 开发场景自动化生成算法、人机交互仿真软件工具集、可扩展计算框架 腾讯CSIG事业部伟世通职位： 软件测试 要求： ADAS软件测试、竞品分析 博世（苏州）职位： 控制算法工程师 要求： ADAS自动泊车系统 亿伽通（吉利系）职位：算法工程师要求：定位产品场景、需求分析； 参与搭建自动驾驶数据智能服务平台；熟悉基于视觉的slam方案 奥迪中国（北京）车联网团队 职位： 测试工程师 要求： ADAS/HAD功能测试 职位：仿真工程师 要求： SiL环境搭建，adas功能仿真测试 斑马智行（阿里、上汽）智加科技（苏州）职位： 传感器标定工程师 要求： 传感器选型、数据读取、内外参标定、在线检测 职位：感知算法工程师 要求： 对图像、点云等的静态场景要素检测和追踪；融合传感器数据后的目标检测和跟踪；服务地图自建、预测规划决策系统 职位： 地图定位工程师 要求： 开发多传感器融合算法以提高地图精度，自动化地图的大规模采集、生成、标注、校政。 职位： 决策规划工程师 要求： 预测、决策、规划等系统 职位： 仿真工程师 要求： 利用真实数据或合成数据搭建动态环境的仿真框架；设计场景，为感知、规划、预测等模块开发仿真测试接口；开发基于仿真测试的自动化分析平台；构建可扩展的仿真框架 魔视智能（上海）极目智能职位： 车辆决策算法工程师 零跑科技职位： 算法工程师 要求： 定位算法模块，基于GPS/IMU的航伟推算算法，视觉定位算法 阿里巴巴（人工智能实验室）职位： 仿真算法工程师 吉利Volvo上海研发中心中智行（南京）l4 解决方案 同元软控（苏州）国产CAD/CAE产品供应商 华为2012华为海思（上海）职位： 软件测试 一汽红旗职位： 车联网构架设计师 要求： T-box端-云构架，v2x 潜在人选： 车联网企业： 斑马、亿伽通、博泰、百度车联网团队、飞驰镁物、哈曼、四维图新、梧桐车联 新势力： 蔚来、小鹏、威马 职位：软件算法、系统工程师 潜在人选： 上汽人工智能实验室 英伟达 福特-argo ai momenta, plusai 职位： 数据挖掘工程师 要求： 大数据建模分析，用户数据、车辆数据、驾驶员数据等 潜在人选： 浪潮 曙光 华为云 阿里云 百度 华域汽车滴滴无人车美团无人车职位： 仿真工程师 一汽大众职位：决策算法工程师 veoneer职位：adas软件开发工程师 易航智能职位： 控制算法工程师 三一无人驾驶无人码头 职位： 算法工程师 易高美职位： hpmap仿真工程师 要求： 大批量仿真数据自动化生成；设计分布式仿真系统底层架构，3D引擎编辑器工具链开发；实现仿真系统场景渲染；对大批量及各种随机虚拟数据自动化脚本工具开发。OpenGL/Direct3D图形接口; unreal, unity引擎]]></content>
      <tags>
        <tag>self-driving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[particle filter (2)]]></title>
    <url>%2F2019%2F04%2F03%2F%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2-2%2F</url>
    <content type="text"><![CDATA[前一篇 介绍贝叶斯滤波的数学原理和蒙特卡洛定位的算法。本篇将介绍基于序贯重要性采样。粒子滤波的思想就是采用一个加权粒子分布去近似后验概率分布p(x) 蒙特卡洛积分定义一连续随机变量X, 其概率密度分布函数为 p(X); 定义Y=f(X)， 则随机变量Y的数学期望： 实际中，概率密度分布p(X)未知，如何保障所采样的点服从p(X) 直接采样通过对均匀分布采样，实现对任意分布的采样。 任何未知概率密度分布的累积概率函数cdf都映射在[0-1]区间，通过在[0-1]区间的均匀采样，再函数z = cdf(y)求逆，即是符合真实 y的概率密度分布的采样点。 但如果cdf()函数未知或无法求逆，直接采样不可行。 接受-拒绝采样用一个已知概率分布函数q(X)去采样，然后按照一定的方法拒绝某些样本，达到近似p(X)分布: p(x_i) &lt;= k p(x_i) 该采样的限制是确定参数k。 重要性采样在一定的抽样数量基础上，增加准确度。未知p(x), 在已知概率密度分布的q(x)上采样{x_1, x_2, … x_n}后估计f的期望： 定义新的随机变量： 关于原随机变量Y在未知概率分布p(x)下的期望，转化为新的随机变量Z在已知概率分布q(x)下的期望。已知概率分布，即知道如何采样。这里 p(x)/q(x) 就是权值。 so the posterior expectations can be computed as: as the importance weights can be defined as: the problem is we can’t get p(x|z) , but a loosed (unnormalized) importance weights as: then do normalized from it: so the posterior expectation is approximated as: sequential importance sampling(SIS)consider the full posterior distribution of states X_{0:k} given measurements y_{1:k} : consider the sequential of q(x): then the unnormalized importance weights can be as: namely: the problem in SIS is the algorithm is degenerate, that variance of the weights increases at every step, which means the algorithm will converget to single none-zero (w=1) weight and the rest being zero. so resampling. sequential importance resampling(SER)resampling process: 1) interpret each weight as the probability of obtaining the sample index i in the set x^i 2) draw N samples from that discrete distribtuion and replace the old sample set with the new one. SER process: 1) draw point x_i from the q distribution.2) calculate the weights in iteration SIS and normalized the weights to sum to unity3) if the effective number of particles is too low, go to resampling process]]></content>
      <tags>
        <tag>control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[partical filter]]></title>
    <url>%2F2019%2F04%2F02%2Fpartical-filter%2F</url>
    <content type="text"><![CDATA[localizing the vehicle involves determing where on the map the vehicle is most likely to be by matching what the vehicles see to the map. Markov localization or Bayes Filter for localization is the generalized filter. thinking of the robot location as a probability distribution, each time the robot move, the distribution becomes more diffuse(wide). by passing control data, map data, observation into the filter will concentrate(narrow) the distribution at each timestep. state spacex = f(x, v) (1) z = h(x, w) (2) v, w is the process noise, measurement noise respectfully, and each is in the normal Gaussian distribution. Bayes filter derivation (b) consider the multiply rule of probability: p(a, b) = p(a|b) p(b) lhs of equation(b) is: given x_k, assuming z_k is independent from all previous measurements z_{1:k-1}: Markov Localizationin which the true state x is unobserved, and the measurements z is observed.assuming 1st order Markov, the probability of current true state: p(x_k | x_{0:k-1}) == p(x_k | x_{k-1}) (3) similarly, the measurement is only dependent on current state, which is a stochastic projection of the true state x_t, : p(z_k | x_{0:k}) = p(z_k | x_k) (4) (3) is referred to as motion model, and (4) as measurement/observation model. the classical problem in partially observable Markov chains is to recover a posterior distribution from all avilable sensor measurements and controls in all timesteps. Especially, for the localization problem here is to obtain the system current state posterior p(x_k | z_{1:k}) based on the all existing measurements, which can be solved by Bayes Filter. ps. the propability distribution of current state is also depend on other known inputs, e.g. map data, control data. predictionfrom Bayes filter equation, p(x_k | z_{1:k-1}) need get first, which is the prediction step. physically, it used to estimate the system state based on all previous measurements. consider x_{k-1} as the random variable, the integration of pdf p(x_k, x_{k-1} | z_{1:k-1}) about x_{k-1} is p(x_k | z_{1:k-1}) consider the multiply rule: by 1st order Markov assuming, the first item in integral can reduced: p(x_k | x_{k-1}) is determined by the system, which obey the same distribution of process noise. p(x_{k-1} | z_{1:k-1}) is known, as the posterior state at timestep k-1. this is where the recursive process. updateusing equation (b) to update the current posterior state. the denominator of (b) is a constant coeffient. p(z_k | x_k) is the likelihood paramter, decided by measurement.]]></content>
      <tags>
        <tag>control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无迹卡尔曼滤波]]></title>
    <url>%2F2019%2F03%2F31%2F%E6%97%A0%E8%BF%B9%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%2F</url>
    <content type="text"><![CDATA[非线性系统： x = f(x, w) （1） z = h(x) + v （2） 随机信号 w, v分别是过程噪声和观测噪声 CTRV 状态方程对于const turn rate and velocity magnitude (CTRV )场景： x = (px, py, v, phi, \dot{phi}) 固定速度和转动速率约束，即： 考虑 dv/dt == 0 , dphi^2\dt^2==0 且\psi是时间的函数, 上述第一项即： 从原状态空间到预测空间，由方程（1),（2）可见，过程噪声w是状态x的非线性项；而z关于观测噪声v是线性的。ukf实际采用增广状态变量sigmax = [x, w]. 过程噪声w包括径向加速度和角加速度 [w_a, w_phi]， 且w不是时间的函数 , 对上述第一项可展开： 预测空间对比扩展卡尔曼 ekf采用一阶线性化近似。无迹卡尔曼ukf，将原状态空间的特征采样点(sigmax)映射到预测空间，采用预测空间里的状态变量f(sigmax)的均值、方差的加权推广作为先验状态估计x^- 和先验误差P^-。 其中权值表述： $$ w = lamda / ( lamda + ns) when i==1 $$ $$ w_i = 0.5/(lamba + ns) when i!=1 $$ $$ X^- = sum(w_i * f(sigmax) ) $$ $$ P^- = sum(w_i * (f(sigmax) - x).^2) $$ 观测空间将原状态空间的特征采样点(sigmax)映射到观测空间，采用观测空间里的状态变量h(sigmax)的均值、方差的加权作为先验观测值Z^- 和观测值先验误差S^-，使用与预测空间同样的权值。 $$ Z^- = sum(w_i * h(sigmax) ) $$ $$ S^- = sum(w_i * (Z^- - z).^2) + R $$ 卡尔曼滤波表示： 后验估计（真实状态变量值）与先验估计（预测空间的状态变量值）的差异，可表示为真实观测值与观测空间里的先验观测值的差异的增益 K。 $$ x - x^- = K (z - Z^-) $$ （3） 可见，卡尔曼增益K在衡量状态误差与观测误差之间的相关性。定义预测空间与观测空间的相关系数： T = sum(w_i * (X^- - x)(Z^- - z)) K = T / S^- （4） ukf算法有（4）， （3） 分别更新卡尔曼增益和状态变量， 预测空间里的先验误差更新由： P = P - KSK^t ps: 在非线性的处理上，线性化或者布点采样都是常用的思路。也是ekf与ukf的区别。 link1 link2]]></content>
      <tags>
        <tag>control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CarND(term2): Extended Kalman Filter]]></title>
    <url>%2F2019%2F03%2F31%2FCarND-term2-Extended-Kalman-Filter%2F</url>
    <content type="text"><![CDATA[sensor measurementRadarradio detection andranging, using radio waves to measure the distance to objects as well as their velocity and angle. Radar used a lot in preventing collions, parking assistance, cruise control. and Radar isn’t affected by weather conditions. while Radar can’t tell an object’s shape correctly. and Radar can’t detect objects if they are out of their line of sight. the Radar measurement data in EKF proejcct is 3D position and velocity vector (ro, theta, ro_dot) in polar coordinates. Lidarlight detection and ranging, using near-infrared light to scan objects and create 3D map of the enviroment. it’s 360-degree view and can track movements and their directions. but it also depends on weather conditions and can’t detecting the speed of other vehicles well. the Lidar measurement data in EKF proeject is 2D position vector (x,y) in Cartesian coordinate system. compare Radar vs Lidar server - client networkUdacity simulator communicate with EKF controller through websocket. in simulators(Udacity carsim, Carla) running time, the message channel between simulator server and external controller need to be open all the time. so the simulator feed the controller with sensor data, and the controller feedback simulator with controlling data. uwebSocketbuilt once uwebSocket, webSocket protocol providing full-duplex communication channel between server and client through a singlt TCP connection. it allows the server to send content ot the client without being first requested by the client, and allowing messages to be passed back and forth while keeping the conenction open. sensor raw dataevery data slice includes a either Lidar or Radar raw measurement and a ground truth measurement. The state variable x is described in Cartesian coord, so for Radar measurement processing, there is a coordinate transfer from Cartesian to polar, and which lead it nonlinear, requiring Extended Karman Filter. EKF controllerthe client side is the EKF controller, which process the sensor measurement. define system state `x_` , state priori covariance `P_`, state transition matrix `F`, process covariance `Q_`, measurement gain matrix &apos;H&apos; measurement covariance &apos;R_&apos; kalman filter gain &apos;K&apos; as discussed in previous blog: 12345678910111213141516171819 void KalmanFilter::Predict()&#123; x_ = F * x_ ; P_ = F * P_ * F.transpose() + Q_ ;&#125;void KalmanFilter::Update(const Vector &amp; measurement)&#123; if(EKF) h = toPolar(x_); y = measurement - h ; else y = measurement - H * x_ ; K = P_ * Ht / (H * P_ * Ht + R_); x_ = x_ + K * y ; P_ = (I - K*H) * P_ ; &#125;]]></content>
      <tags>
        <tag>control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卡尔曼滤波]]></title>
    <url>%2F2019%2F03%2F30%2F%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%2F</url>
    <content type="text"><![CDATA[状态方程状态变量 $x$ 满足, 其中 u为控制变量： $$ x = A x_prev + B u_prev + w_prev $$ (1) 观测变量 $z 满足： $$ z = H x + v $$ (2) 随机信号 $w$ 和 $v$ 分别表示过程激励噪声和观测噪声，假定相互独立，且服从正态分布。 定义状态变量的先验估计 $x^-$， 即基于之前状态对当前状态的预测值； 定义后验估计 $x^$，即已知当前观测值所计算的当前状态变量。 定义先验误差 $e^-$, 后验误差 $e^$, 满足： $$ e^- = x - x^- $$ (3) $$ e^ = x - x^ $$ (4) 卡尔曼滤波表示： 后验估计（观测值所推导的状态变量值）与先验估计（预测的状态变量值）的差异，可表示为观测值与以先验估计为输入的观测值的差异的增益 K。 $$ x^ - x^- = K ( z - H x^- ) $$ (5) K可由先验误差的协方差 P、观测噪声的协方差R 和观测增益H表示: $$ K = P^- H^t / ( HP^-H^t + R ) $$ (6) 可见： 1） 当R 趋于0时， k 趋近于 h 的逆，此时 x^ = x。即当观测误差很小，观测值趋近真实值。 2） 当P趋于0时，即预测值趋近真实值。 算法设计卡尔曼滤波器用反馈控制估计过程状态（变量）： 滤波器估计某一时刻的状态（时间更新/预估），然后以（含噪声的）测量变量获得反馈（测量更新/校政）。 时间更新，当前时步状态先验估计 x^- 及先验误差协方差近似P^-: $$ x^- = A x^-_prev + B u_prev $$ (7.1) $$ P^- = A P^_prev A^t + Q $$ (7.2) 其中 $ P(w) ~ N(0, Q) $ 测量更新，使用(6)更新卡尔曼增益K, 使用（5)更新后验状态变量x^和当前步先验误差协方差值 P^： $$ P = ( I - KH ) P^- $$ (8) 控制器调参测量误差一般可观测得到；而过程误差q需要通过与一个已知误差的在线滤波器对比调整系数。调参一般是离线过程。一般当过程误差和卡尔曼增益会快速收敛并保持常数。但测量误差受环境影响不易保持不变。 扩展卡尔曼当观测值与系统状态变量 或 系统本身是非线性关系，方程(1), (2)变非线性函数。 $$ x = f(x_prev, u_prev, w) $$ (1.2) $$ z = h(x, v) $$ (2.2) link]]></content>
  </entry>
  <entry>
    <title><![CDATA[paper reading-Carla an open urban driving simulator]]></title>
    <url>%2F2019%2F03%2F30%2Fpaper-reading-Carla-an-open-urban-driving-simulator%2F</url>
    <content type="text"><![CDATA[CARLA used to support training, prototyping, validation of self-driving models, including perception and control. CARLA is usded to study the performance of three approaches, 1) classic modular pipeline that comprises a vision-based perception module, a rule-based planner, and a maneuver controller; 2)a deep network that maps sensory input to driving commands via imitaion learning; 3) end-to-end reinforcment learning. all approaches make use of a high-level topological planner. the planner takes the current position of the agent and the location of the goal as input, and use A* algorithm to provide a high-level plan. this plan advises the agent to turn left/right, or keep straight at intersections, but not provide a trajectory neither geometric info. which is a weaker form of common GPS. simulation engineCARLA simulates a dynamic world and provide a simple interface between the world and an agent that interacts with the world. CARLA is designed as a server-client system, where the server runs the simulation and renders the scene, the client API is responsible for interaction between the agent and the server via sockets. the client send commands and meta-commands to the server and receives sensor readings in return. the comands control the vehicle and includes steering, accelerating, and braking. meta-commands controls the behavior of hte server, e.g. resetting hte simulation, modifying the sensor suite. environmentthe static 3D world, such as buildings, traffic signs, and the dynamic objects such as vehicles, pedestrains. the behavior of non-player characters is based on standard UE4 vehicle model(PhysXVehicles), and extended with a basic controller to govern NPC’s behavior: lane following, respecting traffic lights, speed limits, and decision making at intersections. pedestrainspedestrains navigate the streets according to a town-specific navigation map, which conveys a location-based cost, which is designed to encourage pedestrains to walk along sidewalkd and marked road crossing, but allows them to cross road at any point. sensorscamera parameters include 3D location, 3D orientation with respect to the vehicle’s coordinate system, field of view, and depth of field. the semantic segmentation pseudo-sensor provides 12 semantic classes: road, land-marking, traffic sign, sidewalk, fence, pole, wall, building, vegetation, vehicle, pedestrain, and other. a range of measurements associated with the state of the agent. ? measurements concerning traffic rules include the percentage of vehicle’s footprint that impinges on wrong-way lanes or sidewalks, as well as states of the traffic lights and speed limit at the current location of the vehicle. CARLA provides access to exact location and bounding boxes of all dynamic objects. autonomous drivingthe agent interacts with the environment over discrete time steps. at each time step, the agent gets an observation, which is a tuple of sensory inputs, and must produce an action, which represents steering, throttle, brake. modular pipelinethe pipeline includes: perception, planning, continuous control. local planning is critical based on visual perception. perceptionusing semantic segmentation network based on RefineNet to estimate lanes, road limits, and dynamic objects. and a classification model is used to determine proximity to intersections. the local plannercoordinates low-level navigation by generating a set of waypoints, near-term goal states that represents the desired position and orientation of the car in near future. the rule-based state: 1) road-following, 2) left-turn, 3) right-turn, 4) intersection-forward, 5) hazard-stop. transitions between states are performed based on estimates provided by the perception module and on topological info provided by the global planner. continuous controllerusing PID controller, which inputs current pose, speed, a list of waypoints, and outputs steering, throttle, and brake. carla release9.11) enable client to detect collisions and determine lane changes : sensor.other.collision, sensor.other.lane_detector, 2) access to the road network, waypoints nearby current vehicle and define user navigation algorithms: Map 3) support new map created from external RoadRunner/VectorZero, in OpenDriven map standard 9.21) simulation of traffic scenarios by Scenario Runner. e.g. following leading vehicle, stationary object crossing, dynamic object crossing, opposite vehicle running red light, vehicle turn right/left etc 2)upgraded ROS bridge 3) vehicle navigation from client side: BasicAgent, navigate to a point given location while dealing with other vehicles and traffic lights safely; RoamingAgent, drives around making random choices when presented to multiple options 9.31) new town and new pedestrains 2) no rendering mode, a 2D map visualization tool that display vehicles, traffic lights, speed limits, pedestrains, road, etc. help to improve the server framerate 3) traffic light class in client TrafficLightState 4) new sensors. ObstacleDetector, a simple raycast sensor to detect something in front of the ego vehicle and what is it; and GlobalNavigationSatelliteSystem, attach to ego vehicle and get its geolocation, which is based on the geo-reference define in OpenDriven file associated with each map. 9.41) allow client side to change physics properties of vehicle or their components in runtime WheelsPhysicsControl 2) logging and playback system, which includes a camera-following mode to follow a target actor while replaying the simulation, and can replay situations from different viewpoints. and the logging query engine allow users to query different types of events. 3) random streaming port, which makes it possible to stream sensor data in a secondary port 4) import maps, replace maps as tar.gz files in “ExportedMaps” folder]]></content>
      <tags>
        <tag>self-driving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paper reading-autonoVi: AV planning with dynamic maneuvers and traffic constraints]]></title>
    <url>%2F2019%2F03%2F29%2Fpaper-reading-autonoVi-AV-planning-with-dynamic-maneuvers-and-traffic-constraints%2F</url>
    <content type="text"><![CDATA[this is the advanced driver module in Vi-sim simulation platform. this driver module algorithm pipeline: 1) a route plan by graph-search over the network of roads 2) rules based guiding trajectories generation(traffic and lane following rules) 3) set of candidate trajectories(control inputs) generation and evaluated by vehicle dynamic model and collision free model 4) most feasible trajectory evaluated through optimization vehicle state spacethe full state of a vehicle updates: X = (x, y, v, theta, throttle, steering, behavior) the vehicle updates its plan at a fixed palnning rate dt; at each pllaning step, the vehicle computes a target speed v and target steering theta to be achieved by the control system S(u, X) determine if a set of control is feasible, given current state of the vehicle, S(u, X) will return false if the given input u cause a loss of traction or control. sensing and perceptionthe sensing module provide an approximation of the center line of lane, closet point on the lane center to the ego-vehicle, and friction coefficient. route choice and behavior statebehavior set includes merging, right turn, left run, keep straight. the behavior state of the vehicle is described as a finite-state machine(turn left, turn right, merge left, merge right), which restrict potential control decisions and adjust the weight of the cost function. guiding paththe ego-vehicle computes a set of waypoints along the current lane at fixed time intervals. how to create the path based on waypoints collision avoidancedefine obstacles domain for each neighbor of the ego-vehicle, which is defined as all controls that could lead to collision. the obstacles domain and the set of dynamic infeasible domain form the boundary of collision-free space for the ego-vehicle. trajectory samplingthe exact obstacle domain is not computing time efficent, instead here use a sampling strategy around theta and v to determin a feasible control. each sample is referred to as a candidata control u_c. trajectory cost functiononce the set of suitable control candidates has been computed, the most feasible control will be selected by minimizing the cost function for each sample point i : C = sum_i{ C_path(i) + C_cmft(i) + C_mnvr(i) + C_prox(i) 1) path cost, defined as success at tracking its path and the global route. 2) comfort cost, C_cmfg = ||vel_acc|| + ||theta_acc|| 3) maneuver cost, penalize lane changes C_mnvr = lane_change 4) proximity cost, prevent the ego vehicle from passing close to neighbors. control inputone PID controller to driven current speed to match the target speed; another PID controller drives the current steering angle to match the target.]]></content>
      <tags>
        <tag>self-driving</tag>
      </tags>
  </entry>
</search>