<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mobile vehicle interface]]></title>
    <url>%2F2019%2F12%2F29%2Fmobile-vehicle-interface%2F</url>
    <content type="text"><![CDATA[backgroundhuman-machine interface(HMI), in-vehicle device(IVD). human-vehicle interface(HVI), e.g. voice control, touch screen, face detection, e.t.c, which does well for young and passion generation, who have great voice, sharp fingers, and good-look face, and all these improving user experienced tech are based on similar AI. what about the elders ? which should be, in the next 10 - 20 years, the most tough family and society issues in China, and as well as for Japan right now. the basicaly option here is HVI is not enough for the elders groups, or at least is not the only choice for elders, who don’t have great voice, good-look face, or sharp fingers, and even no patient to talk/look/touch with the weak AI system. a very good alternative or additional supportting solution should be mobility vehicle interface(MVI) mobility vehicle interfacewhat kind of mobility devicemobility is a general name, for all kinds of mobile devices, especially for Iphone, Android smart phone and personal care robot. application scenariosfor an elder, who has daily needs for traffic transfer. e.g. go to hospital, to supermarket, to a special restuarant or a park for dinner or sit down with some old friends. the elders are slow in movement and talk, and the current AI based human-vehicle-interface(HVI) definitely make the elders feel pressure. a better or alternative solution is let the mobility device to talk to the vehicle, through an interface mobility vehicle interfacethe mobility vehicle interface(MVI) can be based on existing vehicle OS and mobility OS. there are plenty existing in-vehicle OS, e.g. CarPlayer, Baidu OS, GENIV, QNX e.t.c; and on mobility OS side, the most common are iOS and Andriod. most in-vehicle OS can run the same apps on mobility OS, e.g. google navigation map, instant messages, music, emergency call service e.t.c. so the first solution is app2app. basically the same app ran in the personal mobility device(PMD) talk to the same app run in vehicle OS. PMD has more time with the owner, so has more personality than vehicle, especially as vehicle in future is more like a public service, rather than a personal asset. that’s why mobility vehicle interface(MVI) is a good option, especially for elders, who may not enjoy talk to AI. beyond the easy to implement at this moment, app2app solution has a few limitations: the security is mainly provided by the app supplier, which is not a unit solution, as different app suppliers have different security mechanism. as apps hosted in this system is growing, the adapters or interfaces to make the bridge grows too, which decrease the user experince and increase the system cost. so a better solution is a new mobility-vehicle interface protocol, which is the only bridge between personal mobility and vehicles. and no matter what kind of apps and how many apps hosts in both system, won’t be a burden for the sytem anymore. moblity vehicle interface protocol]]></content>
  </entry>
  <entry>
    <title><![CDATA[can MaaS survive in China]]></title>
    <url>%2F2019%2F12%2F29%2Fcan-MaaS-survive-in-China%2F</url>
    <content type="text"><![CDATA[the difference of US and China: citizenship vs relationshipthe first class cities, e.g. Beijing, Shanghai, Shenzhen, are not different from Chicago, New York, in normal people’s lifestyle: they share the same luxurious brands, Starbucks, Texas beaf Steak, city public services, and the same international popular elements in eletronic consumers, clothing, vehicles, and even the office env. However, down to the third class, or forth class cities in China and US, there are a huge difference. the bottom difference is citizenship（公民意识） vs relationship（关系文化). in US and most developed countries, citizenship is a common sense, no matter in small towns or big cities. in China, the residents in big cities are similar to residents in developed countries; but the normal people in third cities value more about relationship, rather than citizenship, so basically the rules how to live a high-qualitied/successful life in these cities is not universal, which means if a resident from big cities jumping to these small cities, his experince about what is a good career/life choice has to be changed, and further which has a great influence about consuming habit and the acceptance of emerging market. the Chinese goverment is pushing urbanization in most uncitizenlized areas, hopefully this process can be achieved in a few generations, which can be affected by both goverment policy and the external forces, e.g. trade war. any way, there is no short way. Chinese subside marketsin China, one kind of the most profit business is e-trade, e.g. Alibaba, JD, Pinduoduo, e.t.c. they are sinking to the third/forth cities in China in recent years, which is a special phenomenon in China, the reason as I see, is due to the division between citizenship in top class cities and relationship in most small cities in China. for most developed countries, e.g. US, the market is so flat that once one product/service is matured in big cities, there is no additional cost to expand to small towns in national wide. But here in China, the market, the society structure, the resident’s consuming habit are not flat due to the division as mentioned previously. so they need different bussniess strategy for product/service in big cities and most small towns. for the emerging market, the investors and service/product providers need input from top consulting teams, e.g. PWC, Deloitte, BCG, but the research paper from these teams try to ignore the value gap in Chinese large cities and small cities. Of course I can understand the consulting strategy, as emerging market is looking for new services in near future, and it should looks promising. taking mobility as a service (MaaS) as an example, from sharing cars to MaaS is likely happened in urban areas in next 10 years, and expand to most areas in west European and NA countries, but in the most small towns of China, it may never happen. MaaS is a promising service if the society and resident’s value are similar (or plat). for developing countries, e.g. China, India, these emerging market wouldn’t be a great success in national wide. start-ups in MaaS mobiag mobilleo invers maymobility vaimoo in Brazil populus.ai staflsystems polysync.io geotab public resources in moblity as a service(MaaS)Mass alliance International parking &amp; mobility institute shared mobility services in Texas DI_Forces of change-the future of mobility PWC_how shared mobility and automation will reolution Princeton_strategies to Advanced automated and connected vehicles: a primer for state and local decision makers Accenture_mobility as a service whitepaper Bosch_HMI Toyota_Mobility ecosystem Volkswagen_E-mobility module Siemens_Intelligent Traffic Systems MaaS in UK the tech liberation front autonomous vehicle technology]]></content>
  </entry>
  <entry>
    <title><![CDATA[configure hadoop in 2-nodes cluster]]></title>
    <url>%2F2019%2F12%2F28%2Fconfigure-hadoop-in-2-nodes-cluster%2F</url>
    <content type="text"><![CDATA[backgroundit’s by accident that I have to jump into data center, where 4 kinds of data need deal with: sensor verification, with huge amount of special raw sensor data AI perception training, with huge amout of fusioned sensor data synthetic scenarios data, which used to resimualtion inter-middle status log data for Planning and Control(P&amp;C) big data tool is a have-to go through for L3+ ADS team, which has already developed in top start-ups, e.g. WeRide and Pony.AI, as well as top OEMs from NA, Europen. Big data, as I understand is at least as same important to business, as to customers. compared to AI, which is more on customer’s experience. and 2B is a trending for Internet+ diserving into traditional industry. anyway, it’s a good try to get some ideas about big data ecosystem. and here is the first step: hadoop prepare jdk and hadoop in single nodeJava sounds like a Windows langage, there are a few apps requied Java in Ubuntu, e.g. osm browser e.t.c., but I can’t tell the difference between jdk and jre, or openjdk vs Oracle. jdk is a dev toolkit, which includes jre and beyond. so when it’s always better to set JAVA_HOME to jdk folder. jdk in ubuntuthere are many different version of jdk, e.g. 8, 9, 11, 13 e.t.c. here is used jdk-11, which can be download from Oracle website, there are two zip files, the src and the other. the pre-compiled zip is enough to Hadoop in Ubuntu. 1234tar xzvf jdk-11.zip cp -r jdk-11 /usr/local/jdk-11 cd /usr/localln -s jdk-11 jdk append JAVA_HOME=/usr/local/jdk &amp;&amp; PATH=$PATH:$JAVA_HOME/bin to ~/.bashrc, and can run test java -version. what need to be careful here, as the current login user may be not fitted for multi-nodes cluster env, so it’s better to create the hadoop group and hduser, and use hduse as the login user in following steps. create hadoop user1234sudo addgroup hadoopsudo adduser --ingroup hadoop hduser sudo - hduser #login as hduser the other thing about hduser, is not in sudo group, which can be added by: curren login user is hduser: 12345678910111213141516171819202122groups # hadoopsu - # but password doesn't correct#login from the default user terminalsudo -i usermod -aG sudo hduser#backto hduser terminalgroups hduser # : hadoop sudo exit su - hduser #re-login as hduser ``` #### install and configure hadoop hadoop installation at Ubuntu is similar to Java, which has src.zip and pre-build.zip, where I directly download the `pre-build.zip`.another thing need take care is the version of hadoop. since `hadoop 2.x` has no `--daemon` option, which will leads error when master node is with `hadoop 3.x`.```shelltar xzvf hadoop-3.2.1.zip cp -r hadoop-3.2.1 /usr/local/hadoop-3.2.1cd /usr/localln -s hadoop-3.2.1 hadoop add HADOOP_HOME=/usr/local/hadoop and PATH=$PATH:$HADOOP_HOME/bin to ~/.bashrc. test with hadoop version hadoop configure is find here there is another issue with JAVA_HOME not found, which I modify the JAVA_HOME variable in $HADOOP_HOME/etc/hadoop/hadoop_env.sh passwordless access among nodes generate SSH key pair 1234on maste node:ssh-keygen -t rsa -b 4096 -C "master"on worker node:ssh-keygen -t rsa -b 4096 -C "worker" the following two steps need do on both machines, so that the local machine can ssh access both to itself and to the remote. enable SSH access to local machine ssh-copy-id hduser@192.168.0.10 copy public key to the remote node ssh-copy-id hduser@192.168.0.13 tips, if changed the default id_rsa name to sth else, doesn’t work. after the changes above, will generates a known_hosts at local machine, and an authorized_keys, which is the public key of the client ssh, at remote machine. test hadoop on master node 12345hduser@ubuntu:/usr/local/hadoop/sbin$ jps128816 SecondaryNameNode128563 DataNode129156 Jps128367 NameNode on worker node: 123hduser@worker:/usr/local/hadoop/logs$ jps985 Jps831 DataNode and test with mapreduce]]></content>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deploy lgsvl in docker swarm 4]]></title>
    <url>%2F2019%2F12%2F25%2Fdeploy-lgsvl-in-docker-swarm-4%2F</url>
    <content type="text"><![CDATA[backgroundpreviously, tried to deploy lgsvl in docker swarm, which is failde due to the conflict of host network to run lgsvl and the routing mesh of swarm, as I thought. http listen on *why used –network=host, is actually not a have-to, the alternative option is to use &quot;*&quot; as Configure.webHost, instead of localhost nor a special IP address, which lead o HttpListener error: 1The requested address is not vaid in this context. then, we can docker run lgsvl without host network limitations. but still, if run by docker service create, it reports failure: Error initiliazing Gtk+. Gtk/UI in Unitywhen starting lgsvl, it pops the resolution window, which is a plugin of Unity Editor, and implemented with gtk, as explained in last section, which leads to the failure to run lgsvl as service in docker swarm. the simple solution is to disable resolution selection in Unity Editor. 1Build Settings --&gt; Player Settings --&gt; Disable Resolution then the popup window is bypassed. ignore publish portI tried to ignore network host and run directly with routing mesh, but it still doesn’t work. then I remember at the previous blog, when run vkcube or glxgears in docker swarm, it actually does use --network host, so it looks the failure of running lgsvl in docker swarm, is not due to network host, but is due to Gtk/gui. as we can bypass the resolution UI, then directly running as following, works as expected: 1sudo docker service create --name lgsvl --generic-resource &quot;gpu=1&quot; --replicas 2 --env DISPLAY --mount src=&quot;X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; --network host lgsvl add assets into containeranother update is to bind assets from host into lgsvl image, which is stored as sqlite data.db, which is a necessary, as we bypassed the authentication, and the cluster has no access to external Internet. where is nextin recent two month, had digged into docker swarm to run lgsvl. so far, the main pipeline looks work now, and there are still a lot little fix there. to run AV simulation in cloud, is a necessary way to test and verify L3+ AV algorithms/products. previous ADAS test is more on each individual feature itself, e.g. ACC, AEB .e.t.c, all of which are easy to define a benchmark test case, and engineers can easily define the test scenarios systemetically. But for L3+, the env status space is infinite in theory, there is no benchmark test cases any more, and at best we can do is statiscally test cases, which requires a huge number of test cases, which is where virutal simulation test in cloud make sense. from tech viewpoint, the next thing is how to drive the L3+ dev by these simulation tools. and another intesting is the data infrastructure setup]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>lg-sim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web service bypass in lgsvl]]></title>
    <url>%2F2019%2F12%2F15%2Fweb-service-bypass-in-lgsvl%2F</url>
    <content type="text"><![CDATA[backgroundpreviously had talked lg new version code review, where introduced the new server-browser arch, which is focused on the lgsvl server side implementation, which was based on Nancy and sqliteDB; also a simple introduction about reactjs the gap how client send a http request to the server is done by axios. also another issue is how to access asset resource across domain. namely, running the lgsvl service at one host(192.168.0.10), and http request send from another remote host(192.168.0.13). Axiosthe following is an example on how Axios works. 12345678910111213141516171819202122constructor() &#123; this.state = &#123; user: null &#125; &#125;componentDidMount() &#123; axios.get(&apos;https://dog.ceo/api/breeds/image/random&apos;) .then(response =&gt; &#123; console.log(response.data); if(response.status == 200) setState(user, reponse.data) &#125;) .catch(error =&gt; &#123; console.log(error); &#125;);&#125; render() &#123; return ( ) &#125; from React componnet to DOM will call componentDidMount(), inside which axios send a GET request to https://dog.ceo/api/breeds/image/random for a random dog photo. and can also store the response as this component’s state. enactenact is a React project manager, the common usage: 123enact create . # generate project at current dir npm run serve npm run clean enact prject has a configure file, package.json, while can specify the proxy, which is localhost by default. if want to bind to a special IP address, this is the right place to modify. 1234&quot;enact&quot;: &#123; &quot;theme&quot;: &quot;moonstone&quot;, &quot;proxy&quot;: &quot;http://192.168.0.10:5050&quot;&#125;, inside lgsvl/webUI, we need do this proxy configure, to support the across-domain access. Nancy authenticationthis.RequiresAuthentication(), which ensures that an authenticated user is available or it will return HttpStatusCode.Unauthorized. The CurrentUser must not be null and the UserName must not be empty for the user to be considered authenticated. By calling this RequiresAuthentication() method, all requests to this Module must be authenticated. if not authenticated, then the requests will be redirected to http://account.lgsimulator.com. You need to include the types in the Nancy.Security namespace in order for these extension methods to be available from inside your module. this.RequiresAuthentication() is equal to return (this.Context.CurrentUser == null) ? new HtmlResponse(HttpStatusCode.Unauthorized) : null; all modules in lgsvl web server are authenticated by Nancy:RequiresAuthentication(), for test purpose only, we can bypass this function, and pass the account directly: 1234// this.RequiresAuthentication();// return service.List(filter, offset, count, this.Context.CurrentUser.Identity.Name)string currentUsername = &quot;test@abc.com&quot;;return service.List(filter, offset, count, currentUsername) in this way, no matter what’s the account in React client, server always realize the http request is from the user test@abc.com. sqlite dbin Linux, sqlite data.db is stored at ~/.config/unity3d/&lt;company name&gt;/&lt;product name&gt;/data.db in Windows, data.db is stored at C:/users/username/AppData/LocalLow/&lt;company name&gt;/&lt;product name&gt;/data.db it’s interesting when register at lgsvlsimualtor.com, and it actually send the account info back to local db, which give the chance to bypass. debug webUIthe chrome and firefox has react-devtools plugins, which helps, but webUI doesn’t use it directly. to debug webUI it’s even simpler to go to dev mode in browser, and checking the few sections is enough refer bring your data to the front]]></content>
      <tags>
        <tag>lg-sim</tag>
        <tag>react</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deploy lgsvl in docker swarm 3]]></title>
    <url>%2F2019%2F12%2F15%2Fdeploy-lgsvl-in-docker-swarm-3%2F</url>
    <content type="text"><![CDATA[backgroundpreviously tried to run Vulkan in virtual display, which failed as I understand virtual display configure didn’t fit well with Vulkan. so this solution is direct display to allow each node has plugged monitor(which is called PC cluster). for future in cloud support, current solution won’t work. and earlier, also tried to deploy lgsvl in docker swarm, which so far can work with Vulkan as well, after a little bit understand X11 a few demo test can run as followning: deploy glxgears/OpenGL in PC cluster123export DISPLAY=:0 xhost + sudo docker service create --name glx --generic-resource &quot;gpu=1&quot; --constraint &apos;node.role==manager&apos; --env DISPLAY --mount src=&quot;X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; --mount src=&quot;tmp&quot;,dst=&quot;/root/.Xauthority&quot; --network host 192.168.0.10:5000/glxgears deploy vkcube/Vulkan in PC cluster123export DISPLAY=:0 xhost + sudo docker service create --name glx --generic-resource &quot;gpu=1&quot; --constraint &apos;node.role==manager&apos; --env DISPLAY --mount src=&quot;X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; --mount src=&quot;tmp&quot;,dst=&quot;/root/.Xauthority&quot; --network host 192.168.0.10:5000/vkcube deploy service with “node.role==worker”123export DISPLAY=:0xhost + sudo docker service create --name glx --generic-resource &quot;gpu=1&quot; --constraint &apos;node.role==worker&apos; --env DISPLAY --mount src=&quot;tmp&quot;,dst=&quot;/root/.Xauthority&quot; --network host 192.168.0.10:5000/glxgears deploy service in whole swarm123xhost + export DISPLAY=:0 sudo docker service create --name glx --generic-resource &quot;gpu=1&quot; --replicas 2 --env DISPLAY --mount src=&quot;tmp&quot;,dst=&quot;/root/.Xauthority&quot; --network host 192.168.0.10:5000/vkcube which deploy vkcube service in both manager and worker node: overall progress: 2 out of 2 tasks 1/2: running [==================================================&gt;] 2/2: running [==================================================&gt;] verify: Service converged understand .Xauthorityas docker service can run with --mount arguments, which give the first try to copy .Xauthority to manager node, but .X11-unix is not copyable, which is not a normal file, but a socket. in docker service create, when create a OpenGL/vulkan service in one remote worker node, and using $DISPLAY=:0, which means the display happens at the remote worker node. so in this way, the remote worker node is played the Xserver role; and since the vulkan service is actually run in the remote worker node, so the remote worker node is Xclient ? assuming the lower implement of docker swarm serviceis based on ssh, then when the manager node start the service, it will build the ssh tunnel to the remote worker node, and with the $DISPLAY variable as null; even if the docker swarm can start the ssh tunnel with -X, which by default, will use the manager node’s $DISPLAY=localhost:10.0 Xauthority cookie is used to grant access to Xserver, so first make sure which machine is the Xserver, then the Xauthority should be included in that Xserer host machine. a few testes: 1234567ssh in worker: echo $DISPLAY --&gt; localhost:10.0xeyes --&gt; display in master monitor ssh in worker: xauth list --&gt; &#123;worker/unix:0 MIT-MAGIC-COOKIE-1 19282b0a651789ed27950801ef6f1441; worker/unix:10 MIT-MAGIC-COOKIE-1 a6cbe81637207bf0c168b3ad20a9267a &#125;in master: xauth list --&gt; &#123; ubuntu/unix:1 MIT-MAGIC-COOKIE-1 ee227cb9465ac073a072b9d263b4954e; ubuntu/unix:0 MIT-MAGIC-COOKIE-1 75893fb66941792235adba22362c4a6f; ubuntu/unix:10 MIT-MAGIC-COOKIE-1 785f20eb0ade772ceffb24eadeede645 &#125; so which cookie is is for this $DISPLAY ? it shouldb be the one on ubuntu/unix:10; 12ssh in worker: export DISPLAY=:0xeyes --&gt; display in worker monitor then it use the cookie: worker/unix:0. deploy lgsvl service in swarm123xhost + export DISPLAY=:0sudo docker service create --name lgsvl --generic-resource &quot;gpu=1&quot; --replicas 2 --env DISPLAY --mount src=&quot;tmp&quot;,dst=&quot;/root/.Xauthority&quot; --network host --publish published=8080,target=8080 192.168.0.10:5000/lgsvl which gives: overall progress: 0 out of 2 tasks 1/2: container cannot be disconnected from host network or connected to host network 2/2: container cannot be disconnected from host network or connected to host network basically, the service is deployed in ingress network by default, but as well, the service is configured with host network. so it conflict. swarm network the routing mesh is the default internal balancer in swarm network; the other choice is to deploy serivce directly on the node, namely bypassing routing mesh, which ask the service run in global mode and with pubished port setting as mode=host, which should be the same as --network host in replicas mode. the limitation of bypassing routing mesh, is only one task on one node, and access the published port can only require the service from this special node, which doesn’ make sense in cloud env. 12345678910docker service create \ --mode global \ --publish mode=host,target=80,published=8080 \ --generic-resource &quot;gpu=1&quot; \ --env DISPLAY \ --mount src=&quot;tmp&quot;,dst=&quot;/root/.Xauthority&quot; \ --mount src=&quot;X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; \ --network host \ --name lgsvl \ lgsvl:latest tips: –mount src=”X11-unix”,dst=”/tmp/.X11-unix” is kind of cached. so once docker image has ran in worker node, then it doesn’t need to pass this parameter again, but once worker node restart, it need this parameter again in summary about swarm netowrk, the routing mesh should be the right solution for cloud deployment. so how to bypass --network host ? the reason of host network is the lgsvl server and webUI can works in same host; if not, there is kind of cross-domain security failures, which actually is another topic, namely, how to host lgsvl and webUI/React in different hosts. need some study of webUI in next post.]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>lg-sim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[where are you in next 4 years (1)]]></title>
    <url>%2F2019%2F12%2F08%2Fwhere-are-you-in-next-4-years-1%2F</url>
    <content type="text"><![CDATA[it’s more than one year since I started this series “where are you in the next 5 years”, I wound love to transfer to “the next 4 years”, and thanks for the opportunity to get back in China, so there is a high chance to involve in the market heavily in a short time. at the begining of the year, travelled around the whole nation, stay in Shanghai, Beijing, Shenzhen, Guangzhong. and that was a great chance to get familiar with the startups in autonomus vehicle, as this trip really gave me some input, and till now I had another half year in one of the top OEMs in China. combined with this two sources, which gave me the kind of the whole picture of ADS market happening in China. I would love to write this blog more in bussiness thought, rather than engineering way. L4 startupsADS leap time is about 2016 to the first half year of 2018. there are a bunch of startups and also most OEMs have build their ADS teams. the startups, e.g. Pony.ai, WeRide, AutoX, roadstar(the new split), ToSimple, Momenta. which are still very active recently. Today I have taken PlusAI’s tech open day, I have to say, most of these startups have very similar tech roadmap. I personally, think that’s a really sad thing. a few teches they all have: simulation pipeline data pipeline(collect, label, training) AI based perception, motion planning friendly HMI WeRide and Pony.ai are in Robtaxi service; ToSimple and PlusAI are in highway logistics; Momenta is in harbor transformation. Alibaba, jingdong, Meituan e.t.c are in small personal package delivery shuttles, similar as Nuro. OEMs focus in the passenger vehicles. DiDi focus in taxi services as well, similar as Uber, Waymo. all of them can be called as ADS service suppliers. however, most of them use the exactly same sensor packages, including Lidar, Camera, Radar, GPS e.t.c. the software stacks during prodcut dev as mentioned above are mostly similar; there maybe a few special features during the services in deployment, e.g. Robtaxi may have a Uber-like call-taxi app e.t.c, Rather than that, nothing really is amazing about ADS itself. and mostly this is not a tech problem, it’s must be defined or find out by the social guys, who are from the real needs. in the engineering work environment, it’s easily to misunderstand the role of engineering. engineering is the bumper, only when the house need fixed, there is a need for bumper. However, in an engineering-centered env, it’s so easy to tell no difference between I have the bumper and I have the needs. My experieince till now, I am learning how to use the bumper well, but few thinks why need to learn to use the bumper. on the other hand, what kind of tech is really helpful or profitable? by chance, to talk with Unity China team, who are enhancing Unity3D engine with cloud support, unity simualtion, which is the feature I am looking for a while. if the tech pipeline is the waterflow, the Unity team is the one standing at the upper flow, who can implement new features in engine. just like Nvidia, Google e.t.c, these are the guys who really make a difference with their tech. and it’s profitable of course.]]></content>
  </entry>
  <entry>
    <title><![CDATA[where are you in next 4 years (2)]]></title>
    <url>%2F2019%2F12%2F08%2Fwhere-are-you-in-next-4-years-2%2F</url>
    <content type="text"><![CDATA[backgroudjoined the zhongguancun self-driving car workshow today, different level than PlusAI’s tech show at last weekend. there are a few goverment speakers, e.g. zhongguancun tech officer. and sensor suppliers, e.g. SueStar, zhongke hui yan, holo matic, self-driving solution suppliers, e.g. pony.ai; xiantong tech e.t.c, and media press and investors, interesting thing the investors doesn’t look promising. as mentioned last time, the ADS solution suppliers and most sensor startups join the champion, in about 2 years. and goverment are invovled with policy-friendy support. just the captial market or investors don’t buy it at this moment. XianTongXianTong, focused in city road cleaning, which is a special focus, rather than passenger vehicles, pacakge trucks, or small delivery robotics. They have some data in the cleanning serivces in large cities, e.g. beijing, shanghai, hangzhou, and Europen. current cleaner laybor’s harsh and dangerous working env current cleaner laybor’s limitation to working hours, benefits requirements they mentioned the city cleaning market is about 300 billions in China, which looks promising, but how much percent of the cleaning vechiles in this market is not talked. it’s maybe about 20% ~ 60%, as there are a lot human resources, city green plant needs e.t.c, which eats a lot of money, and the current cleaner vehicle products that support ADS maybe has an even smaller part in the whole vehicles used in city cleaning services. so the whole city clearning service market sounds promising, but down to the clearning vehicles, and especailly without a matured and in-market cleaner vehicle product, it’s really difficult to digest and dig golden from the market. I have a feeling, most startups has the similar gap, they do vision big, e.g. to assist the city, the companies, the bussiness, the end customers run/live more efficiently/enjoyable/profitable. but the reality is not that friendly for them, as they spent investor’s money, which expect to get profitable return in a short time. which may push the startups to draw big pictures far beyond their ability, or even far beyond the whole industry’s ability. as well as they draw big pictures, they are very limited to deep into the market, to understand the customers, to design the products with original creativity. creativity or applicationfor investors, these a special industry-application based startups, I think, at most may get investing profit at 1 ~ 4 times. maybe it’s a good idea to understand the successful invest cases happened in last 5 years. And I am afraid that’s also a self-centered option, that most CV happened in high-tech, Internet-based startups. cause the current self-driving market, especially the startups in China, which focus in ADS full-stack solutions, sensors, services providers, are not a game-change player. in history, the first successful company to product PCs, smart phones, Internet searching serivce, social network servie, taxing service, booking (restrount) service, food delivery service, they are game changers. and somehow they are most talked in public, in investers, and most understandable by most knowledge consumers, e.g. engineers. but are they the whole picture of the national economy? what about the local seafood resturant, the local auto repair shops; or the company who product pig’s food, who product customers’ bikes; or the sales company who sell steel to Afria. The economy is more plentiful and complex, than a mind-straight knowledge consumer can image. For a long time, I didn’t realise the local food restarant or a local gym, but they do higher money payback, and definitely higher social status, than a fixed income engineer. so don’t try to find out the secret of each component of the whole economy, and then try to find out the most optimized way to live a life. there is no, or every way to live a life, it is the most optimized way. so the CEOs of these startups, are not crazy to image themselves as the game changers, like Steve Jobs, so they know their energy. surviving firstsince as they know their limitation, so they are not nuts, so they are just enough energy to find a way to survive, even not in a good shape. that’s the other part, as an naive young man, always forgot the fact that surviving is not beautiful most times. the nature has tell the lesson: the young brids only kill his/her brothers/sisters, then he/she can survive to grow into adult. for the deers in Afria plant, each minite is either run to survive or being eaten. companies or human as a single, has the same situation, even the goverment try to make it a little less harsh, but most time to survive is difficult. market sharingas a young man, 1000 billion market sounds like a piece of cake, when comes to the small company, who work so hard to get a million level sale, sounds like a piece of shit. and that’s another naive mind about the reality. like I can’ see the money return from a local seafood resturout, and when I found out it does get more than a million every year, the whole me is shocked. so there is no big or small piece of cake, as it come to survive. most CEOs, they are not nuzy, and they know clearly in their heart, that their company need to survivie and make a profitable income, and that’s enough, to change the world is not most people’s responsibility, but survive it is. however, these CEOs in publich, they talk their company or product as the game-changers, that’s what the investors’ want they to say. so don’t think the market is too small any more, as well as it can support a family to live a life. dream is not the option to most people, that’s the final summary. but survive is a have-to. life is evergreenlife is not only about surviving. if else, human society is an animal world. thinking reasonal always give the doomed impression, and the life in blue; once more perceptual in mind, the day is sunny and joyful. “the theory is grey, but the tree of life is evergreen” develop team in OEMas mentioned, currently there are plenty of system simulation verification special suppliers, e.g. 51 vr, baidu, tencent, alibaba e.t.c, and definitely there softwares are more matured than our team. I am afriad jsut at this moment, the leader teams in OEM don’t realize that to build a simualtion tool espcially to support L3 is mission-impossible. if else, the requirements for simulation, should come from suppliers, rather than build OEM’s own simualtion develop team. I still remember the first day to join this team, sounds like we are really creating something amazing, and nobody else there have more advantages than us. then gradually I realize we can’t customize the Unity engine, we can’t support matured cloud support, we can’t even implement the data pipeline for road test log and analysis. most current team members work stay in requirements level, and in a shadow mode. and actually most of these needs, does have a few external companies/teams have better solution. there does a lot existing issues, from software architecture to ADS algorithms restructure, but these work is mostly not done by OEM develop team. the second-level developing team, can’t make the top-class ADS product. as the new company will split out, this team is less possible to survive in the market, or the leaders have to set up a new developing team. if AI is the direction, that’s another huge blank for this team. I think either go to the ADS software architecture or to AI is a better choice now.]]></content>
  </entry>
  <entry>
    <title><![CDATA[running Vulkan in virtual display]]></title>
    <url>%2F2019%2F12%2F04%2Frunning-Vulkan-in-virtual-display%2F</url>
    <content type="text"><![CDATA[install xserver-xorg-video-dummyapt-cache search xserver-xorg-video-dummy apt-get update sudo apt-get install xserver-xorg-video-dummy which depends on xorg-video-abi-20 and xserver-xorg-core, so need to install xserver-xorg-core first. after update xorg.conf as run Xserver using xserver-xorg-video-dummy driver, and reboot the machine, which leads both keyboard and mouse doesn’t reponse any more. understand xorg.confusually, xorg.conf is not in system any more, so most common use, the xorg will configure the system device by default. if additional device need to configure, can run in root X -configure, which will generate xorg.conf.new file at /root. there are two xorg.conf, one generated by running X -configure, which located at /root/xorg.conf.new ; the other is generated by nvidia-xconfigure, which can be found at /etc/X11/xorg.conf. the following list is from xorg.conf doc ServerLayout section it is at the highest level, they bind together the input and output devices that will be used in a session. input devices are described in InputDevice sections, output devices usualy consist of multiple independent components(GPU, monitor), which are defined in Screen section. each Screen section binds togethere a graphics board(GPU) and a monitor. the GPU are described in Device sections and monitors are described in Monitor sections FILES section used to specify some path names required by the server. e.g. ModulePath, FontPath .. SERVERFLAGS section used to specify global Xorg server options. all should be Options &quot;AutoAddDevices&quot;, enabled by default. MODULE section used to specify which Xorg server (extension) modules shoul be loaded. INPUTDEVICE section Recent X servers employ HAL or udev backends for input device enumeration and input hotplugging. It is usually not necessary to provide InputDevice sections in the xorg.conf if hotplugging is in use (i.e. AutoAddDevices is enabled). If hotplugging is enabled, InputDevice sections using the mouse, kbd and vmmouse driver will be ignored. Identifier and Driver are required in all InputDevice sections. Identifier used to specify the unique name for this input device; Driver used to specify the name of the driver. An InputDevice section is considered active if it is referenced by an active ServerLayout section, if it is referenced by the −keyboard or −pointer command line options, or if it is selected implicitly as the core pointer or keyboard device in the absence of such explicit references. The most commonly used input drivers are evdev(4) on Linux systems, and kbd(4) and mousedrv(4) on other platforms. a few driver-independent Options in InputDevice: CorePointer and CoreKeyboard are the inverse of option Floating, which, when enabled, the input device does not report evens through any master device or control a cursor. the device is only available to clients using X input Extension API. Device section there must be at least one, for the video card(GPU) being used. Identifier and Driver are required in all Device sections. Monitor Sectionthere must be at least one, for the monitor being used. the default configuration will be created when one isn’t specified. Identifier is the only mandatory. Screen SectionThere must be at least one, for the “screen” being used, represents the binding of a graphics device (Device section) and a monitor (Monitor section). A Screen section is considered “active” if it is referenced by an active ServerLayout section or by the −screen command line option. The Identifier and Device entries are mandatory. debug keyboard/mouse not response after X upgrade login to Ubuntu safe mode, by F12 –&gt; Esc (to display GRUB2 menu), then enable network –&gt; root shell run X -configure one line say: List of video drivers: dummy, nvidia, modesetting. uninstall xserver-xorg-video-dummyI thought the dummy video driver is the key reason, so uninstall it, then rerun the lines above, check /var/log/Xorg.0.log: 1234567891011121314151617181920212223[ 386.768] List of video drivers:[ 386.768] nvidia[ 386.768] modesetting[ 386.860] (++) Using config file: &quot;/root/xorg.conf.new&quot;[ 386.860] (==) Using system config directory &quot;/usr/share/X11/xorg.conf.d&quot;[ 386.860] (==) ServerLayout &quot;X.org Configured&quot;[ 386.860] (**) |--&gt;Screen &quot;Screen0&quot; (0)[ 386.860] (**) | |--&gt;Monitor &quot;Monitor0&quot;[ 386.861] (**) | |--&gt;Device &quot;Card0&quot;[ 386.861] (**) | |--&gt;GPUDevice &quot;Card0&quot;[ 386.861] (**) |--&gt;Input Device &quot;Mouse0&quot;[ 386.861] (**) |--&gt;Input Device &quot;Keyboard0&quot;[ 386.861] (==) Automatically adding devices[ 386.861] (==) Automatically enabling devices[ 386.861] (==) Automatically adding GPU devices[ 386.861] (**) ModulePath set to &quot;/usr/lib/xorg/modules&quot;[ 386.861] (WW) Hotplugging is on, devices using drivers &apos;kbd&apos;, &apos;mouse&apos; or &apos;vmmouse&apos; will be disabled.[ 386.861] (WW) Disabling Mouse0[ 386.861] (WW) Disabling Keyboard0Xorg detected mouyourse at device /dev/input/mice.Please check your config if the mouse is still notoperational, as by default Xorg tries to autodetectthe protocol. there is a warning: (WW) Hotplugging is on, devices using drivers &#39;kbd&#39;, &#39;mouse&#39; or &#39;vmmouse&#39; will be disabled disable Hotpluggingfirst generate by X -configure at /root/xorg.conf.new, and copy it to /etc/X11/xorg.conf. then add the additional section in /etc/X11/xorg.conf, , which will disable Hotplugging: 1234Section &quot;ServerFlags&quot;Option &quot;AllowEmptyInput&quot; &quot;True&quot;Option &quot;AutoAddDevices&quot; &quot;False&quot;EndSection however, it reports: 12345678(EE) Failed to load module &quot;evdev&quot; (module does not exist, 0)(EE) NVIDIA(0): Failed to initialize the GLX module; please check in your X(EE) NVIDIA(0): log file that the GLX module has been loaded in your X(EE) NVIDIA(0): server, and that the module is the NVIDIA GLX module. If(EE) NVIDIA(0): you continue to encounter problems, Please try(EE) NVIDIA(0): reinstalling the NVIDIA driver.(EE) Failed to load module &quot;mouse&quot; (module does not exist, 0)(EE) No input driver matching `mouse&apos; switch to nvidia xorg.confwhich reports: 1234(EE) Failed to load module &quot;mouse&quot; (module does not exist, 0)(EE) No input driver matching `mouse&apos;(EE) Failed to load module &quot;evdev&quot; (module does not exist, 0)(EE) No input driver matching `evdev&apos; it fix the Nvidia issue, but still can’t fix the input device and driver issue. switch to evdev driveras mentioned previously, evdev driver is the default driver for Linux, and will be loaded by Xserver by default. so try to both Keyboard and Mouse driver to evdev, which reports: 1234(EE) No input driver matching `kbd&apos;(EE) Failed to load module &quot;kbd&quot; (module does not exist, 0)(EE) No input driver matching `mouse&apos;(EE) Failed to load module &quot;mouse&quot; (module does not exist, 0) looks it’s the problem of driver, even the default driver is missed. I try to copy master node’s /usr/lib/xorg/modules/input/ to worker node, then it reports : 12(EE) module ABI major version (24) doesn&apos;t match the server&apos;s version (22)(EE) Failed to load module &quot;evdev&quot; (module requirement mismatch, 0) which can be fixed by adding Option IgnoreABI . delete customized keyboard and mouseif enable Hotplugging, the X will auto detect the device, I’d try: 1234567891011121314151617181920212223242526272829303132333435Section &quot;ServerLayout&quot; Identifier &quot;Layout0&quot; Screen 0 &quot;Screen0&quot; 0 0 EndSectionSection &quot;Monitor&quot; Identifier &quot;Monitor0&quot; VendorName &quot;Unknown&quot; ModelName &quot;Unknown&quot; HorizSync 28.0 - 33.0 VertRefresh 43.0 - 72.0 Option &quot;DPMS&quot;EndSectionSection &quot;Device&quot; Identifier &quot;Device0&quot; Driver &quot;nvidia&quot; VendorName &quot;NVIDIA Corporation&quot;EndSectionSection &quot;Screen&quot; Identifier &quot;Screen0&quot; Device &quot;Device0&quot; Monitor &quot;Monitor0&quot; DefaultDepth 24 SubSection &quot;Display&quot; Depth 24 EndSubSectionEndSectionSection &quot;ServerFlags&quot; Option &quot;AllowEmptyInput&quot; &quot;True&quot; Option &quot;IgnoreABI&quot; &quot;True&quot;EndSection which reports: 123(II) No input driver specified, ignoring this device.(II) This device may have been added with another device file.(II) config/udev: Adding input device Lenovo Precision USB Mouse (/dev/input/mouse0 there is no ERROR any more, but looks the default Input driver (evdev?) can’t be found out … reinstall xorgMouse and keyboard can be driven by evdev or mouse/keyboard driver respectively. Xorg will load only endev automatically, To use mouse and/or keyboard driver instead of evdev they must be loaded in xorg.conf. There is no need to generate xorg.conf unless you want to fine tune your setup or need to customize keyboard layout or mouse/touchpad functionality. firstly configure new network interface for worker node: configure DHCP network connection setting at /etc/network/interface: 12auto enp0s25 iface enp0s25 inet dhcp ifconfig enp0s25 downifconfig enp0s25 up then reinstall xorg: sudo apt-get update sudo apt-get upgrade sudo apt-get install xserver-xorg-core xserver-xorg xorg which install these libs, xserver-xorg-input-all, xserver-xorg-input-evdev, xserver-xorg-inut-wacom, xserver-xorg-input-vmouse, xserver-xorg-input-synaptics, these are the exact missing parts(input device and drivers). it looks when uninstall video-dummy, these modules are deleted by accident. reboot, both keyboard and mouse work ! “sudo startx” through ssh now the user password doesn’t work in normal login, but when ssh login from another machine, the password verify well. which can be fixed by ssh login from remote host first, then run sudo startx, which will bring the user-password verification back virtual displayxdummy xdummy: xorg.conf run: Xorg -noreset +extension GLX +extension RANDR +extension RENDER -logfile ./10.log -config ./xorg.conf :10 test with glxgears/OpengGL works 1) DISPLAY=localhost:10.0 works 2) DISPLAY=:0 works, but you can’t see it, cause the worker host is in virtual display test with vkcube/Vulkan failed in summary, the vitual display can support OpenGL running, but doesn’t support Vulkan yet. unity simulation cloud SDK is the vendor’s solution, but licensed. refersample xorg.conf for dummy device Keyboard and mouse not responding at reboot after xorg.conf update how to Xconfigure no input drivers loading in X]]></content>
  </entry>
  <entry>
    <title><![CDATA[recent thoughts in ADS]]></title>
    <url>%2F2019%2F11%2F29%2Frecent-thoughts-in-ADS%2F</url>
    <content type="text"><![CDATA[the following are some pieces of ideas during discussion and online resources. system engineering in practicalthe following idea is coming from the expert of system engineering. originally, system engineering or model based design sounds come from aerospace, defense department. the feature of these products: 1) they are the most complex system 2) they are sponsored by the goverment 3) they are unique and no competitors which means they don’t need worry about money and time, so to gurantee the product finally works, they can design from top to down in a long time. the degrade order of requirements level comes as: areospace, defense product &gt;&gt; vehicle level product &gt;&gt; industry level product &gt;&gt; customer level product usually the techs used in the top level is gradually degrading into the next lower level in years. e.g. GPS, Internet, autonomous e.t.c. at the same time, the metholodies from top level go to lower level as well. I suppose that’s why system engineeering design comes to vehicle industry. however, does it really work in this competitional industry? I got the first experince when running scenaio testes in simulation SIL. as the system engineering team define the test cases/scenarios, e.g. 400 test scenarios; on the other hand, the vehicle test team does the road test a few times every week.the result is, most time the 400 test scenarios never catch a system failure; but most road test failure scenario does can be repeated in the simulation env. system engineering based design doesn’t fit well. there are a lot reasons. at first, traditionally the design lifetime of a new vehicle model is about 3~5 years, and startup EV companies recently has even shorter design life cycle, about 1~2 years. so a top-down design at the early time, to cover every aspect of a new model, does almost not make sense. in the V development model, most fresh engineers thought the top-down design is done once for all, the reality is most early stage system engineering desgin need be reconstructured. secondly, system engineering design usually is abstract and high beyond and except engineering considerations, as the system engineers mostly doesn’t have engineering experience in most sections of the sysetem. which results in the system engineering based requirements are not testable, can’t measure during section implementation. there are a few suggestions to set a workable system engineering process: the system engineering team should sit by the develop teams and test teams, they should have a lot of communication, and balance the high-level requirements and also testable, measurable, implementable requirements. basically, system engineering design should have product/component developers as input. both the system engineers and developers should understand the whole V model, including system requirements, component requirements are iteratable. focus on the special requirement, and not always start from the top, each special requirement is like a point, and all these existing points(already finished requirements) will merged to the whole picture finally. take an example, during the road test, there will come a new requirement to have a HMI visulization, then focus on this HMI requirement, cause this requirements may not exist in the top down design. but it is the actual need. system test and verification CI/CDas most OEMs have said they will massive product L3 ADS around 2022, it is the time to jump into the ADS system test and verification. just knew that Nvidia has the full-stack hardware lines: the AI chips in car(e.g. Xavier), the AI training workstation(e.g. DGX), and the ADS system verifcation platform(e.g. Constallation box). data needsthe ADS development depends a lot on data infrastructure: data collect --&gt; data storage --&gt; data analysis there are many small pieces as well, e.g. data cleaning, labeling, training, mining, visulization e.t.c from different dev stage or teams, there are different focus. road test/sensor team, they need a lot of online vehicle status/sensor data check, data logging, visulization(dev HMI), as well as offline data analysis and storage perception team, need a lot of raw image/radar data, used to train, mine, as well as to query and store. planning/control team, need high quality data to test algorithms as well as a good structured in-car computer. HMI team, are focusing on friendly data display fleet operation team, need think about how to transfer data in cloud, vehicle, OEM data centers e.t.c. sooner or later, data pipepline built up is a have to choice. data collection vendorsroad test data collection equipment used in ADS development, is actually not a very big market, compared to in-var computers. but still there are a few vendors already. the top chip OEMs, e.g. Nvidia, Intel has these products. chip poxy, e.g. Inspire traditional vehicle test vendors, e.g. Dspace, Vector, Prescan startups, e.g. horizon Nvidia constellationADS system test usually includes simulation test and road test. and the road test is also called vehicle-in-loop, which is highly expensive and not easy to repeat; then is hardware-in-loop(HIL) test, basically including only the domain controller/ECU in test loop; finally is the is software-in-loop(SIL) test, which is most controllable but also not that reliable. in practical, it’s not easy to build up a closed-loop verification(CI/CD) process from SIL to HIL to road test. and once CI/CD is setted up, the whole team can be turned into data/simulation/test driven. the difficult and hidden part is the supporting toolchain development. Most vehicle test vendors have their special full-stack solution toolchains, but most of them are too eco-customized, it’s really difficult for ADS team, specially OEMs, to follow a speical vendor solution. another reason, test vehicles include components from different vendors, e.g. camera from sony, radar from bosch, Lidar from a Chinese startup, logging equipment from dSpace, and ECUs from Conti. which makes it difficult to fit this mixed system into a Vector verification platform. Nvidia Constallation is trying to meet the gap from SIL, HIL to road test. as it can suppport most customized ECUs. from road test to HIL, it use the exactly same chip. for road test resimulation, Nvidia offer a sim env, and the road test log can feed in directly the ability to do resimulation of road test is a big step, the input is directly scanned images/cloud points, even lgsvl, Carla has no such direct support. but resimulation is really useful for CI/CD. Nvidia constallation as said, is the solution from captured data to KPIs. another big thing is about their high-level scenario description language(HLSDL), which I think is more abstract than OpenScenario. the HLSDL engine use hyper-parameters, SOTIF embedded scenario idea, and optimized scenario generator, which should be massive, random as well as KPI significantly, it should be a good scenario engine if it has these features. Bosch VMSvehicle management system(VMS) is cloud nature framework from Bosch, which is used to meet the similar requirements as Nvidia’s solution, to bring the closed-loop(CI/CD) from road test data collection, data anlaysis to fleet management. they have a few applications based on VMS: fleet batteries management(FBM) for single vehicle’s diaglostic, prediction; and for the EV market, FBM can be used as certification for second-hand EV dealers road coefficient system(RCS) Bosch has both in-vehicle data collection box and cloud server, RCS will be taken as additional sensor for ADS in prodcut VMS in itself Bosch would like to think VMS as the PLM for ADS, from design, test, to deployment. and it shoul be easy to integrate many dev tools, e.g. labeling, simulation e.t.c what about safetyas mentioned previously, 80% of Tesla FSD is to handle AI computing, Nvidia Xavier has about 50% GPU; Mobileye has very limited support for AI. all right, Tesla is most AI aggressive, then Nvidia, then Mobileye is most conserved. which make OEMs take Mobileye solution as more safety, but AI does better performance in perception, so how to balance these two ways? I realized the greats of Mobileye’s new concept: responsibility sensitive safety(RSS), RSS can be used as the ADS safety boundary, but inside either AI or CV make the house power. a lot of AI research on mixed traditional algorithms with AI algorithms, RSS sounds the good solution. would be nice to build a general RSS Mixing AI(RMA) framework.]]></content>
  </entry>
  <entry>
    <title><![CDATA[X11 GUI in docker]]></title>
    <url>%2F2019%2F11%2F29%2FX11-GUI-in-docker%2F</url>
    <content type="text"><![CDATA[XorgXorg is client-server architecture, including Xprotocol, Xclient, Xserver. Linux itself has no graphics interface, all GUI apps in Linux is based on X protcol. Xserver used to manage the Display device, e.g. monitor, Xserver is responsible for displaying, and send the device input(e.g. keyboard click) to Xclient. Xclient, or X app, which includes grahics libs, e.g. OpenGL, Vulkan e.t.c xauthorityXauthority file can be found in each user home directory and is used to store credentials in cookies used by xauth for authentication of X sessions. Once an X session is started, the cookie is used to authenticate connections to that specific display. You can find more info on X authentication and X authority in the xauth man pages (type man xauth in a terminal). if you are not the owner of this file you can’t login since you can’t store your credentials there. when Xorg starts, .Xauthority file is send to Xorg, review this file by xauth -f ~/.Xauthority ubuntu@ubuntu:~$ xauth -f ~/.XauthorityUsing authority file /home/wubantu/.Xauthorityxauth&gt; listubuntu/unix:1 MIT-MAGIC-COOKIE-1 ee227cb9465ac073a072b9d263b4954eubuntu/unix:0 MIT-MAGIC-COOKIE-1 71cdd2303de2ef9cf7abc91714bbb417ubuntu/unix:10 MIT-MAGIC-COOKIE-1 7541848bd4e0ce920277cb0bb2842828 Xserver is the host who will used to display/render graphics, and the other host is Xclient. if Xclient is from remote host, then need configure $DISPLAY in Xserver. To display X11 on remote Xserver, need to copy the .Xauthority from Xserver to Xclient machine, and export $DISPLAY and $XAUTHORITY 12export DISPLAY=&#123;Display number stored in the Xauthority file&#125;export XAUTHORITY=&#123;the file path of .Xauthority&#125; xhostXhost is used to grant access to Xserver (on your local host), by default, the local client can access the local Xserer, but any remote client need get granted first through Xhost. taking an example, when ssh from hostA to hostB, and run glxgears in this ssh shel. for grahics/GPU resources, hostA is used to display, so hostA is the Xserver. x11 forwardingwhen Xserver and Xclient are in the same host machine, nothing big deal. but Xserver, Xclient can totally be on different machines, as well as Xprotocol communication between them. this is how SSH -X helps to run the app in Xclient, and display in Xserver, which needs X11 Forwarding. test benchmark12ssh 192.16.0.13xeyes /tmp/.X11-unixthe X11(xorg) server communicates with client via some kind of reliable stream of bytes. A Unix-domain socket is like the more familiar TCP ones, except that instead of connecting to an address and port, you connect to a path. You use an actual file (a socket file) to connect. srwxrwxrwx 1 root root 0 Nov 26 08:49 X0 the s in front of the permissions, which means its a socket. If you have multiple X servers running, you’ll have more than one file there. is where X server put listening AF_DOMAIN sockets. DISPLAY deviceDISPLAY format: hostname: displaynumber.screennumber hostname is the hostname or hostname IP of Xserver displaynumber starting from 0screennumber starting from 0 when using TCP(x11-unix protocol only works when Xclient and Xserver are in the same machine), displaynumber is the connection port number minus 6000; so if displaynumber is 0, namely the port is 6000. DISPLAY refers to a display device, and all graphics will be displayed on this device.by deafult, Xserver localhost doesn’t listen on TCP port. run: sudo netstat -lnp | grep &quot;6010&quot;, no return. how to configure Xserver listen on TCP 1Add DisallowTCP=false under directive [security] in /etc/gdm3/custom.conf file. Now open file /etc/X11/xinit/xserverrc and change exec /usr/bin/X -nolisten tcp to exec /usr/bin/X11/X -listen tcp. Then restart GDM with command sudo systemctl restart gdm3. To verify the status of listen at port 6000, issue command ss -ta | grep -F 6000. Assume that $DISPLAY value is :0. virtual DISPLAY devicecreating a virtual display/monitoradd fake display when no Monitor is plugged in Xserver broadcastthe idea behind is to stand in one manager(Xserver) machine, and send command to a bunch of worker(Xclient) machines. the default way is all Xclient will talk to Xserver, which eat too much GPU and network bandwith resources on manager node. so it’s better that each worker node will do the display on its own. and if there is no monitor on these worker nodes, they can survive with virtual display. xvfbxvfb is the virtual Xserver solution, but doesn’t run well(need check more) nvidia-xconfigconfigure X server to work headless as well with any monitor connected unity headlessenv setupto test with docker, vulkan, ssh, usually need the following packages: vulkan dev envsudo add-apt-repository ppa:graphics-drivers/ppa sudo apt upgrade apt-get install libvulkan1 vulkan vulkan-utils sudo apt install vulkan-sdk nvidia envinstall nvidia-driver, nvidia-container-runtime install mesa-utils #glxgears docker envinstall docker run glxgear/vkcube/lgsvl in docker through ssh tunnelthere is a very nice blog: Docker x11 client via SSH, disccussed the arguments passing to the following samples run glxgearglxgear is OpenGL benchmark test. 12ssh -X -v abc@192.168.0.13sudo docker run --runtime=nvidia -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v &quot;$HOME/.Xauthority:/root/.Xauthority&quot; --net=host 192.168.0.10:5000/glxgears if seting $DISPLAY=localhost:10.0 , then the gears will display at master node(ubuntu) if setting $DISPLAY=:0, then the gears will display at worker node(worker) and w/o /tmp/.X11-unix it works as well. run vkcubevkcube is Vulkan benchmark test. 123ssh -X -v abc@192.168.0.13export DISPLAY=:0sudo docker run --runtime=nvidia -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v &quot;$HOME/.Xauthority:/root/.Xauthority&quot; --net=host 192.168.0.10:5000/vkcube in this way, vkcube is displayed in worker node(namely, using worker GPU resource), manager node has no burden at all. if $DISPLAY=localhost:10.0, to run vkcube, give errors: No protocol specified Cannot find a compatible Vulkan installable client driver (ICD). Exiting ... looks vulkan has limitation. run lgsvl123export DISPLAY=:0sudo docker run --runtime=nvidia -ti --rm -p 8080:8080 -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v &quot;$HOME/.Xauthority:/root/.Xauthority&quot; --net=host 192.168.0.10:5000/lgsvl /bin/bash ./simulator works well! the good news as if take the manager node as the end user monitor, and all worker nodes in cloud, without display, then this parameters will be used in docker service create to host in the swarm. so the next step is to vitual display for each worker node. referwiki: Xorg/XserverIBM study: Xwindowscnblogs: run GUI in remote serverxorg.conf in ubuntuconfigure XauthorityX11 forwarding of a GUI app running in dockercnblogs: Linux DISPLAY skillsnvidia-runtime-container feature: Vulkan support]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>X11</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[what about Tesla]]></title>
    <url>%2F2019%2F11%2F26%2Fwhat-about-Tesla%2F</url>
    <content type="text"><![CDATA[Tesla is ROCKING the industry. OTA, camera only, fleet learning, shadow mode, Autopilot, Gega Factory, Cybertruck etc. There is a saying: “看不到，看不懂，追不上(can’t see it; can’t understand it; can’t chase it)”. I have to say most Tesla news are more exciting than news from traditional OEMs. and my best wishes to Tesla to grow greater. Tesla timeline2019 release Cybertruck Tesla software V10.0 OTA: Smart Summon(enable vehicle to navigate a parking lot and come to them or their destination of choice, as long as their car is within their line of sight) Driving Visualization(HMI) Automatic lane change lane departure avoidance: Autopilot will warn the driver and slow down the vehicle emergency lane departure avoidance: Autopilot will steer the vehciel back into the driving lane if the off-lane may lead to collision Model 3 safety reward from IIHS and Euro NCAP Tesla Insurance Megapack: battery storage for massive usage V3 super charging station: more powerful station and pre-heating battery Powerpack: energy storage system in South Austrilia Model 3 release (March) and the most customer satisfied vehicles in China cut-off 7% employees globally(Jan) 2018 race mode Model 3 Model 3 the lowest probability of injury by NHTSA: what make Model 3 safe Tesla V9.0 OTA: road status info climate control Navigate on Autopilot Autosteer and Auto lane change combination blindspot warning(when turn signal in engaged but a vehicle or obstacle is detected in the target lane) use high occupancy vehicle(HOV) lane obstacle aware acceleration(if obstacle detected, acc is automatically limited) Dashcam (record and store video footage) Tesla privatization (Aug) 2017 super charing station 10000 globally collabration with Panasonic to produce battery at Buffalo, 1Mpw 2016 purchase SolarCity purchase Grohmann Enginering(German): highly automatic manufacture massive product of Tesla vehicles with hardwares to support fully self driving(Oct) 8 cameras to support 360 view, in front 250 meters env detection 12 Ultrasonic front Radar ADS HAD (x40 powerful than previous) ADS algorithm: deep learning network combine with vision, radar and ultrasonic(AEB, collision warning, lane keeping, ACC is not included yet) Autopilot 8.0 OTA, or namely “see” the world through Radar Tesla’s master plane 2 deadly accident across the truck(2016.7) HEPA defense “biological weapon” accept reservation for Model 3(march) Tesla 7.1.1 OTA: remote summon Tesla 7.1 OTA: vertical parking speed gentelly in living house area highway ACC, traffic jam following more road info in HMI, e.t.c truck, bus, motobike 2015 Autopilot 7.0 update: Autopark(requires driver in the car and only parallel parking) Autosteer Auto lane changing UI refresh Automatic emergency steering side collision warning Autopilot evoluvationin a nutshell, Autopilot is dynamic cruise control(ACC) + Autosteer + auto lane chang. Autopilot 7.0 relied primarily on the front-facing camera. radar hasn’t been used primarily in 7.0 was due to false positives(wrong detection). but in 8.0 with fleet learning. 8.0 made radar the main sensor input. almost entirely eliminate the false positive – the false braking events – and enable the car to initiate braking no matter what the object is as long as it is not large and fluffy but any large, or metallic or dense, the radar system is able to detect and initiate a braking event. both when Autopilot active or not(then AEB) even if the vision system doesn’t recognize the object, it actually doesn’t matter what the object is(while vision does need to know what the thing is), it just knows there is somehting dense. fleeting learning will mark the geolocation of where all the false alarm occurs, and what the shape of that object. so Tesla system know at a particular position at a particular street or highway, if you see a radar object of a following shape - don’t worry it’s just a road sign or bridge or a Christmas decoration. basically marking these locations as a list of exceptions. the radar system can track 2 cars/obstacles ahead and imporove the cut-in , cut-off reponse. so in case the car in front suddenly swerve out of the way of an obstacle. the limit of hardware is reaching, but there will be still a quite improvement as the software and data would improve quite amount. but still perfect safety is really an impossible goal, it’s really about improving the probability of safety. in Autopilot 9.0, Navigate on Autopilot(Beta) intelligently suggests lane changes to keep you on your route in addition to making adjustments so you don’t get stuck behind slow cars or trucks. Navigate on Autopilot will also automatically steer toward and take the correct highway interchanges and exits based on your destination. Autopilot is keeping evaluation with more exicting features: traffic light and stop signs detection enhanced summon naviagte multi-story parking lots automaticly send off vehicle to park Autopilot on city streets Robotaxi service Tesla HardwareHardware 1.0or Autopliot 1 or AP1, it was a joint development between Mobileye and Tesla. It featured a single front-facing camera and radar to sense the environment plus Mobileye’s hardware and software to control the driving experience. AP1 was so good that when Tesla decided to build their own system, it took them years to catch up to the baseline Autopilot functionality in AP1. Mobileye EyeQ3 is good to mark/label in free space, intuitive routing, obstacle-avoid, and traffic signal recognization etc. but it has a few limitations to env light, and reconstruct 3D world from 2D images etc does work as expect all the time. and EyeQ3 detects objects with traditional algorithms, not cool! AP1 Hardware Suite: Front camera (single monochrome) Front radar with range of 525 feet / 160 meters 12 ultrasonic sensors with 16 ft range / 5 meters Rear camera for driver only (not used in Autopilot) Mobileye EyeQ3 computing platform AP1 Core features: Traffic-Aware Cruise Control (TACC), start &amp; stop Autosteer (closed-access roads, like freeways) Auto Lane Change (driver initiated) Auto Park Summon Hardware 2.0AP2 highlights machine learning/neurual networks with camera inputs, so with more sensors and more powerful computing platforms. AP2 Hardware Suite: Front cameras (3 cameras, medium, narrow and wide angle) Side cameras (4 total, 2 forward and 2 rear-facing, on each side) Rear camera (1 rear-facing) Front radar with range of 525 feet / 160 meters 12 ultrasonic sensors with 26 ft range / 8 meters NVIDIA DRIVE PX 2 AI computing platform AP2 Core features: Traffic-Aware Cruise Control (TACC), start &amp; stop Autosteer (closed-access roads, like freeways) Auto Lane Change (driver initiated) Navigate on Autopilot (on-ramp to off-ramp) Auto Park Summon there was AP2.5 update, with redundant NVIDIA DRIVE PX2 and forward radar with longer range (170m) Hardware 3.0or Full Self Driving(FSD) Computer, Telsa guysSterling Anderson from 2015 - 2016, director of Autopilot program. Chris Latter in early 2017, VP for Autopilot software Jim Keller, from 2016 to 2017, VP for Autopilot hardware David Nister, from 2015 to 2017, VP for Autopilot Stuart Bowers from 2018 -2019, VP for Autopilot Pete Bannon, from 2016 to now, Director for Autopilot hardware Andrej Karpathy, from 2017 to now, Director of AI Tesla in mediaTeslaRati Tesla official Telsa motor club Autopilot review zhihu: Tesla Autopilot history 2017 Mercedes-Benz E vs 2017 Tesla Model S Tesla’s Autopilot 8.0: why Elon Mush says perfect safety is still impossible Transcript: Elon Musk’s press conference about Tesla Autopilot under v8.0 update Tesla reveals all the details of its autopilot and its software v7.0 Software update 2018.39 Tesla V10: first look at release notes and features Tesla Autopilot’s stop sign, traffic light recognition and response is operating in shadow mode Tesla’s full self-driving suite with enhanced summon Tesla’s Robotaxi service will be an inevitable player in the AV taxi race Tesla Autopilot AP1 vs AP2 vs AP3 Tesla Hardware 3 Detailed Future Tesla Autopilot update coming soon Autopilot and full self driving capability features multi view Tesla FSD chips zhihu: EyeQ5 vs Xavier vs FSD]]></content>
      <tags>
        <tag>Tesla</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deploy lgsvl in docker swarm-2]]></title>
    <url>%2F2019%2F11%2F21%2Fdeploy-lgsvl-in-docker-swarm-2%2F</url>
    <content type="text"><![CDATA[backgroundpreviously tried to deploy lgsvl by docker compose v3, which at first sounds promising, but due to lack of runtime support, which doesn’t work any way. docker service create --generic-resource is another choice. docker service optionsdocker service support a few common options --workdir is the working directory inside the container --args is used to update the command the service runs --publish &lt;Published-Port&gt;:&lt;Service-Port&gt; --network --mount --mode --env –config docker service create with generic-resourcegeneric-resourcecreate services requesting generic resources is supported well: 1234$ docker service create --name cuda \ --generic-resource &quot;NVIDIA-GPU=2&quot; \ --generic-resource &quot;SSD=1&quot; \ nvidia/cuda tips: acutally the keyword NVIDIA-GPU is not the real tags. generic_resource is also supported in docker compose v3.5: 1234generic_resources: - discrete_resource_spec: kind: &apos;gpu&apos; value: 2 --generic-resource has the ability to access GPU in service, a few blog topics: GPU Orchestration Using Docker access gpus from swarm service first tryfollow accessing GPUs from swarm service. install nvidia-container-runtime and install docker-compose, and run the following script: 12345678910export GPU_ID=`nvidia-smi -a | grep UUID | awk &apos;&#123;print substr($4,0,12)&#125;&apos;`sudo mkdir -p /etc/systemd/system/docker.service.dcat EOF | sudo tee --append /etc/systemd/system/docker.service.d/override.conf[Service]ExecStart=ExecStart=/usr/bin/dockerd -H fdd:// --default-runtime=nvidia --node-generic-resource gpu=$&#123;GPU_ID&#125;EOFsudo sed -i &apos;swarm-resource = &quot;DOCKER_RESOURCE_GPU&quot;&apos; /etc/nvidia-container-runtime/config.tomlsudo systemctl daemon-reloadsudo systemctl start docker to understand supported dockerd options, can check here, then run the test as: docker service create --name vkcc --generic-resource &quot;gpu=0&quot; --constraint &apos;node.role==manager&apos; nvidia/cudagl:9.0-base-ubuntu16.04 docker service create --name vkcc --generic-resource &quot;gpu=0&quot; --env DISPLAY=unix:$DISPLAY --mount src=&quot;X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; --constraint &apos;node.role==manager&apos; vkcube which gives the errors: 1/1: no suitable node (1 node not available for new tasks; insufficient resourc… 1/1: no suitable node (insufficient resources on 2 nodes) if run as, where GPU-9b5113ed is the physical GPU ID in node: docker service create --name vkcc --generic-resource &quot;gpu=GPU-9b5113ed&quot; nvidia/cudagl:9.0-base-ubuntu16.04 which gives the error: invalid generic-resource request `gpu=GPU-9b5113ed`, Named Generic Resources is not supported for service create or update these errors are due to swarm cluster can’t recognized this GPU resource, which is configured in /etc/nvidia-container-runtime/config.toml second tryas mentioined in GPU orchestration using Docker, another change can be done: ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock --default-runtime=nvidia --node-generic-resource gpu=${GPU_ID} which fixes the no suitable node issue, but start container failed: OCI.. 1234root@ubuntu:~# docker service ps vkcc ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSorhcaxyujece vkcc.1 nvidia/cudagl:9.0-base-ubuntu16.04 ubuntu Ready Ready 3 seconds ago e001nd557ka6 \_ vkcc.1 nvidia/cudagl:9.0-base-ubuntu16.04 ubuntu Shutdown Failed 3 seconds ago &quot;starting container failed: OC…&quot; check daemon log with sudo journalctl -fu docker.service, which gives: 1Nov 21 13:07:12 ubuntu dockerd[1372]: time=&quot;2019-11-21T13:07:12.089005034+08:00&quot; level=error msg=&quot;fatal task error&quot; error=&quot;starting container failed: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v1.linux/moby/9eee7ac30a376ee8f59704f7687455bfb163e5ea3dd6d09d24fbd69ca2dfaa4e/log.json: no such file or directory): nvidia-container-runtime did not terminate sucessfully: unknown&quot; module=node/agent/taskmanager node.id=emzw1f9293rwdk97ki7gfqq1q service.id=qdma7vr1g519lz9hx2y1fen9o task.id=ex1l4wy61kvughns5uzo6qgxy third tryfollowing issue #141 123456nvidia-smi -a | grep UUID | awk &apos;&#123;print &quot;--node-generic-resource gpu=&quot;substr($4,0,12)&#125;&apos; | paste -d&apos; &apos; -ssudo systemctl edit docker[Service]ExecStart=ExecStart=/usr/bin/dockerd -H fd:// --default-runtime=nvidia &lt;resource output from the above&gt; and run: docker service create --name vkcc --generic-resource &quot;gpu=1&quot; --env DISPLAY --constraint &apos;node.role==manager&apos; nvidia/cudagl:9.0-base-ubuntu16.04 it works with output verify: Service converged. However, when test image with vucube or lgsvl it has errors: 1Nov 21 19:33:20 ubuntu dockerd[52334]: time=&quot;2019-11-21T19:33:20.467968047+08:00&quot; level=error msg=&quot;fatal task error&quot; error=&quot;task: non-zero exit (1)&quot; module=node/agent/taskmanager node.id=emzw1f9293rwdk97ki7gfqq1q service.id=spahe4h24fecq11ja3sp8t2cn task.id=uo7nk4a3ud201bo9ymmlpxzr3 to debug the non-zero exit (1) : docker service ls #get the dead service-ID docker [service] inspect r14a68p6v1gu # check docker ps -a # find the dead container-ID docker logs ff9a1b5ca0de # check the log of the failure container it gives: Cannot find a compatible Vulkan installable client driver (ICD) check the issue at gitlab/nvidia-images forth trydocker service create --name glx --generic-resource &quot;gpu=1&quot; --constraint &apos;node.role==manager&apos; --env DISPLAY --mount src=&quot;X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; --mount src=&quot;tmp&quot;,dst=&quot;/root/.Xauthority&quot; --network host 192.168.0.10:5000/glxgears BINGO !!!!! it does serve openGL/glxgears in service mode. However, there are a few issues: constraint to manager node require host network the X11-unix and Xauthority are from X11 configuration, which need more study. also network parameter need to expand to ingress overlay mostly, vulkan image still can’t run, with the same error: Cannot find a compatible Vulkan installable client driver (ICD) generic-resource support discussionmoby issue 33439: add support for swarmkit generic resources how to advertise Generic Resources(republish generic resources) how to request Generic Resources nvidia-docker issue 141: support for swarm mode in Docker 1.12 docker issue 5416: Add Generic Resources Generic resources Generic resources are a way to select the kind of nodes your task can land on. In a swarm cluster, nodes can advertise Generic resources as discrete values or as named values such as SSD=3 or GPU=UID1, GPU=UID2. The Generic resources on a service allows you to request for a number of these Generic resources advertised by swarm nodes and have your tasks land on nodes with enough available resources to statisfy your request. If you request Named Generic resource(s), the resources selected are exposed in your container through the use of environment variables. E.g: DOCKER_RESOURCE_GPU=UID1,UID2 You can only set the generic_resources resources’ reservations field. overstack: schedule a container with swarm using GPU memory as a constraint label swarm nodes $ docker node update --label-add &lt;key&gt;=&lt;value&gt; &lt;node-id&gt; compose issue #6691 docker-nvidia issue #141 SwarmKitswarmkit also support GenericResource, please check design doc 1234$ # Single resource$ swarmctl service create --name nginx --image nginx:latest --generic-resources &quot;banana=2&quot;$ # Multiple resources$ swarmctl service create --name nginx --image nginx:latest --generic-resources &quot;banana=2,apple=3&quot; ./bin/swarmctl service create --device /dev/nvidia-uvm --device /dev/nvidiactl --device /dev/nvidia0 --bind /var/lib/nvidia-docker/volumes/nvidia_driver/367.35:/usr/local/nvidia --image nvidia/digits:4.0 --name digits swarmkit add support devices option refermanage swarm service with config UpCloud: how to configure Docker swarm Docker compose v3 to swarm cluster deploy docker compose services to swarm docker deploy doc alexei-led github Docker ARG, ENV, .env – a complete guide]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>lg-sim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deploy lgsvl in docker swarm]]></title>
    <url>%2F2019%2F11%2F19%2Fdeploy-lgsvl-in-docker-swarm%2F</url>
    <content type="text"><![CDATA[Backgroundpreviously, vulkan in docker gives the way to run vulkan based apps in Docker; this post is about how to deploy a GPU-based app in docker swarm. Docker swarm has the ability to deploy apps(service) in scalability. Docker registryDocker Registry is acted as a local Docker Hub, so the nodes in the LAN can share images. update docker daemon with insecure-registries modify /etc/docker/daemon.json in worker node: &quot;insecure-registries&quot;: [&quot;192.168.0.10:5000&quot;] systemctl restart docker start registry service in manager node docker service create –name registry –publish published=5000,target=5000 registry:2 access docker registry on both manager node and worker node : $ curl http://192.168.0.10:5000/v2/ #on manager node $ curl http://192.168.0.10:5000/v2/ #on worker node insecure registry is only for test; for product, it has to with secure connection, check the official doc about deploy a registry server upload images to this local registry hubdocker tag stackdemo 192.168.0.10:5000/stackdemo docker push 192.168.0.10:5000/stackdemo:latest curl http://192.168.0.10:5000/v2/_catalog on worker run: docker pull 192.168.0.10:5000/stackdemo docker run -p 8000:8000 192.168.0.10:5000/stackdemo the purpose of local registry is to build a local docker image file server, to share in the cluster server. Deploy composedocker-compose builddocker-compose build is used to build the images. docker-compose up will run the image, if not exiting, will build the image first. for lgsvl app, the running has a few parameters, so directly run docker-compose up will report no protocol error. run vkcube in docker-composedocker-compose v2 does support runtime=nvidia, by appending the following to /etc/docker/daemon.json: 123456"runtimes": &#123; "nvidia": &#123; "path": "nvidia-container-runtime", "runtimeArgs": [] &#125; &#125; to run vkcube in compose by: xhost +si:localuser:root docker-compose up the docker-compose.yml is : 12345678910111213version: '2.3'services: vkcube-test: runtime: nvidia volumes: - /tmp/.X11-unix:/tmp/.X11-unix environment: - NVIDIA_VISIBLE_DEVICES=0 - DISPLAY# image: nvidia/cuda:9.0-base image: vkcube# build: . however, currently composev3 doesn’t support NVIDIA runtime, who is required to run stack deploy. support v3 compose with nvidia runtimeas discussed at #issue: support for NVIDIA GPUs under docker compose: 123456789services: my_app: deploy: resources: reservations: generic_resources: - discrete_resource_spec: kind: 'gpu' value: 2 update daemon.json with node-generic-resources, an official sample of compose resource can be reviewed. but so far, it only reports error: ERROR: The Compose file &apos;./docker-compose.yml&apos; is invalid because: services.nvidia-smi-test.deploy.resources.reservations value Additional properties are not allowed (&apos;generic_resources&apos; was unexpected` deploy compose_V3 to swarmdocker compose v3 has two run options, if triggered by docker-compose up, it is in standalone mode, will all services in the stack is host in current node; if triggered through docker stack deploy and current node is the manager of the swarm cluster, the services will be hosted in the swarm. btw, docker compose v2 only support standalone mode. take an example from the official doc: deploy a stack to swarm: 123456789docker service create --name registry --publish published=5000,target=5000 registry:2docker-compose up -ddocker-compose psdocker-compose down --volumesdocker-compose push #push to local registrydocker stack deploydocker stack services stackdemodocker stack rm stackdemodocker service rm registry after deploy stackdemo in swarm, check on both manager node and worker node: curl http://192.168.0.13:8000 curl http://192.168.0.10:8000 docker service runtimedocker run can support runtime env through -e in CLI or env-file, but actually docker service doesn’t have runtime env support. docker compose v3 give the possiblity to configure the runtime env and deploy the service to clusters, but so far v3 compose doesn’t support runtime=nvidia, so not helpful. I tried to run vkcube, lgsvl with docker service: docker service create --name vkcc --env NVIDIA_VISIBLE_DEVICES=0 --env DISPLAY=unix:$DISPLAY --mount src=&quot;/.X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; vkcube docker service create --name lgsvl -p 8080:8080 --env NVIDIA_VISIBLE_DEVICES=0 --env DISPLAY=unix$DISPLAY --mount src=&quot;X11-unix&quot;,dst=&quot;/tmp/.X11-unix&quot; lgsvl for vkcube, the service converged, but no GUI display; for lgsvl, the service failed. Docker deploydocker deploy is used to deploy a complete application stack to the swarm, which accepts the stack application in compose file, docker depoy is in experimental, which can be trigger in /etc/docker/daemon.json, check to enable experimental features a sample from jianshu docker-compose.yml: 1234567891011121314151617181920212223242526272829303132version: "3"services: nginx: image: nginx:alpine ports: - 80:80 deploy: mode: replicated replicas: 4 visualizer: image: dockersamples/visualizer ports: - "9001:8080" volumes: - "/var/run/docker.sock:/var/run/docker.sock" deploy: replicas: 1 placement: constraints: [node.role == manager] portainer: image: portainer/portainer ports: - "9000:9000" volumes: - "/var/run/docker.sock:/var/run/docker.sock" deploy: replicas: 1 placement: constraints: [node.role == manager] a few commands to look into swarm services: 12345docker stack deploy -c docker-compose.yml stack-demo docker stack services stack-demo docker service inspect --pretty stack-demo # inspect service in the swarmdocker service ps &lt;service-id&gt; # check which nodes are running the servicedocker ps #on the special node where the task is running, to see details about the container summaryat this moment, it’s not possible to use v3 compose.yml to support runtime=nvidia, so using v3 compose.yml to depoly a gpu-based service in swarm is blocked. the nature swarm way maybe the right solution. refer run as an insecure registry https configure for docker registry in LAN a docker proxy for your LAN alex: deploy compose(v3) to swarm monitor docker swarm docker swarm visulizer swarm mode with docker service inspect a service on the swarm voting example enable compose for nvidia-docker nvidia-docker-compose compose issue: to support nvidia under Docker compose potential solution for composev3 with runtime swarmkit: generic_resources Docker ARG, ENV, .env – a complete guide]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>lg-sim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vulkan in docker to support new lgsvl]]></title>
    <url>%2F2019%2F11%2F13%2Fvulkan-in-docker-to-support-new-lgsvl%2F</url>
    <content type="text"><![CDATA[backgroundDocker is a great idea to package apps, the first time to try play with docker swarm. lg-sim has updated to HDRP rendering, which has a higher quality, as well requires more GPU features, Vulkan. currently Vulkan is not supported by standard docker neither nvidia-dcker, which is deprecated after docker enginer &gt; 19.03. there is nvidia images, the special one we are interesed is vulkan docker, and there is an related personal project, which is based on the cudagl=10.1, which is not supported by non-Tesla GPU. so for our platform, which has only Quadra P2000 GPUs, the supported CUDA is 9.0, so we need to rebuild the vulkan docker based on CUDA9.0. check the vulkan dockerfile, instead of using cudagl:9.0, change to: FROM nvidia/cudagl:9.0-base-ubuntu16.04 after build the image, we can build the vulkan test samples. if no issue, load lg-sim into this vulkan-docker. a few lines may help: 12345/usr/lib/nvidia-384/libGLX_nvidia.s.0 /usr/share/vulkan/icd.d/proc/driver/nvidia/version new lgsvl in dockerthe previous lg-sim(2019.04) can be easily run in docker, as mentioned here. the above vulkan-docker image is the base to host lgsvl (2019.09). additionaly, adding vulkan_pso_cache.bin to the docker. the benefit of host lgsvl server in docker, is to access the webUI from host or remote. so the network should be configured to run as --net=host. if configure as a swarm overlay network, it should support swarm cluster. a few related issue can be checked at lgsvl issues hub. VOLUME in dockerfilethe following sample is from understand VOLUME instruction in Dockerfile create a Dockerfile as: 123FROM openjdkVOLUME vol1 /vol2CMD [&quot;/bin/bash&quot;] 12docker build -t vol_test . docker run --rm -it vol_test check in the container, vol1, vol2 does both exist in the running container. 123bash-4.2# ls bin dev home lib64 mnt proc run srv tmp var vol2 boot etc lib media opt root sbin sys usr vol1 also check in host terminal: 1234root@ubuntu:~# docker volume ls DRIVER VOLUME NAMElocal 0ffca0474fe0d2bf8911fba9cd6b5875e51abe172f6a4b3eb5fd8b784e59ee76local 7c03d43aaa018a8fb031ef8ed809d30f025478ef6a64aa87b87b224b83901445 and check further: 123root@ubuntu:/var/lib/docker/volumes# ls 0ffca0474fe0d2bf8911fba9cd6b5875e51abe172f6a4b3eb5fd8b784e59ee76 metadata.db7c03d43aaa018a8fb031ef8ed809d30f025478ef6a64aa87b87b224b83901445 once touch ass_file under container /vol1, we can find immediately in host machine at /var/lib/docker/volumes : 1234root@ubuntu:/var/lib/docker/volumes/0ffca0474fe0d2bf8911fba9cd6b5875e51abe172f6a4b3eb5fd8b784e59ee76/_data# ls -lt total 0-rw-r--r-- 1 root root 0 Nov 7 11:40 css_file-rw-r--r-- 1 root root 0 Nov 7 11:40 ass_file also if deleted file from host machine, it equally delete from the runnning container. The _data folder is also referred to as a mount point. Exit out from the container and list the volumes on the host. They are gone. We used the –rm flag when running the container and this option effectively wipes out not just the container on exit, but also the volumes. sync localhost folder to containerby default, Dockerfile can not map to a host path, when trying to bring files in from the host to the container during runtime. namely, The Dockerfile can only specify the destination of the volume. for example, we expect to sync a localshost folder e.g. attach_me to container, by cd /path/to/dockfile &amp;&amp; docker run -v /attache_me -it vol_test. a new data volume named attach_me is, just like the other /vol1, /vol2 located in the container, but this one is totally nothing to do with the localhost folder. while a trick can do the sync: 1docker run -it -v $(pwd)/attach_me:/attach_me vol_test Both sides of the : character expects an absolute path. Left side being an absolute path on the host machine, right side being an absolute path inside the container. volumes in composewhich is only works during compose build, and has nothing to do with docker container. copy folder from host to container COPY in dockerfile ERROR: Service ‘lg-run’ failed to build: COPY failed: stat /var/lib/docker/tmp/docker-builder322528355/home/wubantu/zj/simulator201909/build: no such file or directory the solution is to keep the folder in Dockerfile’s current pwd; if else, Docker engine will look from /var/lib/docker/tmp. VOLUME summaryIf you do not provide a volume in your run command, or compose file, the only option for docker is to create an anonymous volume. This is a local named volume with a long unique id for the name and no other indication for why it was created or what data it contains. If you override the volume, pointing to a named or host volume, your data will go there instead. when VOLUME in DOCKERFILE, it actually has nothing to do with current host path, it actually generate something in host machine, located at /var/lib/docker/volumes/, which is nonreadable and managed by Docker Engine. also don’t forget to use --rm, which will delete the attached volumes in host when the container exit. warning: VOLUME breaks things understand docker-compose.ymlUnderstand and manage Docker container volumes what is vulkan SDK Graham blog]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>lg-sim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[build cluster on Docker swarm]]></title>
    <url>%2F2019%2F11%2F06%2Fbuild-cluster-on-Docker-swarm%2F</url>
    <content type="text"><![CDATA[Docker micro serviceskey concepts in Docker when deploy an application(lg-sim) to swarm cluster as a service, which is defined in a/the manager node, the manager node will dispatch units of work as taskes to worker nodes. when create a service, you specify which container image to use and which commands to execute inside runing containers. In the replicated services, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state. For global services, the swarm runs one task for the service on every available node in the cluster. Docker swarm CLI commands1234567891011121314151617181920docker swarm init docker swarm joindocker service create --name --env --workdir --userdocker service inspect docker service ls docker service rmdocker service scale docker service psdocker service update --argsdocker node inspect docker node update --label-add docker node promote/demote # run in workerdocker swarm leave# run in managerdocker node rm worker-nodedocker ps #get running container IDdocker exec -it containerID /bin/bashdocker run -itdocker-compose up build/run delete unused Docker networkas Docker network may confuse external when access to the local network interfaces, sometimes need to remove the docker networks. 1234567docker network lsdocker network disconnect -f &#123;network&#125; &#123;endpoint-name&#125;docker network rm docker stop $(docker ps -a -q)docker rm $(docker ps -a -q)docker volume prune docker network prune the above scripts will delete the unused(non-active) docker network, then may still active docker related networks, which can be deleted through: 12345678910111213sudo ip link del docker0``` ### access Docker service Docker container has its own virutal IP(172.17.0.1) and port(2347), which allowed to access in the host machine; for externaly access, need to map the hostmachine IP to docker container, by `--publish-add`. the internal communication among docker nodes are configured by `advertise_addr` and `listen-addr`.#### through IP externalyTo publish a service’s ports externally, use the --publish &lt;PUBLISHED-PORT&gt;:&lt;SERVICE-PORT&gt; flag. When a user or process connects to a service, any worker node running a service task may respond.taking example from [5mins series](https://www.cnblogs.com/CloudMan6/tag/Swarm/) docker service create –name web_server –replicas=2 httpddocker service ps web_server access service only on host machine through the Docker IPcurl 172.17.0.1docker service update –publish-add 8080:80 web_server access service externallycurl http://hostmachineIP:80801234567#### configure websocket protocolfor lg-sim server to pythonAPI client, which is communicated through `websocket`, it&apos;s better if the service can be configured to publish through websocket.#### publish httpd server to swarm service docker service create –name web_server –publish 880:80 –replicas=2 httpd1the container IP is IP in network interface `docker0`(e.g. 172.17.0.1), which can be checked through `ifconfig`. `80` is the default port used by httpd, which is mapped to the host machine `880` port. so any of the following will check correctly: curl 172.17.0.1:880curl localhost:880curl 192.168.0.1:880curl 192.168.0.13:880 #the other docker node1234567891011121314151617181920 #### publish lg-sim into swarm service the previous version(2019.04) of lg-sim doesn&apos;t have a http server built-in, since 2019.7, they have `Nancy http server`, which is a great step toward dockerlize the lg-sim server.### manage data in Docker `Volumes` are stored in a part of the host filesystem, which is located `/var/lib/docker/volumes`, which is actually managed by Docker, rather than by host machine.Volumes are the preferred way to [persist data in Docker](https://docs.docker.com/v17.09/engine/admin/volumes/#more-details-about-mount-types) containers and services. some use cases of volume:* once created, even the container stops or removed, the volume still exist.* multiple containes can mount the same voume simultaneously; * when need to store data on a remote host * when need to backup, restore, or migrate data from one Dockr to another#### RexRayan even high-robost way is to separate volume manager and storge provider manager. [Rex-Ray](https://rexray.readthedocs.io/en/v0.3.3/) docker service create –name web_s \ –publish 8080:80 \ –mount “type=volume, volume-driver=rexray, source=web_data, target=/usr/local/apache2/htdocs” \ httpd docker exec -it containerIDls -ld /usr/local/apahce2/htdocschown www-data:www-data test visitcurl http://192.168.0.1:8080docker inspect containerID ``` source reprensents the name of data volume, if null, will create newtarget reprensents data volume will be mounted to /usr/local/apache2/htdocs in each container in RexRay, data volume update, scale, failover(when any node crashed, the data volume won’t lose) also be taken care. refer5mins in Docker Docker swarm in and out what is swarm advertise-addr can ran exec in swarm execute a command within docker swarm service]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux network tool]]></title>
    <url>%2F2019%2F11%2F05%2FLinux-network-tool%2F</url>
    <content type="text"><![CDATA[Linux network commandsipip command is used to edit and display the configuration of network interfaces, routing, and tunnels. On many Linux systems, it replaces the deprecated ifconfig command. 123456789ip link del docker0 # delete a virtual network interface ip addr add 192.168.0.1 dev eth1 #assign IP to a specific interface(eht1)ip addr show #check network interface ip addr del 192.168.0.1 dev eth1 ip link set eth1 up/downip route [show] ip route add 192.168.0.1 via 10.10.20.0 dev eth0 #add static route 192.168.0.1ip route del 192.168.0.1 #remove static routeip route add default via 192.168.0.1 # add default gw netstatenetstate used to display active sockets/ports for each protocol (tcp/ip) 12netstat -lat netstat -us nmclinmcli is a Gnome command tool to control NetworkManager and report network status: 12nmcli device status nmcli connection show = routeroute 1234route ==&gt; ip route (modern version) ##print router route add -net sample-net gw 192.168.0.1 route del -net link-local netmask 255.255.0.0 #delete a virtual network interface ip route flush # flashing routing table tracepathtracepath is used to traces path to a network host discovering MTU along this path. a modern version is traceroute. 1234tracepath 192.168.0.1 ``` ### networking service systemctl restart networking/etc/init.d/networking restart orservice NetworkManager stop123456789## network interface location at `/etc/network/interfaces` `eno1` is onboard Ethernet(wired) adapter. if machines has already `eth1` in its config file, for the second adapter, it will use `eno1` rather than using `eth2`.[ifconfig](https://www.ibm.com/support/knowledgecenter/ssw_aix_71/i_commands/ifconfig.html) is used to set up network interfaces such as Loopback, Ethernet network interface: a software interface to networking hardware, e.g. physical or virtual. physical interface, such as `eth0`, namely Ethernet network card. virtual interface such as `Loopback`, `bridges`, `VLANs` e.t.c. ifconfig -aifconfig eth0 #check specific network interfaceifconfig eth0 192.168.0.1 #assign static IP address to network interfaceifconfig eth0 netmask 255.255.0.0 #assign netmask ifconfig docker0 down/up 1234567891011121314151617181920212223242526replaced by `ip` command later.### why enp4s0f2 instead of eth0[change back to eth0](https://www.itzgeek.com/how-tos/mini-howtos/change-default-network-name-ens33-to-old-eth0-on-ubuntu-16-04.html)```shelllspci | grep -i &quot;net&quot;dmesg | grep -i eth0ip a sudo vi /etc/default/grub GRUB_CMDLINELINUX=&quot;net.ifnames=0 biosdevname=0&quot;update-grub# update /etc/network/interfaces auto eth0iface eth0 inet static sudo reboot Unknown interface enp4s0f2due TO /etc/network/interfaces has auto enp4s0f2 line, which always create this network interface , when restart the networking service. ping hostname with docker0usually there may be have multi network interfaces(eth0, docker0, bridge..) on a remote host, when ping this remote host with exisiting docker network (docker0), by default will talk to the docker0, which may not the desired one. build LAN cluster with office PCs setup PC IPs 1234567891011121314151617181920master node: IP Address: 192.168.0.1 netmask: 24Gateway: nullDNS serer: 10.3.101.101 worker node:IP address: 192.168.0.12netmask: 24Gateway: 192.168.0.1DNS: 10.255.18.3``` * `ufw disable` * update `/etc/hosts` file: 192.168.0.1 master 192.168.0.12 worker if need to update the default hostname to `worker`, can modify `/etc/hostname` file, and reboot. * ping test : ```script ping -c 3 master ping -c 3 worker set ssh access(optionally) sudo apt-get install openssh-server ssh-keygen -t rsa # use custmized key cp rsa_pub.key authorized_key referUbuntu add static route 10 useful IP commands]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里11]]></title>
    <url>%2F2019%2F10%2F29%2F%E6%9C%AA%E6%9D%A55%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C11%2F</url>
    <content type="text"><![CDATA[真实一直有隐藏的一面，而那些窥视过隐藏面的人，注定于大多数人不同，光鲜或者孤独。想想不甘心做韭菜又上升无望的漫漫岁月，简直注定了人生悲剧。 脆弱的感情自以为是个单纯的人，所以喜欢工程师氛围、高校科研氛围、美国的社区文化。回到国内，这些氛围都找不到了。扑面而来的气息，显得很黑暗森林。我怀疑一个单纯的人，在这样的环境下，如何能生存。所以，花尽心思去思考怎么顺应社会。话语中，充满了数字和术语，像一个机器在聊天，而不是一个有感情的人，女生都没办法接近我这样的人。 谈女生总会到一个话题，你稳定了吗。这个”未来5年“系列已经过了一年了，换环境、换公司、换小方向，到底有什么本质的变化，确定了一个方向，确定了一些问题吗？实际上都没有，甚至还没清楚自己要在哪里（国家、城市），做什么行业（汽车、咨询），都是一些自定义的限制。 小城市的行业回来的火车上，遇见了小城的公务员一家去武汉旅游。他们在当地的审计局工作，每年有一些培训在全国各地，比如，南京、武汉。然后一年还有5～10天年薪假。所以一年会有一两次国内的旅行，长一个月，短半个月的。他们的工资在5千 ～ 6千，小城市买了房子，买了10几万的德国/日本车；单位同事，家底好的可以开30-40万的abb。 体系内 学校、医院、法务机关、警察、税务、工商、建设、银行、电力、通信、交通、烟草、农林牧渔等等，庞大的网络可以解决三四线城市大量优质年轻人就业，因为是政府机关，在这些组织里面的年轻人，自然是党国的忠实跟随者。他们拿着高于当地平均值的工资，足够在小城市活的衣食无忧，甚至如上面的小夫妻，有车有房，有一年两次的度假。相比，大城市里租房、加班、年假等于零的奋斗青年，简直是苦逼。 体系外 三四线城市，（政府）体系以外的行业典型： 美容养生、学生教育、餐饮、休闲娱乐（电影、健身、文化项目）、汽修、服装、家具，是属于可以创业的行当。江浙以外，很少有三四线城市有自己的传统支柱产业，比如，农产品加工、能源石化、机械加工、小商品生产等等；更鲜有高附加值支柱产业，诸如，电子器件产业群、芯片产业群、汽车产业群、生物制药产业群、it产业群、精密制造产业群、金融服务业等等。 小城就一家国有钢铁厂，因为北方的能源、钢铁产能过剩，也气数不多。在这些小城市发展高附加值的产业，相当于空中楼阁。所以，体系外的年轻人生存状态其实很空心化。 空心化 “空心化”不是对服务业的贬低，但是相比发达欧美国家的小城市，它们会有一些其貌不扬，但历史百年的世界级的公司和产品。比如，德国狼堡，世界汽车产业中心；美国密西根奥本山小镇，世界级汽车应用软件商聚集地。生活在这些地方的年轻人，当然可以选择进入当地的服务业，诸如快餐连锁、超市服装、甚至房租装修、水电工等等。但是对于做研究、懂技术的年轻人，他们也能找到很容易进入一个创造绝对价值的公司。国内三四线小城市的“空心化”，就是除了传统的服务行业以及政府体系可以吸纳年轻人之外，缺乏生产绝对价值的支柱公司。 倒逼年轻人只能进入传统服务业或者政府机构，能够发挥年轻人创作力的机会太少了。服务业是景上添花，没有高附加值制造业的世界地位，服务业只会被局限在传统的衣食住行。这样一个显著的问题，就是影响了年轻人的价值体系。研究生毕业以后，收入水平、价值实现竟然不如隔壁小学毕业开早点的王老二。那国家投入这么多教育，是为了让这些年轻人去自卑吗。 结局 愚民之策，老百姓根本看不到问题，实际上在短期内，服务业是滞后价值创造产业的。开早点的王老二，还可以一个月赚5万，而且不需要为一个大学毕业生买单。长远点，低端/传统服务业，比如餐厅、修车店、服装店会慢慢饱和，甚至开展恶性竞争。每家新开的餐厅，头两个月可以吸引顾客，之后就被隔壁新开的餐厅带流量了。 一旦大部分传统服务行业进入这个阶段，除了政府直接控制的体系内，老百姓可能就受不了。要么政府强行维稳，全国陷入“中等收入陷阱”。要么社会就会开始乱。 中国有大量的三四线城市，可能都面对“空心化”的问题。大概率进入“中等收入陷阱”。看到问题而又无奈的人，会选择在这场演化之前逃离。国家层面的软实力，不是一天能建立起来的。这恐怕也是中国虽然有着表面的gdp数据，但实际上社会系统相比成熟的发达国家还差很远的地方。如果在演进之前，能慢慢发展出好的系统话。 自动驾驶失业潮有预测，到2030年，l3+自动驾驶的行业渗透可能只在10%。相比普遍的乐观预测，到2020年以后，l3+前装量产。国内近来发了一些很政策导向的产业牌照：全球第一张5G商用牌照， 全球第一张自动驾驶商用牌照。相比，苹果在2020年都不会上5G，美国撤销自动驾驶交通委员会。到底是国内领先，还是国家在画大饼？要保障这么多新生劳动力进入市场，国家也很南啊。 政策可以画行业大饼，但市场比政策更理性。waymo已经被资本市场降低估值，应该就是市场对这个行业的真实回应。按照资本的延后性，大概这个行业的裁员潮会尾随而来。 如果失业了，新职业选择在哪里？变化是永恒的。越是高级的打工仔，受裁员影响越高。在互联网级的快速迭代市场，专业的深度也许不如专业的可移植性对打工仔更合适。 进军服务业看《中国机长》又一次加深的这样的体会。飞机设计、航空发动机、自动驾驶飞控系统是远在普通人的关心之外的；相比，机长、空姐、机场运营人员、服务人员容纳着大量就业，也最直接产生价值。而且他们服务产生的价值，也是经济活力的主要贡献和决定者。相比，虽然国家没有自主的民航发动机、没有自主的cae软件，谁又觉得有什么问题呢。不过是一小群人为此担忧，大部分百姓甚至政策决策者只需要看到中国的航运数据又上升了，航运公司对各地gdp的贡献又大了，就可以士气高涨。 离客户越近，价值越大。在核心技术没有先发优势的市场，做技术只是backup的需求。相比，服务业更靠近广大终端客户人群，价值传递最短、损耗最小、获利最大。所以，空心化后的精英都会去高附加值服务业，比如，金融、互联网。 我们确实不该操政策精英的心，相比，做一个短视的普通人，识时务，进挣钱的行业，就是普通人该干的。 所以，还是要好好准备mba。]]></content>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里 12]]></title>
    <url>%2F2019%2F10%2F29%2F%E6%9C%AA%E6%9D%A55%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C-12%2F</url>
    <content type="text"><![CDATA[国内工作半年了，先后对开源的仿真软件做了定制化实现，对高精地图、ros、场景定义等有了大量快速的接触。同时也搭建了团队的gitlab, jenkins服务, rosbag管理服务、webviz服务，pg地图服务等等，并且也投入了一些时间去搭建数据基础框架，但并没有实际业务需求。在传统oem，用DevOps去规范开发流程，以及探索数据业务、云服务等相对较新颖。相对，新兴的造车企业，会比较好的融合DevOps流程，开发上云。 本质上是开发ADS支撑工具及数据基础设施，但由于OEM内部的ads开发团队比较弱，这些支撑工具以及数据工具并不发挥价值。作为工具开发者，存在感会变弱。因为离核心业务较远，虽然技术很通用。 OEM’s role in emerging techOEM itself mostly doesn’t build tools itself, even like Ford, who hired a great team to develop CAE tools in early days and build them tremendous, still retired most of them or sell them as packages to pure software vendors later. on the other side, the pioneers or leaders of the industry, usually build and apply new methodlogy before the market is there. and after the market is there, they can have the professional team to take in charge. the leaders take the role as incubators, as many Internet startups got the ideas from giants e.g. Google, Facebook e.t.c ADS at this moment is the emerging tech, there is no mature solution or tool chain for ADS, so the leaders of automobile makers and suppliers are the guys who should take the lead to set up standards and tools till ADS market goes to mature. and the giants in automobile should catch this great opportunity to take control in long term. the problems of Chinese OEMs (sadly there is no strong Chinese suppliers yet), they are customed to be followers, when US or German auto makers do the research and make the new bussiness stratagey, Chinese OEMs will follow. as the blog said, Chinese has no friend, neither Chinese goverment. be the followers or imitators is minimal loss strategy for most Chinese companies as well. and this is also why west coutry companies are afriad of the competition from Chinese. Chinese take copy others ideas as a strategy to success, rather than a shame. a tool developer机缘巧合，职业最初，就属于工具支撑组，开发和维护工具。随着，ads的爆发，more and more software skills are required by tradititional automobile makers and suppliers; and they prefer transfer themselves to modern digital driven. but a special tool is never the core product of auto makers. there comes a time, the OEM engineers asked, why you guys build tools, rather than buy from suppliers. The right thing for OEMs is to integrate components, rather than building tools. the cost of building tools by OEM itself is really too much after a while we realized. But at first, OEMs thought it was too costy to buy commecial tools, which gives survival space of a small team to grow inside OEM teams. actually at the early time, small team dreams big, especially in these days, many open source projects can be used freely. the small team almost support every aspect related to software tools, and as well to add features to simulation platform, which is also based on an open source project lg-sim, software manager looks like a good role in the team. but the reality is, to customize any tool, will cost a huge energy for a small team, as we are not involved in these open source projects. more, the manager team doesn’t consider an independent tool team. and even like this, we are moving slowly. infrasturcture for ABCfor ADS startups, they emphasize their infrasturcture features with : cloud platform remote monitor/control system (for vehicle team management) OTA data infrastructure(storage, management, analysis) and in single vehicle features with powerful computing ability, from GTC 2018 speak, most startup use the powerful Xdriver GPU； WeRide Momenta Tusimple AutoX Roadstar.AI (dead) plusAI ponyAI all these looks very Internet, but too far from massive vehicle product. the other mind is these teches are actually common for average engineers, but the chanllenge is to build a team, to realy make it work. survive in OEMit’s maybe more interested to take myself as an ABC(AI, big data, cloud computing) guy to find application scenarios in automobile industry product development. while ABC application scenarios are not matured yet of course, usually it is driven by leader teams, when it comes to drive by market, may be too late. beyond automotive, industry IoT maybe an even general domain to quickly apply ABCs. there should be three steps: familiar with the tech methods understand the hurting points of the industry product and management update -&gt; value return the life leap happens to a few lucky guys. most guys are common and grow in a linear speed. there is a burden the more you know, the harder your life. sadly … digitalizeOEMs are involved more and more in transfering digital, smart. previously most invovled in market, ads strategy, supply chain management; and as connected vehicles, self driving, future mobility service bursting in recent years, digitalization is sinking in vehicle product R&amp;D. AI, big data, cloud, blockchain e.t.c. new techs will be transfering the traditional industry. hopefully we can define the new bussniess together.]]></content>
  </entry>
  <entry>
    <title><![CDATA[lg-sim source code review (2019.09)]]></title>
    <url>%2F2019%2F10%2F22%2Flg-sim-source-code-review-2019-09%2F</url>
    <content type="text"><![CDATA[the recent version of lg-sim 2019.09 has additionally support for webUI, cloud, which is a big enhencement. the following code review is focusing on new components to support WebUI, cluster management. databasedatabase used sqlite project, which is an embedded in-memory db; the ORM used PetaPoco project; there are a few db related serices, e.g. vehicleService, simulationService, which will be used in RESTful web request. DatabaseManager: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152static IDatabaseBuildConfiguration DbConfig ;Init()&#123; DbConfig = GetConfig(connectionString) using db = new SqliteConnection(connectionString)&#123; db.Open()&#125;CreateDefaultDbAssets()&#123; using db = Open() if (info.DownloadEnvs != null)&#123; map = new MapModel()&#123;url = url LocalPath=localPath;&#125; id = db.Insert(map) &#125; if (info.DownloadVehicles != null)&#123;&#125; if(defaultMap.HasValue) &#123; sim1 = new SimulationModel()&#123; Cluster=0, Map=defaultMap.Value, ApiOnly=false&#125;; AddSimulation(db, sim1, noBridgeVehicle); AddSimulation(db, sim, apolloVehicle); db.Insert(simx) &#125;&#125;AddSimulation(IDatabase db, sim,vehicle)&#123; conn = new ConnectionModel()&#123; simulation = id, vehicle = vehicle.Value, &#125; db.Insert(conn)&#125;AddVehicle(db, sim,vehicle)&#123; conn = new VehicleModel()&#123; Url = url , &#125; db.Insert(vehicle) return vehicle.Id;&#125;PendingVehicleDownloads()&#123; using db=Open()&#123; var sql = Sql.Builder.From(&quot;vehicles&quot;).Where(&quot;status=00&quot;, &quot;Downloading&quot;); return db.Query&lt;VehicleModel&gt;(sql); &#125;&#125; database ORMsusing PetaPoco to define ORM models, such asusers, sessions, maps, vehicles, clusters, simulations PetaPocoa few operators in PetaPoco, such as Page, used to query a page; Single, used to query a single item; Insert; Delete; Update database provider123SQLiteDatabaseProvider =&gt; GetFactory(&quot;Mono.Data.Sqlite.SqliteFactory, Mono.Data.Sqlit&quot;) called inside DatabaseManager::Init(). database services VehicleService : IVehicleService MapService NotificationService =&gt; NotificationManager.SendNotification() DownloadService =&gt; DownloadManager.AddDownloadToQueue() ClusterService SessionService SimulationService Web modelweb model is where WebUI server located, which is served by Nancy project, in which a new design metholody is applied: Inversin of Control Contaner(IoC), namely 控制反转 in chinese. IoC is used to inject implementation of class, and manage lifecycle, dependencies. web model is the server where talk to webUI through routes, which is defined in the React project. Nancyused to build HTTP based web service, FileStream StreamResponse TinyIoC UnityBootstrapper : DefaultNancyBootstrapper, used to automatic discovery of modules, custm model binders, dependencies. /Assets/Scripts/Web/Nancy/NancyUnityUtils 12345678910111213141516171819202122232425ConfigureApplicatinContainer(TinyIoCContainer container)&#123; container.Register&lt;UserMapper&gt;(); container.Register&lt;IMapService&gt;(); container.Register&lt;IClusterService&gt;(); container.Register&lt;IVehicleService&gt;(); container.Register&lt;ISimulationService&gt;();&#125; ``` ### route modules ```c#SimulationModule : NancyModule()&#123; class SimulationRequest&#123;&#125; class SimulationResponse&#123;&#125; Get(&quot;/&quot;, x=&#123;&#125;) Post(&quot;/&quot;, x=&#123;&#125;) Put(&quot;/&#123;id:long&#125;&quot;, x=&#123;&#125;) Post(&quot;/&#123;id:long&#125;/start&quot;, x=&#123;&#125;) Post(&quot;/&#123;id:long&#125;/stop&quot;, x=&#123;&#125;) &#125; inside some of the route modules will do query or update the sqlite database for special objects. till here is the complete web back-end server, with a http server and an in-memory database. the webUI frontend is based on React, about React check a previous blog. in the React WebUI project will have http requests corresponding to each of routes defined here. React front end is very independent framework, which should be handled by an independent team in standard pipeline. Config &amp; Loader /Assets/Scripts/Web/Config.cs: used to define WebHost, WebPort, ApiHost, ClourUrl, Headless, Sensors, Bridges e.t.c Loader.cs : 12345678910111213141516171819202122232425262728293031323334353637383940Loader Instance ; //Loader object is never destroyed, even between scene reloadsLoader.Start()&#123; DatabaseManager.Init(); var config = new HostConfguration&#123;&#125; ; // ? Server = new NancyHost(Config.WebHost); Server.Start(); DownloadManager.Init();&#125;Loader.StartAsync(simulation)&#123; Instance.Actions.Enqueue() =&gt; var db = DatabaseManager.Open() AssetBundle mapBundle = null; simulation.Status &quot;Starting&quot; NotificationManager.SendNotification(); Instance.LoaderUI.SetLoaderUIState(); Instance.SimConfig = new SimlationConfig()&#123; Clusters, ApiOnly, Headless, Interative, TimeOfDay, UseTraffic... &#125; &#125;Loader.SetupScene(simulation)&#123; var db = DatabaseManager.Open() foreach var agentConfig in Instance.SimConfig.Agents: var bundlePath = agentConfig.AssetBundle var vehicleBundle = AssetBundle.LoadfromFile(bundlePath) var vehicleAssets = vehicleBundle.GetAllAssetNames(); agentConfig.Prefab = vehicleBundle.LoadAsset&lt;GO&gt;(vehicleAssets[0]) var sim = CreateSimulationManager(); Instance.CurrentSimulation = simulation &#125; DownloadManager class Download(); Init(){ client = new WebClient(); ManageDownloads(); } network modulenetwork module is used for communication among master and clients(workers) in the cluster network for cloud support. the P2P network is built on LiteNetLib which is a reliable UDP lib. LiteNetLibLiteNetLibis Udp package, used to P2P communication among master node and slave nodes here, where are defined as MasterManager, ClientManager. usage of LiteNetLib can be found in the wiki-usage)]]></content>
      <tags>
        <tag>network</tag>
        <tag>lg-sim</tag>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IoC in C#]]></title>
    <url>%2F2019%2F10%2F22%2FIoC-in-C%2F</url>
    <content type="text"><![CDATA[Inversion of Control(IoC) is a design mechanism to decouple components dependencies, a light-weight implementation is: TinyIoC, which is also part of Nancy. IoC idea uses commonly in webUI(and backend server) apps, which is an user friendly solution to cloud deployment management as well as apps in mobile, which should be the right direction in future ADS software tool development. the idea of IoC can be explained by the following example from register and resolve in unity container 12345678910111213141516171819202122232425262728293031323334353637383940public interface ICar&#123; int Run();&#125;public class BMW : ICar&#123; private int _miles = 0; public int Run() &#123; return ++_miles; &#125;&#125;public class Ford : ICar&#123; private int _miles = 0; public int Run() &#123; return ++_miles; &#125;&#125;public class Driver&#123; private ICar _car = null; public Driver(ICar car) &#123; _car = car; &#125; public void RunCar() &#123; Console.WriteLine(&quot;Running &#123;0&#125; - &#123;1&#125; mile &quot;, _car.GetType().Name, _car.Run()); &#125;&#125; the Driver class depends on ICar interface. so when instantiate the Driver class object, need to pass an instance of ICar, e.g. BMW, Ford as following: 123456789101112Driver driver = new Driver(new Ford());driver.RunCar()``` **to use IoC**, taking UnityContainer framework as example, a few other choices: TinyIoC e.t.c.```c# var container = new UnityContainer(); Register create an object of the BMW class and inject it through a constructor whenever you need to inject an ojbect of ICar. 12container.Register&lt;ICar, BMW&gt;(); Resolve Resolve will create an object of the Driver class by automatically creating and njecting a BMW object in it, since previously register BMW type with ICar. Driver drv = container.Resolve&lt;Driver&gt;(); drv.RunCar() summarythere are two obvious advantages with IoC. the instantiate of dependent class can be done in run time, rather during compile. e.g. ICar class doesn’t instantiate in Driver definition automatic new class management in back. the power of IoC will be scaled, once the main app is depends on many little services.]]></content>
      <tags>
        <tag>C#</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rosbag tools in ADS]]></title>
    <url>%2F2019%2F10%2F21%2Frosbag-tools-in-ADS%2F</url>
    <content type="text"><![CDATA[backgroundin ADS, data fusion, sensor performance, L3+ perception, localization algorithms development relys a lot on physicall data collection, commonly in rosbag format with information/data about gps, rtk, camera, Lidar, radar e.t.c. to build up the development process systemly is a critial thing, but also ignored by most ADS teams. for large OEMs, each section may have their own test vehicles, e.g. data fusion team, sensor team e.t.c, but few of them take care data systematically, or build a solution to manage data. one reason is the engineers are lack of ability to give software tool feedbacks/requirements, so they still stay and survive with folders or Excel management, which is definitely not acceptable and scalable for massive product team. thanks for ROS open source community conributing a great rosbag database manage tool: bag_database. with docker installation, this tool is really easy to configure. a few tips: web server IP port in Docker can be accessed from LAN by parameter -p during docker run. mount sbm driveas mentioned, most already collected rosbag is stored in a share drive, one way is to mount these data. 1234sudo mount -t cifs //share_ip_address/ROS_data /home/david/repo/rosbag_manager/data -o uid=david -o gid=david -o credentials=/home/david/repo/rosbag_manager/data/.pwd sudo umount -t cifs /home/david/repo/rosbag_manager/data Tomcat server configurebag_database is hosted by Tomcat, the default port is 8080. For our services, which already host pgAdmin4 for map group; gitlab for team work; xml server for system engineering; for webviz. check the port is occupied or not: 1netstat -an | grep 8088 so configure /usr/loca/tomcat/conf/server.xml: 12345 &lt;Service name="Catalina"&gt; &lt;Connector port="your_special_port" ...&lt;/Service&gt; a few other toolsros_hadoopros_hadoop is a rosbag analysis tool based on hdfs data management. which is a more scalable platform for massive ADS data requirements. if there is massive ros bag process in need, ros_hadoop should be a great tool. there is a discussion in ros wiki install hadoopapache hadoop download single alone install concept in hdfs namenode daemon process, used to manage file system datanode used to data block store and query secondary namenode used to backup hdfs shell command hdfs path URL hdfs-site.xml/usr/local/hadoop/etc/hadoop/hdfs-site.xml configure file dfs.datanode.data.dir -&gt; local file system where to store data blocks on DataNodes dfs.replicaiton -&gt; num of replicated datablocks for protecting data dfs.namenode.https-address -&gt; location for NameNode URL dfs.https.port -&gt; copy local data into hdfs hdfs dfs -put /your/local/file/or/folder [hdfs default data dir] hdfs dfs -ls mongodb_storemongodb_store is a tool to store and analysis ROS systems. also in ros wiki mongo_rosmongo_ros used to store ROS message n MongoDB, with C++ and Python clients. mongo-hadoopmongo-hadoop allows MongoDB to be used as an input source or output destination for Hadoop taskes. ros_pandasrosbag_pandas tabblestabbles used to tag any rosbags or folders. hdfs_fdwhdfs for postSQL at the endtalked with a friend from DiDi software team, most big Internet companies have their own software tool teams in house, which as I know so far, doesn’t exist in any traditional OEMs. is there a need for tool team in OEMs? the common sense is at the early stage, there is no need to develop and maintain in-house tools, the commericial ones should be more efficient; as the department grows bigger and requires more user special development and commericial tools doesn’t meet the needs any more, tool teams may come out. still most decided by the industry culture, OEM’s needs is often pre-defined by big suppliers, so before OEMs are clear their software tool requirements/need, the suppliers already have the solutions there – this situation is epecially true for Chinese OEMs, as their steps is behind Europen suppliers maybe 50 years.\ I am interested at the bussiness model of autovia.ai, which focus on distributed machine learning and sensor data analytics in cloud with petabyte of data, with the following skills: large scale sensor data(rosbag) processing with Apache Spark large scale sensro data(rosbag) training with TensorFlow parallel processing with fast serialization between nodes and clusters hdmap generation tool in cloud metrics and visulization in web loading data directly from hdfs, Amazon S3 all these functions will be a neccessary for a full-stack ADS team in future to development safety products, which I called “ data infrastructure for ADS”. referMooreMike: ros analysis in Jupter mount smb share drive to ubuntu autovia.ai]]></content>
      <tags>
        <tag>ros</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[safety guy in AV]]></title>
    <url>%2F2019%2F10%2F16%2Fsafety-guy-in-AV%2F</url>
    <content type="text"><![CDATA[backgroundwhy Waymo still can’t release fully driveless cars on road; why vehicle startups, NIO is going to brankruptcy? one of the hidden reasons vehicle as a consumer product can’t go to fast iteration as social media, mobility apps in Internet companies, is safety. vehicle development(VD) needs to satisfy safety requiements at first priority. so in VD, the cutting edge tech/ new solutions usually doesn’t make a difference, but the processes, how to handle safety requirements, system requirements, e.t.c are really the first thing in vehicle engineers’ mind. so now, deep learning AI in object detection is fairly matured, but it’s not used common in vehicle camera products. and for these start ups, who say themselves as tech-driven doesn’t survive well, because VD should be process driven. the best managment/survive way for original equipment manufacture(OEM) companies is build up their process system. e.g. GM managemnet is about process, and Toyota agile management is about a more flexible process, too. no OEMs said their team is tech-driven. what is SOTIF ?safety of the intended functionality(SOTIF): the absence of unreasonable risk due to hazards resulting from functional insufficiencies of the intended functionaliyt or by reasonably foreseeable misuse by persons. SOTIF is a way to define safey by design, while still there are unsafe post design, which is beyond the standards can handle. safety chanllenges in ADSis conserative behaivor of AI unsafe ? YES. as Waymo cars driving in San Fransisco, it’s more conservative than human drivers, which makes itself unsafe and surrounding vehicles unsafe. and the definiation of conservative or aggressive of driving behavior is further area depends, e.g. how human drivers drive in San Fransisco is different than drivers from Michigan, how AI handle this? how does AI satisfy VD requirements, and how to verify the AI really satisfy? there is no standard solution yet. since AI is black box, it may satisfy VD requirements in 99.9% situations, but failed in 0.1% case. an ADS solution works well in 99% scenarios is not an acceptable solution at all, which is really big chanllenge for most self driving full-stack solution start-ups. so Waymo and a few giants are working on test driven verification, which requires build up a very strong simulation team to power up safety test by simulation, which’s even chanllenging for most traditional OEMs. and definitely, there is no way to handle unstructured stochastic traffic environments, classification of safety hazards IS26262 internal cyber security (ISO21434) functional insufficient &amp; human misuse (IS02148) external infrastructura cyber security (traffic light system, jamming sensors signals) malfunction behaviors of EEHarzard Analysis &amp; Risk Assignment | | V ASIL B/C/D &lt;-- sys requires * func redundency * system safety mechanism * predictive safety mechanism this is a close-loop analysis, to satisfy sys requirement will get new situations, which lead to new system requirements system requirements ^ | | V SOTIF Analysis this is a double direction design, with any new requiremenst, there is a need to do SOTIF analysis, which may find out some potential bugs/problems in the design, then back to new system requirements. in summary, the input to SOTIF analysis is system requirements, and output is new system requirements. functional insufficients unsafe by design e.g. full pressure brake design, is not sufficient in scenarios, when there is rear following vehicle in full speed Tech limitations safe known unsafe known safe unknown unsafe unknown SOTIF is to identify all unsafe cases and requies to make them safe, but not know how, but SOTIF can’t predict unknown. back to why Waymo doesn’t release their driveless vehicle yet, cause they don’t have a way to prove their ADS system has zero unsafe unknown situations, and most possiblely the current ADS solution may be reached 80% functions of the final completed fully ADS solution, but that’s even not the turning point for ADS system. where to go? rule based vs AI based as neither purely rule based nor purely AI based can achieve safety goals, so there are combined ways, but then there need a manager module to decide which should be weighted more in a special situation. a new undeterministic/unpredicted system explaination reinforcement learning is actually a good way to train agents in unpredicted simulation environemnt, but even in simulation, it can’t travel every case in the state space; then to use R.L to train vehicles in real traffic environment with random pedestrains, npcs, situations, which’s impossible so far. a new AI system current AI is still correlation analysis, while human does causal reasoning. refersafety first for automated driving handover to PR ISO 26262 standard how to reach complete safety requirement refinement for autonomous vehicle]]></content>
  </entry>
  <entry>
    <title><![CDATA[cruise webriz configure]]></title>
    <url>%2F2019%2F10%2F16%2Fcruise-webriz-configure%2F</url>
    <content type="text"><![CDATA[backgroundduring ADS development, ros bag is used a lot to record the sensor, CAN and scenario info, which helps in vehicle test, data fusion, perception.cruise webviz is an open source tool to visualize rosbag and allow user defined layout in web environment, without the pain to run roscore and other ros nodes. while js is not a common tool in my visibility, the way how an js project is organized is really confused at the first time. there are a punch of new things: react [regl]https://www.giacomodebidda.com/how-to-get-started-with-regl-and-webpack/) and js functions/modules are really patches, they can be pached anywhere in the project, so usually it requires a patch manager(e.g. lerna) to deal with versions, dependencies etc; and bigger there are packages, which is independent function component. webviz has a few packages, e.g. regl-worldview, webviz-core, which has more than 40 patches(modules) used. cruise:worldview cruise:webviz core rosbag.js lernalerna is an open source tool to manage js project with multi packages. lerna init will generate lerna.json, which defines the required modules package.json defines useful infomation about the module depencies, CLI(which give a detail about how things work behind) and project description lerna bootstrap --hoist react will install all required modules in the root folder /node_modules, if doesn’t work well, leading to Can&#39;t Resovle module errors, may need manually install some. webpackwebpack is an open source tool for js module bundler. webpack.config.js, defines entry, where the compiler start; output , where the compiler end; module, how to deal with each module; plugin, additionaly content post compiliation; resovle.alias define alias for modules. 1234567891011121314var path=require("path")module.exports =&#123; entry: &#123; app:["./app/main.js"] &#125; output: &#123;path: path.resolve(__dirname, "build")&#125; &#125;npm install webpack-dev-servernpm list | head -n 1 webpack-dev-server --inline --hot the JS bundle could be loaded from a static HTML page, served with any simple web server. That’s exactly what this /packages/webviz-core/public/index.html file is, which is used for https://webviz.io/try. The webpack dev server is configured to serve it at /webpack.config.js npm commands123456npm config list npm install packages@version.minor [-g] [--save]npm uninstall packages npm search packagesnpm cache clean --force webpack-serverwebpack-server is a little nodejs Express server. to install it as a CLI tool first, then run it under the webviz project root folder, with optional parameters, e.g. –host, –port e.t.c webviz configure npm config set registry https://r.npm.taobao.org npm install puppeteer –unsafe-perm=true npm run bootstrap if failed: sudo npm cache clean –force npm install -g lerna npm run bootstrap sudo npm install/rebuild node-sass npm run build if failed, manually installed unresovled modules npm test if failed based on a few test modules, install then npm install webpack-dev-server –save webpack-dev-server.js –host IP –port 8085 #under project root a little hack, by default npm install inter-ui phli’s inter-ui), where insidewebviz/packages/webvix-core/src/styles/fonts.module.scess, it uses: url(&quot;~inter-ui/Inter UI (web)/Inter-UI.var.woff2&quot;) format(&quot;woff2-variations&quot;), modified this line to: url(&quot;~inter-ui/Inter (web)/Inter.var.woff2&quot;) format(&quot;woff2-variations&quot;) refertabbles webviz in ros community access web-server in LAN issue: how to run cruise webviz Ternaris Marv minio: high performance object storage for AI webviz on remote bag visuaize data from a live server npm package install in China taobao.npm]]></content>
      <tags>
        <tag>ros</tag>
        <tag>cruise</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[reinforcement learning in nutshell-2]]></title>
    <url>%2F2019%2F10%2F13%2Freinforcement-learning-in-nutshell-2%2F</url>
    <content type="text"><![CDATA[function approximationwrite the mapping from \ pair to value as $ S X A -&gt; Q $. usually S, Q can be continuous high-dimensional space, and even in discrete state problem. e.g. in GO game, the state space is about 10^170, to store each \ pair in a table is almost impossible. a good and approximated way is using a hash map, where input a state, and through this hash map to get its value, which is one step, rather than many steps even with a tree strucutre of the whole state-action/value space. in math, this is called function approximation, it’s very intuitive idea, to fit a function from the sample points; then any point in the space, it can be represented by the function’s paramters, rather than search in all the sample points, which may be huge; and further as only the function’s parameters is required to store, rather than the whole sample points information, which is really a pleasure. Deep Q-Learningpreviously in Q-Learning, to update Q(S,A): $ Q(S,A) &lt;- \delta ( Q(S, A) + R + \gamma ( Q(S&apos;, A&apos;) - Q(S, A)) to approximate q(s,a) can use a neural network(NN), the benefit of NN is to approximate any function to any precise, similar to any complete orthonormal sequence, e.g. Fourier Series. CNN approximatorthe input to Q neural network is the eigenvector of state S, assuming the action set is finite. the output is action value in state-action-hyperparameter: q(s, a, \theta), Q(S, A, \theta) ~= Q(S, A) as the right most section, here assuming the action set is finite, so for each action, there is an output. experience replayinstead of discarding experiences after one stochastic gradient descent, the agent remembers past expereicnes and learns from them repeatdely, as if the experience has happened again. this allows for greater data efficiency. another reason, as DNN is easily overfitting current episodes, once DNN is overfitted, it’s difficult to produce various experinces. so exprience replay can store experiences including state transitions, rewards, and actions, which are necessary to perform Q learning. at time t, the agent’s experience e_t is defined as: all of agent’s experiences at each time step over all episodes played by the agent are stored in the replay memory. actually, usually the replay memory/buffer set to some finite size limit, namely only store the recent N experiences. and then choose random samples from the replay buffer to train the network. the key reason here is to break the correction between consecutive samples. if the CNN learned from consecutive samples of experience as they occurred sequentially in the environment, the samples are highly correlated, taking random samples from replay buffer. target networkuse a separate network to estimate the target, this target network has the same architecture as the CNN approximator but with frozen parameters. every T steps(a hyperparameter) the parameters from the Q network are copied to this target network, which leads to more stable training because it keeps the target function fixed(for a while) go futher in DQNDouble DQNhere use two networks, the DQN network is for action selection and the target network is for target Q value generation. the problem comes: how to be sure the best action for the next state is the action with the highest Q-value ? Prioritized Replay DQNDueling DQNyet a summaryat the begining of this series, I was thinking to make a simple summary about DQN, which looks promising in self-driving. After almost one month, there are more and more topics and great blogs jumping out, the simple idea to make a summary is not easy any more. I will stop here, and once back. refermath equation editor Petsc Krylov zhihu: function approximate dqnbook: experience replay Toward Data Science explained replay memory going deeper into RL: understanding DQN improvements in DQN]]></content>
      <tags>
        <tag>reinforcement learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[configure pgAdmin4 in server mode]]></title>
    <url>%2F2019%2F10%2F13%2Fconfigure-pgAdmin4-in-server-mode%2F</url>
    <content type="text"><![CDATA[backgroundpgSQL is common used to store hd map in ADS stack, and the client is usually pgAdmin4. in big tech companies, e.g. Tencent, Alibaba, Uber e.t.c, their data infrastructure for ADS stack development is usually based on pgSQL. for our team work, we also store most map info in pgSQL, and for better development toolchain, there is a need to configure the client pgAdmin4 as a web server, rather than request every developer to install a copy of pgAdmin4 at their local side. in LAN web server, apache2 is used to serve a few services, so there is a need to configure multi virutal host in Aapche2, with differnt port and same IP. pgSQLServer official install postgresql start pgSQLserver jump into pgSQL server: 1234/etc/init.d/postgresql start sudo su - postgres pgAdmin4installed by apt-get install pgadmin4, will put pgAdmin4 under local user or root permission, which will be rejected if accessed by remote clients, whose permission is www-data so it’s better to install from src and in a different Python virtual env. setup Python virtual Env install pgAdmin4 from src there maybe a few errors as following: * No package &apos;libffi&apos; found * error: invalid command &apos;bdist_wheel&apos; [sol](https://stackoverflow.com/questions/34819221/why-is-python-setup-py-saying-invalid-command-bdist-wheel-on-travis-ci) * error: command &apos;x86_64-linux-gnu-gcc&apos; failed with exit status 1, [sol](https://stackoverflow.com/questions/21530577/fatal-error-python-h-no-such-file-or-directory) configure pgAdmin4follow this configure pgadmin4 in Server mode configure Apache2follow previous blog to set Apache2 for pgadmin4: &lt;VirtualHost *:8084&gt; ServerName 10.20.181.119:8084 ErrorLog &quot;/var/www/pgadmin4/logs/error.log&quot; WSGIDaemonProcess pgadmin python-home=/home/david/py_venv/pgenv WSGIScriptAlias /pgadmin4 /home/david/py_venv/pgenv/lib/python3.5/site-packages/pgadmin4/pgAdmin4.wsgi &lt;Directory &quot;/home/david/py_venv/pgenv/lib/python3.5/site-packages/pgadmin4/&quot;&gt; WSGIProcessGroup pgadmin WSGIApplicationGroup %{GLOBAL} Require all granted &lt;/Directory&gt; &lt;/VirtualHost&gt; restart Apache2 server will make this works. referwhy sudo - su godaddy about sudo - su configure pgadmin4 in Server mode]]></content>
      <tags>
        <tag>pgAdmin4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[host mod_wsgi in Apache2]]></title>
    <url>%2F2019%2F10%2F13%2Fhost-mod-wsgi-in-Apache2%2F</url>
    <content type="text"><![CDATA[Apache2 BackgroundApache2 virutal host site-available all virtual hosts are configured in individual files with /etc/apache2/sites-available site-enabled until the *.config in site-available are enabled, Apache2 won’t know know. sudo sevice apache2 reload IP based virtual host use IP address of the connection to determine the correct virtual host to serve, so each host needs a separate IP we had name-based vhost in LAN, a few web servers sharing the same IP in the same physical machine, but with different Ports, and even we don’t use DNS server to tell the domains. apache mode coresrefer VirtualHost the title to define this virutal host, and tell which port to listen. by default is port80 ServerName sets the request scheme, hostname, and port that the server uses to identify itself ServerAlias sets the alternate name for a host WSGIDaemonProcess for wsgi app, which usually define in a seperate Python virtual environment, rather than the default localhost user or root. so WSGIDaemonProcess point to the python virtual env. WSGIScriptAlias point to the wsgi_app.wsgi DocumentRoot set the directory from which httpd/apache2 will serve files /var/www/html mod_wsgi install apt-get install apache2 libapache2-mode-wsgi-py3 config in Apache2config with configure file create example.conf under /etc/apache2/conf-available/ 12345678&lt;VirtualHost *:8084&gt; ServerName 10.20.181.119:8084 ServerAlias example.com DocumentRoot &quot;/var/www/html&quot; ErrorLog &quot;/var/www/example/logs/error.log&quot; &lt;/VirtualHost&gt; the served web content is stored at /var/www/html, which can simple include a index.html or a few js. enable configure, which will create corresponding conf under conf-enable folder sudo a2enconf example check configure sudo apachectl -S sudo apachectl configtest config with virtual host create example.conf under /etc/apache2/site-available/12345678&lt;VirtualHost *:8085&gt; ServerName 10.20.181.119:8085 ServerAlias application.com DocumentRoot &quot;/var/www/wsgy_example&quot; ErrorLog &quot;/var/www/wsgy_example/logs/error.log&quot; WSGIScriptAlias /application /var/www/wsgy_app/application.wsgi&lt;/VirtualHost&gt; here used the additional WSGI script, link 123456789101112131415161718import osimport syssys.path.append(&apos;/var/www/wsgy_app/&apos;)os.environ[&apos;PYTHON_EGG_CACHE&apos;] = &apos;/var/www/wsgy_app/.python-egg&apos;def application(environ, start_response): status = &apos;200 OK&apos; output = b&apos;Helo World&apos; response_headers = [(&apos;Content-type&apos;, &apos;text/plain&apos;), (&apos;Content-length&apos;, str(len(output)))] start_response(status, response_headers) return [output] enable configure, which will create corresponding conf under site-enable folder sudo a2ensite wsgy_example check configure sudo apachectl -S sudo apachectl configtest add multi ports:in /etc/apache2/ports.conf: Listen 8083 Listen 8084 since this Apache server host system_engineering web and pgadmin4 web, and share the same IP. restart apachesudo systemctl restart apache2 test apachein browser: 10.20.181.119:8085/application should view “hello world” refername based virtual host Apache2 virutal host vhost with different ports]]></content>
      <tags>
        <tag>apache2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gitlab integrated Jenkins]]></title>
    <url>%2F2019%2F10%2F09%2FGitlab-integrated-Jenkins%2F</url>
    <content type="text"><![CDATA[backgroundafter built the Gitlab server in LAN for two month, managing about 20 projects in ADS group currently. the needs for CI is coming up. at very beginning, I tried Gitlab CI runner, doesn’t work through. so Jenkins! Jenkins installationthere is a great series Maxfields jekins-docker tutorial, since originally I had Docker env and which is a prepartion for cloud deployment in future. Jenkins in Docker installation, start Jenkins in Docker 1docker run --rm -p 8080:8080 -p 50000:50000 -v jenkins-data:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock jenkinsci/blueocean setup wizard also Jenkins installed directly in Ubuntu Jenkins integrated with GitlabJenkins default has integrated with BitBucket and Github, to integrate with Gitlab: set Gitlab token in Gitlab Project main page, go to User Profile &gt;&gt; Settings &gt;&gt; Peronsal Access Token, naming the token and click api domain, the token is generated. install Gitlab Plugins in Jenkins main page: Manage Jenkins &gt;&gt; Manage Plugins &gt;&gt; Search Available Plugins (gitlab). add gitlab key credentials in Jenkins main page, go to Credentials, choose the key type to Gitlab API token, then generate the key credential, used to access Gitlab project. configure Jenkins with gitlab in Jenkins main page, go to Manage Jenkins &gt;&gt; Configure System, at the gitlab section, add the gitlab host url and use the credential created at previous step. since the gitlab server is hosted in LAN, even don’t have DNS for the gitlab server, purely raw IP address. so the gitlab host URL is like: http://10.20.110.110:80 rather than the project git url (e.g. http://10.20.110.110/your_name/your_project.git) Gitlab Hook Plugingitlab events will be post to Jenkins through webhook, which is a common way to notify external serivces, e.g. dingding office chat, JIRA e.t.c. To set it up, need to configure Gitlab Hook plugin in Jenkins. as mentioned in this post, JDK10 or 11 is not supported for Jenkins, if the current OS system has already JDK11, need addtionally install jdk8, and configure the default jdk=8: 123456update-java-alternatives --list sudo update-alternatives --config java java -version Jenkins Projectadd a new Jenkins Item, select FreeStyle, go to Configure. in Source Code Management section, select Git. add Repository URL and set gitlab username and password as Jenkins Credentials. in Build Triggers section, select Build when a change is pushed to Gitlab, which display the Gitlab webhook URL: http://localhost:8080/jenkins/project/demo; go to Adavance section to generate the secret token. when getting all these done, back to Gitlab project, as Admin or Maintainer, in Project Settings &gt;&gt; Integrations &gt;&gt; Add Webhooks.URL and Secret Token are from the previous settings. there is a common issue: Url is blocked: Requests to localhost are not allowed, please refer to allow request to localhost network from system hooks referriot games: putting jenkins in docker jenkins to gitlab authentication devops expert: Emil mind the product: PM’s guide to CD &amp; DevOpsmanage multi version of JDK on Ubuntu a jianshu refer a aliyun refer tencent refer allow request to localhost network from system hooks]]></content>
  </entry>
  <entry>
    <title><![CDATA[warm up Hilber space]]></title>
    <url>%2F2019%2F09%2F29%2Fwarm-up-Hilber-space%2F</url>
    <content type="text"><![CDATA[abstract space Cauchy Series in metric space, existing an any positive and small \etta, exist a natural number N, $m, n &gt; N$, which satisfies: it’s called a Cauchy Series. intuitively, as the numbers goes above, the elements get closer. Complete Space any Caunchy Series in these abstract space, its convergence is still in the original space. e.g. rational number space is not complete space, as is not in rational number space anymore. intuitively, a complete space is like a shell without any holes on its surface. Linear Space with linear structural set, which can be described by base vector, so any hyper-point in a linear space can be represented by the linear combination of its base, so also called as vector space. in another way, linear space has only add and scalar multi operator. Metric Space to describe length or distance in linear space, adding normal in the linear space, gives normed linear space or metric space. Banach Space a complete metric space Inner Product Space with inner product feature in metric space. for the infinite space, there are two different sub space, either the inner product of the series converged or not. the convergence sub space is a completed space. Hilbert Space a completed inner product space Eurepean Space a finite Hilbert Space. functional analysisthe optimzed control theory problem, is to find the functional, under system dynamic constraints, which gives the extremum point(function) in the infinite normed linear space; in general. in general, the functional is depends on the system dynamic path, which gives some famuous theorem: Banach fixed point. PDE theory, partial geometry, optimized control theory are all part of functional analysis. from computational mechanics field, there is a chance go to PDE theory; from modern control field, there is a chance go to optimized control theory; from modern physics field, the students may also get familiar with partial geometry. the beauty of math is show up, all these big concepts finally arrive to one source: functional analysis. to transfer all these big things into numerial world, which gives CAE solver, DQN solvers e.t.c. the closest point property of Hilbert SpaceA subset of A of a vector space is convex: if for all a, b belongs to A, all \lamba such that 0 &lt; \lamba &lt; 1, the point belongs to A assuming A is non-empty closed convex set in Hilbert Space H, for any x belongs to H, there is a unique point y of A which is closer to x than any ohter point of A: orthogonal expansionsif (e_n) is an orthonormal sequence in a Hilbert space H, for any x belongs to H, (x, e_n) is the nth Fourier coefficient of x with respect to (e_n), the Fourier series of x with respect to the sequence (e_n) is the series: pointwise converges where f_nis a sequence of functions in complete orthonormal sequence given an orthonormal sequence of (e_n) and a vector x belongs to H then: but only when this sequence is complete, the right-hand side converge to left-hand side workks. an orthonormal sequence (e_n) in Hilbert Space H is complete if the only member of H which is orthogonal to every e_n is the zero vector. a complete orthonal sequence in H is also called an orthonormal basis of H. H is separable if it contains a complete orthonormal sequence, the orthogonal complement (which named as E^T) of a subset E of H is the set: for any set E in H, E^T is a closed linear subspace of H. the knowledge above gives the convergence in Hilbert Space, application like FEA, once created the orthonormal basis of solution space, the solution function represented by the linear combination of these basis is guranted to convergence. Fourier seriesit is a complete orthonormal sequence in L^2(-\pi, \pi), and any function represented by a linear combination of Fourier basis is converged. but here Fourier basis is the arithmetic means of the nth partial sum of the Fourier series of f, not the direct basis itself. Dual spaceFEA solutions for PDEPDE can be represented in a differential representation: or an integral representation: integral formulation has included existing boundary conditions. by multiplying test function \phi on both side of integral representation, and using Green’s first identity will give the weak formulation(variational formulation) of this PDE. in FEA, assuming the test function \phi and solution T belong to same Hilbert space, the advantage of Hilbert space, is functions inside can do linear combination, like vectors in a vector space. FEA also provides the error estimates, or bounds for the error. the weak formulation should be obtained by all test functions in Hilbert space. this is weak formulation due to now it doesn’t exactly requries all points in the domain need meet the differential representation of the PDE, but in a integral sense. so even a discontinuity of first derivative of solution T still works by weak formulation. Hilbert space and weak convergenceconvergence in another words means, the existence of solution. for CAE, DQN algorithms, the solution space is in Hilbert space. Lagrange and its dualityto transfer a general convex optimization problem with lagrange mulitplier: =&gt; we always looks for meaning solution, so L should exist maximum extremum. if not, as x grow, the system goes divergence, no meaningful solution. define: so the constrainted system equation equals to : to rewrite the original equation as: then the system’s duality is defined as : for a system, if the original state equation and its duality equation both exist extremum solution, then : the benefits of duality problems is the duality problem is always convex, even when the original problem is non-convex the duality solution give an lower boundary for the original solution when strong duality satisfy, the duality solution is the original solution Krylov Spacereferconvex func finite element method Lagranget duality standford convex optimization Going deeper into RF: understaing Q-learning and linear function approximation Nicholas Young, an introduction to Hilbert Space why are Hilbert Space important to finite element analysis]]></content>
  </entry>
  <entry>
    <title><![CDATA[reinforcement learning in nutshell-1]]></title>
    <url>%2F2019%2F09%2F18%2Freinforcement-learning-in-nutshell-1%2F</url>
    <content type="text"><![CDATA[RL conceptsassuming timestep t: the environment state S(t) agent’s action A(t) discouted future reward R(t), which satisfy: with current state `s` and taking current action `a`, the env will give the reward `r_{t+1}`. `\gamma` is the discount ratio, which usually in [0,1], when $\gamma = 0 $, agent&apos;s value only consider current reward. check [discount future reward]() in following chapter. agent’s policy P(a|s), in current state s, the propability of action a agent’s state value V(s), in state s and took policy \pi, which usually described as an expectation: agent’s action value Q(s, a), which consider both state s and action a effects on value calculation. namely, agent’s value is the expectation of its action value with probability distribution p(a|s). state transfer propability P explore rate \eta, it’s the chance to choose non-max value during iteration, similar as mutation. Markov decision processassuming state transfer propability, agent&#39;s policy, and agent&#39;s value follow Markov assumption. namely, the current state transfer propability, agent’s policy and value only relates to current state. agent’s value function V(s) meets Belman’s equation: and agent’s action value function q(s) also has Belman’s equation: in general, env’s state is random; the policy to take action is policy P(a|s). Markov Decision process is: state, action, policy, each [state, action, policy] is an episode, which gives the state-action-reward series: s0, a0, r1, s1, a1, r2, ... s(n-1), a(n-1), rn, sn Markov assumption is state(n+1) is only depends on state(n). Belman’s equationBelman’s equation gives the recurrence relation in two timesteps. the current state value is calcualted from next future status with given current env status. discounted future rewardto achieve better reward in long term, always conside the reward as sum of current and future rewards: R(t) = r(t) + r(t+1) + ... r(n) on the other side, as the env state is random in time, the same action doesn’t give the same reward usually, and as time goes, the difference is even larger. think about the B-tree, as it goes deeper, the number of nodes is larger. so the reward in future doesn’t count the same weight as earlier reward, here define the discount ratio \gamma &lt;- [0, 1] R(t) = r(t) + \gamma r(t+1) + ... + \gamma^(n-t) r(n) R(t) = r(t) = \gamma R(t+1) policy iterationthere are two steps: policy evaluation, with current policy \pi to evaluate state’s value V&#39; policy improvment, with the state value V&#39;, with a special policy update strategy(e.g. greedy) to update policy. the ideal result is to find the fixed point in value space, which corresponds to the optimized policy at current state with current policy update strategy. in policy iteration, we only consider policy-value mapping. value iterationwhere policy iteration based valued is implicit, only value iteration explicit by Belman’s equation. this is also a fixed point application, as we can use the explicit mapping (Belman’s equation) in state value space. so there should guarantee existing an optimized policy. an optimization problemreiforcement learning solution is to find the optimal value function to achieve the max reward in each timestep, which leads to optimal policy \pi*, or max value func, or max action value func. $$ v(s) = max(v_i(s)) foreach i in value funcs $$ $$ q(s, a) = max(q_i(s,a)) foreach i in action value funcs $$ mostly the solution space is not known at hand, if else, the optimal solution can get directly. on the other hand, current state, action set tells a little information about the solution space, as they are part of the solution space, so the optimization algorithms used in convex space can be applied here. a few common optimization algorithms includes: dynamic programming(DP)DP can used in both policy evaluation and policy iteration, but in each calculation step(not even one physical step, and in one physical step, there can be hundreds or thousands of calculation steps) iteration, DP has to recalculate all state(t=current) value to the very first state(t=0) value, which is disaster for high-dimensional problem, and DP by default is kind of integer programming, not fitted to continuous domain either. Monte Carlo (MC)MC gets a few sample solutions in the solution space, and use these samples solution to approxiamte the solution space. in a geometry explaination, the base vectors of the solution space is unknown, but MC finds a few points in this space, then approximate the (nearly) base vectors, then any point in this space can be approximated by the linear combination of the nearly base vectors. MC has no ideas of the propability of state transfer, MC doesn’t care the inner relations of state variables, or model-free. MC is robost as it’s model free, on the other hand, if the solution space is high-dimensional, there is a high chance the sampling points are limited in a lower dimension, which weak the presentation of MC approximation; also MC needs to reach the final status, which may be not avaiable in some applications. time-serial Time Difference(TD)MC use the whole list of status for each policy evaluation; in 1st order TD, policy evaluation only use next reward and next state value: $ v(s) = R(t+1) + \gamma S(t+1)|S $ for n-th order TD, the update will use n-th reward : $ v(s) = R(t+1) + \gamma R(t+2) + ... + \gamma^(n-1) R(t+n) + \gamma^n S(t+n)|S $ it’s clear here, as n-&gt;infinitly, n-th order TD is closer to MC. so how close is enough usually, namely which n is enough ? SARSAin TD, there are two ways: on-line policy, where the same one policy is using to both update value func and upate action; while off-line policy, where one policy is used to update value func, the other policy is used to update action. SARSA is kind of on-line policy, and the policy is e-greedy, to choose the max value corresponding action in every iteration with a high probablitiy (1-e), as e is very small. $ \delta(t) v(S, A) = R + \gamma (v(S&apos;, A&apos;) - v(S, A)) $ in the iteration equation above, \delta(t) will give the iteration timestep size, S&#39;, A&#39; are next state and next action, compare to pure 1-st order TD, where the next state value is modified as: $(Q(S’, A’) - Q(S, A)$, in this way value func keep updated in every func, which can be considered as local correction, compared to the unchanged pure 1-st order TD, which may be highly unstable/diverge in long time series. as time-serial TD can’t guarantee converge, so this local correction makes SARSA numerical robost. SARSA(\lambda) in mutli-step based is same as n-th order TD. $ v(s) = R(t+1) + \gamma R(t+2) + ... + \gamma^(n-1) R(t+n) + \gamma^n ( v(S`) - v(S) ) $ the problem of SARSA is v(S, A) may go huge, as the algorithm is on-line policy, which requires huge memory. Q-learningQ-learning is off-line policy TD. the policy iteration is use e-greedy policy, same as SARSA; while the policy evaluation to update value func use greedy policy. in a word: from status S, using e-greedy policy to choose action A, and get reward R, and in status S&#39;, and using greed policy to get action A&#39;. $ \delta(t) Q(S, A) = R + \gamma (Q(S&apos;, A&apos;) - Q(S, A)) $ (a) where $ A= max v(a|S) $. while in SARSA, both S&#39; and A&#39; update using e-greed. usually Q(S,A) is called Q-valued. the benefit of choosing a different policy to update action, is kind of decouping status and action, so in this way, they can reach more area in real state-action space, which also lead the solution space a little more robost. but still equation (a) is not guaranted to converge as time goes. the converged Q(S,A) should be convex, which means its second-order derivative must be less than 0, then the max Q values (max extremum) achieves when first-order derivate is 0. while Q-learning has the same problem as SARSA, the huge memory to store Q(S,A) table. to make intuitive example, think about an robot walk with 2 choice(e.g. turn left, turn right), and the grid world has 20 box in line, which has 2^20 ~= 10e6 elements in Q(S, A). it can’t scale to any real problem. referSutton &amp; Barto, Reinforcement learning: an introduction Pinard fromeast mathxml editor equation html editor fixed point mechanism]]></content>
      <tags>
        <tag>reinforcement learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python modules in help]]></title>
    <url>%2F2019%2F09%2F18%2Fpython-modules-in-help%2F</url>
    <content type="text"><![CDATA[unittestwhy unit test ?unit test is to test the components of a program automatically. a few good reasons of unit tests: test driving practice early sanity checking for regression test, that the new changes won’t break early work unit test is to test the isolated piece of code, usually white box test written by developer; functional test is to test the function requirements, usually black box test written by testers. the testcase output can be OK, FAIL, ERROR. assertion functions basic boolean asserts: assertEqual(arg1, arg2, msg=None) assertIsNot(arg1, arg2, msg=None) comparative asserts: assertAlmostEqual(first, second, places=7, msg=None, delta=None) asserts for collections: assertListEqual(list1, list2, msg=None) assertTupleEqual(tuple1, tuple2, msg=None) assertSetEqual(set1, set2, msg=None) assertDictEqual(dic1, dic2, msg=None) command line interface run all unittests python3 -m unittest discover -v -c run single test module python3 -m unittest -v -c tests/test_XXXX.py run individual test case python3 -m unittest -v tests.test_XXX.TestCaseXXX.test_XXX how to write a unittestunittest.TestCaseany user defined test class should first derived from unittest.TestCase setUp() &amp; tearDown()setUp() provides the way to set up things before running any method starting with test_xx() in user defined test class. tearDown() is where to end the setting ups depends on other modules/classusually the class/methods we try to test have depends on other modules/class, but as unit test want to isolate these depends, so either mock the other methods or fake data. in general there are three types of depends: pure variablethis is the most simple case, then you can directly define the same pure type variable in your test_xx() method methods in other moduleshere need to use patch, to mock the method and define its return value is your test_xx() method, then in the class where call the real method will be replace by this mocked one. methods in other classhere need to use mock, either MagicMock or Mock will works. mock &amp; patchmock module is built in unittest.mock after a later Python version, if not, mock module can be installed by pip install mock, then directly import mock in code. the philosophy of unit test is to isolate classes test, but in reality most classes have some instances of other classes, so to test the ego class isolated from other classes, that’s where mock helps. 123456789101112131415from mock import MagicMock, patchclass TestScenario(TestCase): def test_mock(self): sim.method = MagicMock(return_value="xx") agent = sim.method() self.assertEqual(agent, "xx") @patch("module/add_agent") def test_patch(self, mock_add_agent): mock_add_agent.return_value = "xx" agent = module.add_agent() self.assertEqual(agent, "xx") Mock helps to mock the method in a class, while patch helps to mock a global method in a module. automatical unittest generatorthere are projects assit to generate unittest code automatically, e.g auger test with while LoopTODO test with raise ErrorTODO coverage.py (one time only) install coverage.py pip3 install –user coverage run all tests with coverage ~/.local/bin/coverage run -m unittest discover generate html report ~/.local/bin/coverage html –omit “~/.local/“,”tests/“ ps, output is in htmlcov/index.html logginglogging() supports more detail message type(info, warn, error, debug) than print(), and the message format is more strict with \%. while print() works both formatted message and message as string simply: 12print("1+1 = ", num)print("1+1 = %d", % num) the following is a general log class, which can plug in any existing Python project: import logging import os class log(object): def __init__(self, logger_name): self.log = logging.getLogger(logger_name) self.log.setLevel(logging.DEBUG) console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) self.log.addHandler(console_handler) def set_output_file(self, filename): file_handler = logging.FileHandler(filename) file_handler.setLevel(logging.INFO) formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s') file_handler.setFormatter(formatter) self.log.addHandler(file_handler) def main(): test = log('scenario-mm') file_name = "record.log" dir_name = "/home/python_test/" try: os.makedirs(dir_name) except OSError: pass file_path = os.path.join(dir_name, file_name) test.set_output_file(file_path) test.log.debug("debug in episode 1...") test.log.info("info ...") test.log.warning("warn ...") test2 = log("zjjj") test2.set_output_file('record.log') test2.log.info("test2 info") test2.log.warning("test2 warn") if __name__ == "__main__": main() pygamereferpython automatic test series auger: automated Unittest generation for Python unittest official unittest in lg-sim project coverage.py official unittest + mock + tox]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[math in ADS system]]></title>
    <url>%2F2019%2F09%2F14%2Fmath-in-ADS-system%2F</url>
    <content type="text"><![CDATA[when dealing with plan and control models in ADS system, there are plenty of basic math, and the understanding of these math are definitely helpful to build a better and stronger algorithm. to design a ADS software system, there are two parts, the engineering background and software development. enginneering side is the core logic, while software is about how to organize the software and present, especially with new ideas, e.g. cloud based architecture, web user interface. vector analysiscoordinate systemasssumed a Cartesian system [O: e1, e2, e3], a vector r can be represented as (x, y, z). vector operator vector add a + b = (a_x + b_x) i + (a_y + b_y) j + (a_z + b_z) k the triangle inequality is: |a + b| &lt;= |a| + |b| multiply scalar a . b = a . b_x i + a . b_y j + a . b_z k namely, by multiplying each direction component of the vector by a. dot product a . b = |a|.|b|. \cos = a_x . b_x + a_y . b_y + a_z . b_z a simple physical explaination: assume a is displacement, b is force, then the power of force in the displacement is by dot product. the geometric meaning of dot product is projection. namely, |a|cos means project of vector a on vector b, which also gives the decomposion of a, which has a unique projection component vector and vertical component vector. a = a_p + a_v a_p = ( a . e_p ) . e_p cross product assume three vectors non-co-plane, (a, b, c), rotating start from a to b, the thumb point to c, which is a right-hand-coordinate system. a simple physical meaning: assume a is the force, b is the force arm, then a x b gives the torque, which perpendicular to plane. a x b = |a||b| \sin the geometric meaning of cross product is the area of parallelogram by . base vectors in right-hand Cartesian coordiante system satisfy: ei x ej = ek (i != j) mixed product $$ a x b \cdot c $$ which gives the volume of parallel hexagonal propped by curve(volume) integration Gauss integration Stokes integration scalar field directional derivative assuming vector l in space, its unitfied vector can be present as directional cosine [cos\alpha, cos\beta, cos\gamma]^T. for any function u=u(x, y, z), who is derivable at M0(x0, y0, z0), then: \frac{\partial u}{\partial l} = dot_product(\Delta u, &lt;\cos \alpha, \cos \beta, \cos \gamme&gt; ) gradient \Delta u = \frac{\partial u}{\parial x} \b{i} + \frac{\partial u}{\parial y} \b{j} + \frac{\partial u}{\parial z} \b{k} the directional derivative which gives the maximum of \Delta u at a point, it’s the gradient direction. the gradient direction in space, stands for the direction from lowest co-value layer to highest co-value layer, which in physical, means the most high rate of change in general. Halmilton operator \Delta = \frac{\partial}{\parial x} \b{i} + \frac{\partial}{\parial y} \b{j} + \frac{\partial}{\parial z} \b{k} vector field directional derivative the gradient field of a vector field gives a tensor field, which raise the dimension one more. the dot product(inner product) of a vector field, decrease to scalar field, the cross product of a vector field keeps a vector field. for a vector field, usually talk about its flux and divergence. analytical geometryplane equationassume plane \pi in Cartesian coordinate system [O: X, Y, Z], O‘s projection in plane is N, the directional cosine of ON is (l, m, n), for any point P in plane, NP is always perpendicular to the directional cosine, namely: dot_product( NP, (l, m, n) ) = 0 as l**2 + m**2 + n**2 == 1, which gives: lx + my + nz - p = 0 (1) equation 1) is the normalized plane equation. and (l, m, n) is the normal vector of lane \pi, and p should be no lesss than 0. linear equationassumed a line go across point P_0 $(x_0, y_0, z_0)$ and in direction \lamba, then any point P $(x,y,z)$ on this line, satisfy: x - x_0 = |PP_0| . l y - y_0 = |PP_0| . m z - z_0 = |PP_0| . n taking |PP_0| as t, the equation above is the parmeterized line equation, namely: \frac{x-x0}{l} = \frac{y-y0}{m} = \frac{z-z0}{n} in general, a linear is the cross interface of two linear plane, so its genearl equation is to satisfy both plane equation: A1.x + B1.y + C1.z + P1 = 0 A2.x + B2.y + C2.z + P2 = 0 coordiante transferin general, coordiante transfer means base vector linear transfer. assuming original coordinate system [O: e1, e2, e3], and new coordinate system [O’, e1’, e2’, e3’] both the original base vector and new base vector &lt;e1’, e2’, e3’&gt; prop up the same 3D linear space. and there should be transfer matrix between them: e = matrix[3x3] \cdot e’ for any point P transfer from original to new coordinate system, {x} = {a} + {x’} . matrix[3x3] matrix[3x3] is: a11 a21 a31 a12 a22 a32 a13 a23 a33 each raw in the matix above, stands for a base vector tranfer from original one to the new one, namely: $$ e1&apos; = a11.e1 + a21.e2 + a31.e3 $$ the component value of each coordinate, can be simply by: x1’ = e1’ . OP’ referwrite mathematic fomulars in Markdown]]></content>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里 10]]></title>
    <url>%2F2019%2F09%2F14%2F%E6%9C%AA%E6%9D%A55%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C-10%2F</url>
    <content type="text"><![CDATA[个人与公司的生存和发展法则。因为企业生存状态以及老板的眼光，国内企业会有一种氛围：老板花钱是让来工作的，来学习都不该提倡。个人而言，学习才能保持议价权。也需要有个良好的心态，面对这样的企业现状。 国内就业大环境也长期不看好。本来优质的大公司和岗位不多，相对缺乏壁垒的，最近的一个新闻： 一汽大众不招传统专业，比如，车辆、机械。不过话说回来，一汽大众的工作在长春也是绝对高薪。不招人背后，是技术岗位都在德国，那国内的年轻人在这样的企业怎么积累？ 当然，即使在这样的企业内，还会分三六九等。年轻人被局限的穷迫程度可想而知，这样缺乏格局的环境，也没办法培养年轻人的领导力。 上一篇 提到北欧国家。国家大环境，确实对年轻人的选择有太大影响。国内年轻人如果能选择行业，确实应该避开传统制造业、农业等低增长周期的企业。当然，互联网、金融、地产、消费服务等快增长周期的行业，算适应国家的大环境，人也不轻松。 国与家选择一个国家，就是选择了一套制度。在中秋夜，每有悠闲的时候，心里反而更沉，似乎只有加班工作，可以抵消内心深处的不安。 回来看了很多关于中国发展的报道《中国制造2025》以及生活现状：住在筒子楼里的爱情，缺乏社会秩序和行业规则的生活和工作环境，慢慢溢出。 回国前，对国内的生活的1万种可能，都不包括现实的样子。 小镇青年的生活，完全是另一个折叠。我以为的真实生活到底只是我愿意认知的真实吧了，包括对国与家的态度。有时候觉得，是不是自己跟周围的人太格格不入了，可惜看到《计算机应用》学报上的文章，真的很无奈，大部分年轻人在这样的状态下追求的是什么价值？ 商业的逻辑小城唯一的一家沃尔玛，进去看到商品的丰富，真的是感动了一下。想当年在美国，出入walmart, whole food, wegments, kroger, 虽然也感谢生活的便利舒适和优雅，不过没有这么渴望。 在国内生活，思路也发生了转变。进沃尔玛看到陈列的各式衣服、首饰，第一反应，竟然是如何成为沃尔玛的供货商。这个平台大到足够小富即安，年入百万。这样的思路，放在美国生活的时候，估计有点丧心病狂。 也可能国内的环境慢慢教人认识到：商人和企业家是不同的吧。 记得之前评价恒大造车，说他是对行业的玷污，那不过是把恒大想成了汽车行业的企业家，需要对这一行有超出商业范畴的责任。不过人家本质上是一名商人。 读《中国制造2025》另外一个感受，就是制造业升华的制约，已经不在制造业本身，而是体制。企业家、商人做了他们该做的，甚至在中国这样行业秩序、商业秩序、社会秩序相对匮乏的条件下，他们已经做了足够多了。 记得有位中国vp评价说，相比中国的投资人，哪有外国的投资团队的活路。大概就是想表达，中国各行各业的生存之多艰，相对好的制度下的培育的行业团队，中国行业是颇有杀伤力的。 当然，这样的杀伤力更是“劣币驱逐良币”。长久看是恶化商业环境，既不是鼓励了资源优化、也不是鼓励创新、更不是鼓励更人性的生存环境。所以这样的“强势”，真叫人哭笑不得。也大约是全世界都逐渐清醒地抵制中国发展模式的根因。 商人与企业家的不同，更突出的体现在mindset不同。于我，可能默认自己对行业还有一些初心，不是唯利是图的。所以，愿意了解的都是行业内技术、管理实践等等。当我在拼命地理解和转化自动驾驶新技术和趋势，对国外成熟管理模式的移植，当然也很力不从心。开淘宝店，开咖啡馆的朋友，根本没有这样的mindset。服务行业不是没有科学管理和技巧可言，只是偏技术迭代的mindset偏离了服务的本质。 然鹅，在基础科学研究和应用科学转化上的各种弊病，以及反应在知名企业可供选择的求职口上的差距，实际上让国内想成长为企业家的年轻人，或安心当工程师的年轻人，是非常吃亏的。即便比美国年轻人更努力，也不会比他们生活的更舒适和自信。在这样的实体环境下，谈技术积累，应用转化，企业家精神，就像明知无望，还让人上的理想主义。可是现在的年轻人，才不会为了“爱国“二字义无反顾，国家欠着我等年轻人多了，收割我等韭菜，还要我等为着看不到的未来埋单。 相比实业，在国内，服务业的发展，至少不会明显存在差距。因为人总是要吃喝拉撒被服务的，虽然服务质量、体验、管理等，不如发达经济体，但是服务业没准备国际化：开饭店的、洗衣房的、健身房的等等，没准备打入国际市场，总之还可以混个体面生活。 国家抱怨国内的小老板，小企业家，小富即安，不思进取，不能担当“百年老店”。说实话，心里都明白：有制度保障，企业家大富吗，保障企业百年长青吗？与其显山露水，被制度卡脖子，大家都不愿做被拍死的出头鸟。也大概是万一做大了，就赶紧换国籍的原因。 我不再抱怨企业家、商人和个体，什么样的社会体制孕育企业和个人。说国内企业无奸不商，老百姓缺乏仁心仁德，只是国家的问题。在对国内实业无望之后，大量实业人才流入金融证券等服务业，大量企业家沦为商人，投机买地，也是无奈之举。不知道还有没有明天，今天就做一回人吧。谁不爱惜一次的生命呢。 两个人都挣50万，在学校承包食堂的经理a和在大公司上班的工程师b，显然是不同的。 国内混久了，初心早就喂狗了。活着，才是中国人的最高哲学。 ps, 不吐槽，不快乐。you just can’t change your role in life, even you don’t like it sometimes, so enjoy it happy or unhappy. 个人技能进阶带团队，会有一个时间段特别兴奋，感觉每天都有很多进步；也有一个时间段，非常阻塞，看不清下一步。又在一个阻塞期，想到了底层实力。 底层实力做技术/工程应用，前期缺乏很高明的应用框架设计，大部分时候就是解决具体的问题。手段如何都不重要。大概是测试驱动的开发，不断调试，试错，直到找到一个通畅的解决。下次再碰到新问题，继续试错调整。这样的工作模式，也许不算agile。 目前阶段开发的几个层次： 熟悉项目，写简单的逻辑 使用成熟的库/框架（网络，前端，分布式） 掌握新编程思想(协程、异步、容器、微服务） 与此同时，对开发工具，生产环境(git, Docker)，应用场景的拓展逐渐成型。 产品导向缺乏底层实力，会容易被卡住。或者说缺乏产品导向，走着走着，不知道下一步在哪儿了。产品导向，需要一个人多任务，多人协调能力。 初创公司生存技术突破做ai等技术场景的初创公司，花大量的资源和人力，研究和实现新算法，提高算法的识别率，精度等。 产品入口相反，传统行业的创业，并没有领先技术和雄厚资本，总是从最不起眼的组装产品开始的。 资本壁垒一些规模效应的创业团队，投资人大量投入资本拼杀。]]></content>
  </entry>
  <entry>
    <title><![CDATA[npc wp planner in ADS simulation]]></title>
    <url>%2F2019%2F09%2F11%2Fnpc-wp-planner-in-ADS-simulation%2F</url>
    <content type="text"><![CDATA[backgrounda ADS simulation platform should have the three components: vehicle dynamics model for game engine(e.g. unReal, Unity3D) based ADS simulation platform, there has simple game-level vehicel dynamics(VD), and in engineering side, there are more precise VD, such as Carsim, CarMaker, and a few smaller ones. sensor models currently camera and Lidar model are pretty good, for mm Radar, which has a frequency around 24G hz or 77G hz, which is far more beyond the digital computer can sample, so to simulate Radar(or Radar model) is not that practicalable. as can find in famous simulator, e.g. lg-sim and Carla, they are puting a lot efforts in sensor models, which should be more useful in future. another good way of sensor model simulation, is to test and verify the vendor’s sensors. most time, the performance test from vendors are not fit well. and as the physical model of the sensors is not visible, to simualate as a black box is useful then. scenario describe PEGSUS project used OpenScenario xml language; in other case, the scenario can be described directly in Python. basically what need to define during scenario descibtion, includes: scenario static env scenario npc actors movement in more advanced solutions, the process of builing virtual envs directly start from dronze/cars images or cloud point images. what’s in this post is one part of npc movement control: lane change. lane_change_done eventat default lg-sim, there is a lane-change event, which describes when lane change happens; for a more precise npc control, here adds a lane-chagne-done event, basically to describe when the lane change is done. similar to lane-change event, here defines the lane-change-done event in both server and client side. Unity.transform.position vs local positionhere is the math part. Unity has plenty APIs to handle these, which is good to study in. dot product Vector3.Dot() cross product p2p distance Vector3.Distance() interpolate Mathf.Lerp() world 2 local transfer Transform.InverseFromPoint(position) camera matrix equations of lines waypoints described laneNPC agents are drived by waypoints embedded in the virtual roads, in lg-sim, waypoints is inside LaneSegmentBuilder. but in general the waypoints are not ideal: their distance is not unique their index in neighboring lanes doesn’t keep consistent they don’t care of curves of real roads for waypoints based planner, rather AI-based planner, e.g. lane-change planner, the processes are as following: get currentTarget in target(current) lane which is usually not pointed by the same currentIndex as the previous lane, so need to figure out the closeset wp from the waypoint list in current lane, and this closest wp should be in front of NPC (no consider NPC retrograde). keep currentTarget update during NPC (lane-change) operation, there is case when the currentTarget is behind NPC position, if it’s not expected, it’s always a error planner, leading NPC header turn over. so need to check currentTarget is always in front of NPC, if not, update currentTarget to next wp. 1) driving direction vs currentTarget2NPCposition driving direction in global dot product currentTarget2NPCposition should greater than zero, if not, update currentTarget 2) NPC in currentTarget local position NPC local position should always in negative axis, if not, update currentTarget. the trick here, can’t use currentTarget in nPC local position, as when NPC is head of currentTarget, NPC will be pointed in reverse, which makes the local coordinate reverse as well. but currentTarget is the fixed wp always. lane-change-done criteria ideally when the whole NPC occupied in the target lane, that’s when the lane-change operation done. but that’s never the reality. as then, if there is the next next lane, the NPC is partly in the next next lane, so can’t keep lane change in only one neighboring lane. but in reality, highway or express way, the vehicle can’t across two lane in one time. so to keep lane-change-done and done in just the next lane, the criteria is when the NPC position in Z or X direction projection to currentTarget X or Z direction projection is more than half of the lane width: Mathf.Abs(frontCenter.position.z - currentTarget.z) &lt; laneWidth/2.0 usually in game engine, NPC can be controlled by AI, will discuss later.]]></content>
      <tags>
        <tag>lg-sim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[a nested conditonal variable model in python]]></title>
    <url>%2F2019%2F09%2F06%2Fa-nested-conditonal-variable-model-in-python%2F</url>
    <content type="text"><![CDATA[background the work is derived from lg-sim/lgsvl/remote, the original remote function is listen message return from server on each command request, basically a async request-receive model. additionaly, we want lg-sim server to send every episode state info to client too, luckily, the server - client communicate is based on Websocket，which support server actively pushing message. so simple add a episode state info in server side and send it through websocket at every frame works. in conditional variable design, notify() won’t be hanged-up, while wait_for() can be hanged-up if no other thread call notify() at first. in Python class, to keep object status, it’s better to use class member variable, rather than object member variable, which can’t track its status when the object reset. (/) try finally the status update in try() package won’t be the final status . 12345678910111213141516 def process(type): status = False try: if(type=="a"): status = True finally: if(type == "b"): status = False print("cv.status ....\t", status) return statusdef main(): type_list = ["a", "a", "b", "b", "a", "a", "a"]; for _ in range(len(type_list)): print(process(type_list[_])) the client designin client, the message received is handeled in process(). by default, there is only one type of message, namely the event message, so only one conditional variable(CV) is needed to send notification to command() to actually deal with the recieved message. first we defined the new message type(episode message), as well as a new real-handel fun episode_tick(). then modifying remote::process() as: 123456789101112131415try: self.event_cv.acquire() self.data = json.loads(data) if check_message(self.data) == "episode_message" ： self.event_cv.release() self.event_cv_released_already = True with self.episode_cv: self.episode_cv.notify() else : self.event_cv.notify()finally: if self.event_cv_released_already: self.cv_released_already = False else: self.cv.release() will it dead-locked ?as both command() and episode_tick() have conditional variable wait_for() : 1234567public command(): with self.event_cv : self.event_cv.wait_for(lambda: self.data is not None)public episode_tick(): with self.episode_cv.wait_for(lambda: self.data is not None) so if notify() is not called in any situation, dead-locked happens, meaning the wait_for() will never return but suspended. remote.process() is running in another thread, rather than hte main sim thread, where run sim.process(). remote.process() used to accept the message, and sim.process() is the actual place to handle these received messages. the two threads run simutaneously. for any message received, if its type is event_message, then it triggers event_cv.notify(), so command() in sim::process() won’t dead block; to avoid episode_message dead locked, in sim::process() need to call episode_tick() only when the message type is episode_message, which can be checked by remote.event_cv_released_already == True, 123456789101112def sim::process(events): j = self.remote.command(events) while True: if self.remote.event_cv_released_already : self.remote.episode_tick() if j is None: return if "events" in j: self._process_events(j) j = self.remote.command（“continue")]]></content>
      <tags>
        <tag>python</tag>
        <tag>multithread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vehicle dynamics model in ADS simulation]]></title>
    <url>%2F2019%2F09%2F03%2Fvehicle-dynamics-model-in-ADS-simulation%2F</url>
    <content type="text"><![CDATA[VD backgroundusually to simualate vehicle dynamic(VD) system, either by physical model, e.g. pysical models of engine, gearbox, powertrain; or parameter model, which doesn’t take into the physical process of the dynamic system, but tracking the system’s input, output and fit their relationship with polynomail equation or functions with multi-pieces. traction force is derived from engine torque, which goes to gearbox(powertrain system) and then divided by radius of wheels, then distribute to wheels. traction torque air drag air lift force traction force $$ F(traction) = F(air drag) + F(air lift force) + F(tire drag) + acc * mass $$ in a detail way, the equation above should split into lateral state equation and longitudional state equation, if consider driver control module, which will give laterl control equation and longitudional control equation. brake torque and ABS systemABS(anti-block system) works in the situation, when driver input brake torque is larger than the max ground-tire torque can attached between tire and ground. once max ground-tire torque is achieved, namely the max fore-aft force T is achived, the traction direction traction slip angular decceleration will leap, this is the dead-blocking situation, and when it happens, the driver input brake torque is saturated. to avoid block situation happens, usually track decelleration of traction slip angular during brake torque increase, if the value of decelleration of slip traction angular is beyond a threshold value, ABS controller will trigger to decease brake torque. drive stabilitythe driving stability is mainly due to forces on tires, sepcially the lateral angular velocity derived from Lateral Force lateral controltaking driver as one element, the driveing system is a close-loop control system. the system works on a road situation: the driver pre-expect a driving path(predefined path) and operate the steering wheel to some certain angular the vehicle take a move with a real driving path(real path) the real path is not exactly fitted to the predefined path, leading the driver take an additional conpensation control longitudinal controlsimilar as lateral control VD in Unityany vehicle in Unity is a combination of: 4 wheels colliders and 1 car collider. WheelConllider1) AxleInfo AxleInfo represents the pair of wheels, so for 4-wheel vehicle, there are two AxleInfo objects. 12345678910struct AxleInfo &#123; WheelCollider left ; WheelCollider right; GameObject leftVisual ; GameObject rightVisual ; bool motor ; #enable movement of this wheel pair bool steering ; # enable rotation of this wheel pair float brakeBias = 0.5f; &#125; 2) core parameters wheel damping rate suspension distance Force apply point distance (where ground force act on wheel) suspension spring forwardSlip(slip angle), tire slip in the rolling(tractional) direction, which is used in calculating torque sidewaySlip, the lateral direction slip, which leads to stability issue. 3) visualization the WheelCollider GameObject is always fixed relative to the vehicle body, usually need to setup another visual GameObject to represent turn and roll. implementation from lg-sim 1234567891011void ApplyLocalPositionToVisuals(WheelCollider collider, GameObject visual) &#123; Transform visualWheel = visual.transform; Vector3 position; Quaternion rotation; collider.GetWorldPose(out position, out rotation); visualWheel.transform.position = position; visualWheel.transform.rotation = rotation; &#125; 4) WheelCollider.ConfigureVehicleSubsteps 1public void ConfigureVehicleSubsteps(float speedThreshold, int stepsBelowThreshold, int stepsAboveThreshold); Every time a fixed update happens, the vehicle simulation splits this fixed delta time into smaller sub-steps and calculates suspension and tire forces per each smaller delta. Then, it would sum up all resulting forces and torques, integrate them, and apply to the vehicle’s body. 5) WheelCollider.GetGroundHitreturn the ground collision data for the wheel, namely WheelHit wheel friction curvefor wheels’ forward(rolling) direction and sideways direction, first need to determine how much the tire is slipping, which is based on speed difference between the tire’s rubber and the road,then this slip is used to find out the tire force exerted on the contact point the wheel friction curve taks a measure of tire slip as an Input and give a force as output. The property of real tires is that for low slip they can exert high forces, since the rubber compensates for the slip by stretching. Later when the slip gets really high, the forces are reduced as the tire starts to slide or spin 1) AnimationCurveunity official store a collection of Keyframes that can be evaluated over time vehicleController() in lg-sim1) the controllable parameters: 123456currentGear currentRPMcurrentSpeed currentTorquecurrentInput steerInput 2) math interpolate function used Mathf.Lerp(a, b, t) a -&gt; the start value b -&gt; the end value t -&gt; the interpolation value between start and end 3) fixedUpdate() 1234567891011121314// cal trace force by rigidbodyrigidbody.AddForce(air_drag)rigidbody.AddForce(air_lift)rigidbody.AddForceAtPosition(tire_drag, act_position)// update current driving torquecurrentTorque = rpmCurve.Evalue(currentRPM / maxRPM) * gearRaion * AdjustedMaxTorque // apply torque // apply traction control// update speedcurrentSpeed = rigidbody.velocity.magnitude// update fuel infofuelLevel -= deltaConsumption// update engine temp// update turn signal light 4) ApplyTorque() 123456float torquePerWheel = accelInput * (currentTorque / numberofWheels) foreach(axle in axles): if(axle.left.motor): axle.left.motorTorque = torquePerWheel if(axle.right.motor): axle.right.motorTorque = torquePerWheel 5) TractionControl() 123456789101112TractionControl()&#123; AdjustTractionControlTorque(axle.hitLeft.forwardSlip)&#125; AdjustTractionControlTorque(forwardSlip)&#123; if(forwardSlip &gt; SlipLimit) tractionMaxTorque -= 10 else tractionMaxTorque += 10 &#125; in lg-sim, the VD model is still simple, as there is only traction/logitudional control. referadd equation in markdown wheelcollider doc whell collider official whellcollider tutorial]]></content>
      <tags>
        <tag>lg-sim</tag>
        <tag>vehicle dynamics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里-9]]></title>
    <url>%2F2019%2F08%2F31%2F%E6%9C%AA%E6%9D%A55%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C-9%2F</url>
    <content type="text"><![CDATA[低速发展和高附加值昨天看到一篇比较中欧(美)国家差异的帖子：北欧国家是如何保持高福利的。非常有意思的一个概念：高附加值，低发展速度的行业。 这些国家有一些传统强势行业。很有意思的是，印象中的传统行业，比如农业、畜牧业、纺织、手工业等等，跟高科技、现代化很远，似乎是非洲国家的土著人干的活儿。相比，在国内，互联网、技术创业、地产、金融、mba营销、国际贸易等才是时尚。 早年在农业大学，对以色列强大的现代农业是有所耳闻的。只是没有这种洞察力，现代化的传统行业与国民福利之间还有这样的隐形关系。 相比，美国的农业、畜牧业等传统行业很不错；美国也引领科技、金融这些时尚的行业。在美国生活的人民确是有了进退的余地。向左，可以退居大农村；向右，可以进军纽约、硅谷当金融、科技新贵。 国家的选择看似主动，也是现实的驱动。国家可以集中发力在互联网、金融、创业等方面做的很有声有色，但与此同时，传统的制造业，典型低发展速度，需要长期积累，国家发力也解不了。比如，汽车发动机、精密仪器、芯片制造等。 制造业在国内的标签是低附加值，资本不愿意进入，国家队发力也没气色。然而经过了这些长期积累的国家，比如欧洲小国瑞士、德国，制造业出口完全是高端制造的典范。而且马太效应显著，让这些欧洲国家不需要社会剧烈的经济变动，不需要老百姓担心行业变动、反倒一年还有20天带薪假。政府和人民都其乐融融，有时候看北欧人的生活，建筑、厨房、国家公园、办公环境、甚至监狱，简直像在童话里；报道一个小小的新闻，似乎都要惊动整个镇子上的人一样，这里简直就生活在像伊甸园。 而中国总是一片火热的场景，5年大力发展基建、5年大力吹捧全国创业、又5年搞工业互联网。国家没有消停，老百姓有人欢喜有人愁。 中国没有先发优势，在这些传统行业缺少长期积淀，强行进入既不不讨资本喜欢，也不讨人喜欢。比如报道，西安火箭研究员工资20万；大国重器的老焊工，无数荣誉奖章，一个月才一万多，在北京买不起房；国家队出动安利中小企业贷款，但银行就是宁愿带给亏损的国企，也不给需要钱的中小企业。 对于这些需要长期资本补血，短期回报低，才能形成壁垒的传统行业。在这个资本、市场竞争相对成熟的年代，确实比100年前，搞种植、搞养殖，或西门子、博世等制造业刚出道积累行业(非金钱资本)“资本”要困难的多。似乎印证了上周接触深圳汽车电子行业的感触，一个词：活着。根本谈不上行业积累。 可以说，北欧国家以及北美国家，因为上几代人的积累，给他们留下的行业遗产、社会制度遗产，才允许他们活的这么滋润。相比，中国没有上几代人的遗产，又进入了成熟资本市场的21世纪，所以，是压力也是机遇。去发掘新世纪的矿吧。 对这个大环境的从业者而言，选择农业、传统制造业，也确实不聪明。干着比人累的活儿，挣得比人少，社会还不待见。年轻人选择行业要识大局。相比，这个时代热的东西，确实该多关注。存在即合理。 工程师的心态国内有些企业家喜欢讲情怀，吹“工匠精神”, 结果把自己作死了。而华为这样的企业，可以拿高薪，也是加班拿命换的。能够心安理得的当一名工程师，不需要为生计、价值实现、社会待见操心的，只会在这些“低速、高附加值”的欧美公司。一个国家的企业能进化到这种形态，基本是社会稳定，企业稳定，developed，达到了一个最佳平衡点，再投资也不会增加收益，每个岗位都相对稳定，组织内部管理效能达到最优，系统的规范化远超过个人能力，所以每个员工心安理得，做好自己份内的事就好。剩下的时间，享受生活，或者出于兴趣爱好，搞点车库创业都不在话下。 相比，国内的市场还在巨变当中，没有一家企业已经进化到developed，企业组织系统还不成熟，个人能力还能发挥显著价值，那老老实实当工程师的，在还在发生资源重组系统里，就会被挤到最底层。所以，不要怪国内的年轻人心态不正。因为这个时代，传统技工不会长在国内，国内应该培养属于这个时代的阶层和行业。想跟欧美，比工匠精神，确实是拿短处跟人家的长处比。]]></content>
  </entry>
  <entry>
    <title><![CDATA[reactjs introduction]]></title>
    <url>%2F2019%2F08%2F30%2Freactjs-introduction%2F</url>
    <content type="text"><![CDATA[React is the front-end framework used in lg-sim WebUI(version 2019.07). as well for Cruise webviz, Uber visualization, Apollo, they all choose a web UI design, there is something great about web framework. what is Reactused to build complex UI from small and isolated pieces of code “components” 1234567class vehicleManager extends React.Component &#123; render() &#123; return &lt;html&gt;&lt;/html&gt; &#125; &#125; React will render the html on screen, and any changes in data will update and rerender. render() returns a description(React element) of what you want to see on the screen, React takes the descriptions and displays the result. build a react hello-world app12345npx create-react-app react-democd react-demonpm start index.jsindex.js is the traditional and actual entry point for all Node apps. in React, it tells what to render and where to render. componentscomponents works as a func, and props is the func’s paramters, the func will return a React element which then rendered in view components can be either class, derived from React.Component, which then has this.state and this.setState() ; or a function, which need use Hook to keep its state. the design advantage of components is obvious: to make components/modules reuseable. usually in route.js will define the logic switch to each component based on the HTTP request. setStatewhenever this.setState() takes an object or a function as its parameter, update it, and React rerender this component. when need to change a component state, setState() is the right way, rather than to use this.state = xx, which won’t register in React. Hookexample from stateHoook 12345678910111213141516import &#123; useState &#125; from 'react';function Example() &#123; // Declare a new state variable, which we'll call "count" const [count, setCount] = useState(0); return ( &lt;div&gt; &lt;p&gt;You clicked &#123;count&#125; times&lt;/p&gt; &lt;button onClick=&#123;() =&gt; setCount(count + 1)&#125;&gt; Click me &lt;/button&gt; &lt;/div&gt; );&#125; Hook is a speicial feature, used to share sth with React function. e.g. useState is a way to keep React state in function, which by default has no this.state in Hook way, even afer function() is executed, the function’s variable is not clear, but keep until next render. so basically Hook give a way to make function stateable. intial hook the only parameter pass in hook::useState(new Map()) is the initial state. useState(initState) return from useState it returns a pair [currentState, setStatefunc], e.g. [maps, setMaps], setStatefunc here is similar as this.setState in class component access state directly {currentState} update state {() =&gt; setStatefunc(currentState)} Context context gives the way to access data, not in the strict hierachy way. the top is the context Provider, and the is the context consumer, there can be multi consumers. createContextas in react context official doc 1const Context = React.createContext(defaultValue); during render, the component which subscribe this context will get the context content from its context provider and put in rendering, only when no avialable provider is found, defaultValue is used. 1&lt;Context.Provider value=&#123;/xx/&#125; &gt; whenever the value in Provider changes, all consumers will rerender. EventSourceEventSource is HTTP based one-way communication from server to client, which is lighter than Websocket eventsource vs websocket, also the message type is only txt in EventSource, while in Websocekt it can be either txt or binary stream. by default, when client received a [“/event”] message, will trigger onMessage(). but EveentSource allow to define user-speicial event type, e.g. VehicleDownload, so in client need to a new listener: 1myEventSource.addEventListener('VehicleDownload', (e)=&gt;handleVehicleEvents(e)) react routejs specialfirst class object a function is an instance of the Object type a function can have properties and has a link back to its constructor method can store a function in a variable pass a function as a parameter to another function return a function from another function promisearrow function]]></content>
      <tags>
        <tag>react</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nancyfx study]]></title>
    <url>%2F2019%2F08%2F30%2Fnancyfx-study%2F</url>
    <content type="text"><![CDATA[document, Nancy is used in lg-sim webUI 2019.07 version, pretty new staff for me. IntroductionNancy is a lightweight, low-ceremony framework for building HTTP based services on .NET and Mono. Nancy is designed to handle DELETE, GET, HEAD, OPTIONS, POST, PUT and PATCH request and provides a simple, elegant Domain Specific Language(DSL) for returning a response. build to run anywhereNancy was designed to not have any dependenceis on existing frameworks, it’s used pretty much wherever you want to. host in Nancy acts as an adaptor for a hosting environment, thus enabling Nancy to run on existing techs, such as ASP.NET, WCF. the bare minimum requires to build a Nancy service are the core framework and a host. helloworld serviceall module should be derived from NancyModule, and define a route handler. tips: always make the module public, so NancyFx can discover it. 12345678public class HelloWorld : NancyModule &#123; public HelloModule() &#123; Get[&quot;/&quot;] = parameters =&gt; &quot;Hello World&quot; ; &#125;&#125; exploring modulesmodule is where you define the logic, is the minimum requirement for any Nancy app. module should inherit from NancyModule, then define the behaviors, with routes and actions. modules are globally discoveredthe global discovery of modules will perform once and the information is then cached, so not expensive. Define routesto define a Route need to specify a Method + Pattern + Action + (optional) Condition 123456789public class VehicleModule : NancyModule&#123; public VehicleModule() &#123; Get[&quot;/vehicle&quot;] = _ =&gt; &#123; // do sth &#125;; &#125; &#125; or async run: 123456789public class VehicleModule : NancyModule&#123; public VehicleModule() &#123; Get[&quot;/vehicle&quot;, runAsnyc: true ] = async(_, token) =&gt; &#123; // do sth long and tedious &#125;; &#125; &#125; Methodmethod is the HTTP method used to access the resource, Nancy support: DELETE, GET, HEAD, OPTIONS, POST, PUT and PATCH. secret for selecting the right route to invokein case when two routes capture the same request, remember : the order in which modules are loaded are non-deterministic routes in a given module are discovered in the order in which they are defined if several possible matches found, the most specific match. root pathall pathes used in Nancy are relative to root path, which tell Nancy where its resources are stored on the file system, which is defined in IRootPathProvider static contentstatic content is things e.g. javascript files, css, images etc. Nancy uses a convention based approach to figure out what static content it is able to serve at runtime. Nancy supports the notion of having multiple conventions for static content and each convention is represented by a delegate with the signature Func&lt;NancyContext, string, Response&gt; the delegate accepts two parameters: the context of the current request and the application root path, the output of the delegate is a standard Nancy Response object or null, which means the convention has no static content. define your own conventions usign the bootstrapperlink View enginesview engine, takes a template and an optional model(the data) and outputs(usually) HTML to be rendered into the browser. in lg-sim, the view is rendered in nodejs. MVCmodel view controller understand controller a controller is reponsible for controlling the flow logic in the app. e.g. what reponse to send back to a user when a user makes a browser request. any public method in a controller is exposed as a controller action. understand view a view contains the HTML markup and content that is send to the browser. in general to return a view for a controller action, need to create a subfolder in the Views folder with the same name as the controller. understand model the model is anything not inside a controller or a view. e.g. validation logic, database access. the view should only contain logic related to generating the user interface. the controller should only contain the bare minimum of logic required to return the right view. C# anonymousc# programming guide 12345(input-parameters) =&gt; expression (input-parameters) =&gt; &#123;&lt;sequence-of-statements&gt;&#125;TestDelegate testD = (x) =&gt; &#123;(Console.WriteLine(x);&#125;; =&gt; is the Lambda operator, on its left is input paramters(if exist). Lambda expression/sequence is equal to a delegate class. delegate is a refer type, used to pass one function as paramter to another function. e.g. in event handle. Nancy in lg-sim WebUI/Assets/Scripts/Web a few steps to build Nancy server: Nancyhost.start() add NancyModule instance(where define route logic) a few other libs used : PetaPoco, a light-weight ORM(object relational mapper) framework in C# SQLite refermeet nancy nancy doc in chinese `]]></content>
      <tags>
        <tag>c#</tag>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里 (8).md]]></title>
    <url>%2F2019%2F08%2F24%2Fwhere-are-you-in-next-5-years-9%2F</url>
    <content type="text"><![CDATA[这一系列的思考来源两个事件：深圳汽车电子行业代表来司交流；与全球机器人大赛参展。 深圳的中小企业 vs 资本大户布谷鸟 智能驾驶座舱解决方案供应商。主要产品，车载显示屏，仪表屏和基于Android操作系统的上层应用。展示完，第一感受就是深圳华强北来了。 当年（2007年)，中国风靡深圳山赛手机。甚至多年以后，山赛智能手机成了中国发展的一个典型案例，为众人津津乐道： 眼看他起高楼，眼看他宴宾客，眼看他楼塌了。 这个产业最初蓬勃，但凡投资进去的人，都躺着赚钱，只是没能力在蓬勃发展期间为转型蓄力。后来政策、市场、甚至原材料的任何风吹草动，都足以摧毁它。 这个产业的问题放大了看，就是 华为/阿里 与 中兴/联想 的对比。国内的制造业小企业，最初都是代理、组装、贴标签、山赛货。如果碰到市场爆发，春风得意，赚的盆满钵满。但是，后续如何发展，创始人的vision就立判高下了。 一个选择是立足长远利益，未雨绸缪，生于有患。在打开了市场之后，立马投入产品和相关技术、供应链、品牌积累。可以接受眼前的低资金回报，选择了艰辛但持久的终成就行业头部的道路。 一个选择是只看赚钱。自己做产品积累，回钱周期太久，根本不考虑。资本趋利，大不了赚到钱，再转战下一片红海。这是势利的商人思路，他们对某一个具体行业不带任何感情，冷血。任何行业只是一个资本生长的土壤，不管是做手机、地产、保险、造车等等，都不是出于热爱，而是资本逐利。比如，宝能收购万科，就是冷血的资本挑战性情的行业中人。 但凡对行业还有所爱的，都难以忍受被资方强奸。所以，恒大站出来说要造车，我的第一反应，就是恒大要来强奸造车人。恒大没有对汽车的感情，只不过跟进资本进军汽车领域。资本原本是公司的生产要素之一，但是资本又最不具有公司界限约束，任何外部资本都能挑战公司自身。所以，如何让资本服务于行业公司？ 制造业创业回到布谷鸟，号称做车载计算平台，不自己做计算芯片，不自己做车载屏幕，不自己做车载定制Android操作系统，顶多开发几个上层app。这不又是一家华强北组装厂吗。创业还靠廉价组装竞争？相比，北上的创业公司，诸如硬件地平线、软件旷视等算是技术/境界高多了。 也不得不提本司。背靠大树，天然壁垒，但实际做的还是华强北的活儿。不同之处是，资本压力小，对技术储备有规划但没有环境和姿态的转变。会聘一两个外国人、一两个教授撑面儿，但这几位基本处于边缘，决策、项目规划都不考虑他们。估计其他家，也好不了哪里去。要不然，市场会有反馈的。 这样的团队/小公司里面，对想要拔高产品/技术见识的年轻人其实很艰难。因为打开市场，销售是关乎存活的，相比产品是自研的，还是贴牌的，有没有底层开发/设计积累都是次要的。 要去这种小团队做产品/技术，就需要能单挑担子的产品人/技术大牛。当然，付出与回报怎么权衡。给ceo待遇，似乎可以考虑。 传统制造业大公司里面的小团队，虽然没有自己去开辟市场的压力，大不了内销，但同时带来的问题，是积重的公司元老，对职业经理人，工程师文化，都是严重的挑战。 公司元老经常会看到，职业mba人在中国企业水土不服，或者回国的硅谷工程师对中国企业文化的抱怨。虽然，宏观数据国内企业似乎都很国际化了，底层的/微观的现代公司管理/制度上，却不是一代就能跟上国际的。比如，mbaer到中国公司，很难战胜资源/权利在握的元老们，按照现代管理办法去调整公司，那基本上就等着被边缘化，然后公司不养闲人，下一步就是踢出去。公司又回到原来的管理模式。老人们再次证明自己不瞎折腾，是公司的积重，继续升官发财。 工程师文化，不管是如google等互联网公司，还是如ford等汽车制造业公司，都比较明显。hr, manager, 甚至leader团队把自己归为工程师团队提供服务的，在衣食住行、工作环境、设备、交流、培训等等都照顾的很好。 虽然北美是更成熟的资本社会，他们的企业反而不那么见钱就撒网，没见过ford投资移动互联网，google要去收购造车厂，或者发展文化传媒业务的。他们的业务更倾向纵深，专注，在一个领域深耕，然后成为行业头部。这可能也是成熟资本市场的现代化公司该有的样子。 相比，国内的企业太草莽。国内的工程师只有听命于人。技术积累对绝大多数企业，都是可以谈谈但不会认真做的。这样的情况，当然长远是没前途的。一波政策、市场的红利之后，基本作死。 人口基数国内也有很多有情怀的创业人或中小企业主，他们也许并不甘与政策红利，也想认真做产品。但是，在这个社会没办法。 美国的创业环境，想想apple, google, facebook，cruise等等创业经历。首先他们自己没有生存压力，出于热爱开始的；创业阶段，市场/政策对他们的包容度很好，允许决策失败；然后，资本市场、专业人才的补充都有成熟的供应体系。 国内，一方面有廉价的人口红利，但对大部分创业公司，规模经济反而很难真正成为其产品的利好因素，除了资本玩家，比如，摩拜单车、瑞幸咖啡、yy连锁酒店，滴滴出行等等，这些玩家，根本不是在拼产品，而是资本。谁占有更多资本，谁就能笑到最后。 这样的创业环境，不能培养社会范围内更好的创业土壤，反而破坏了创业各方面的供应系统，包括创始人的初衷，专业人才队伍，资本法务系统建立等等。归结一个词：浮躁。 人口基数大，在政治家、资本家眼里是好词，但是对个体，就意味着就业竞争、服务质量差、人际关系不温存。 经历过大公司的年轻人在三四线小城市，有很多生活无忧的年轻人。他们大学毕业后，在大城市瞎晃了一年半载，找不到合适的企业，就打道回府了。家里有条件，在当地慢慢都会活的滋润。我是觉得，经历下一个现代化管理的大公司，也是个不错的体验。至少知道人类社会，最优秀的组织体系是怎么运作的。 当然，没有上升，一辈子在大公司的系统里打工，就有点无趣。不如回家悠哉悠哉。 世界机器人大会2019整体感受，小企业生存多艰。]]></content>
  </entry>
  <entry>
    <title><![CDATA[real-world env in ADS simulation]]></title>
    <url>%2F2019%2F08%2F20%2Freal-world-env-in-ADS-simulation%2F</url>
    <content type="text"><![CDATA[this topic is based on lg-sim, the advantage is plenty Unity3D resources. to drive planning design, simulation has to keep as real-world env as possible. for L3 and below, the high-fidelity is actually most about HD map, since in L3 ADS, sensors only cares about the road parternors on road, and planning module take hd map as input. so there is a need to build simulation envs based on hd map. osmin the Unity engine, all in the env(e.g. traffic light, road lanes, lane mark etc) are GameObjects. to describe a map info, one of most common used map format is OSM (another is opendrive, but no open parser in unity yet), however which is not born to used in hd map, but still is very flexible to extend to descripe all info, a hd map requires. e.g. lane id, lane width, lane mark type e.t.c the open-source tool OsmImporter is good enough to parse osm meta data into Unity GameObjects. for different map vendors, first to transfer their map format into the extended-osm format. and then can generate all map info related gameObjects in Unity env. that’s the main idea to create real-world env in simualtor. the end is to make a toolchain from map vendor input to Unity 3D env. waypointsin either lg-sim or carla, the npc vehicles in self-driving mode, is actually controlled by following waypoints in the simulator, and waypoints is generated during creating map in step above. carla has plenty client APIs to manage the waypoints, and then drive npcs. hd-map toolby default, both lg-sim and carla has a tool to create hd-map, that’s basically mark waypoints on the road in an existing map, which is not strong. carla later support Vector one to build in map more efficiently. L3+ virtual env generatorthere are plenty teams working on building/rendering virtual envs directly from sensor data, e.g Lidar cloud point or camera image, and plenty image-AI techs here, which of course gives better immersed experince. and the test task is mostly for perception, data-fusion modules, which is heavier in L3+]]></content>
      <tags>
        <tag>simulation</tag>
        <tag>lg-sim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[threading and websockets]]></title>
    <url>%2F2019%2F08%2F16%2Fthreading-and-websockets%2F</url>
    <content type="text"><![CDATA[threading.Thread123456789101112run()start()join([time])isAlive()getName()setName() to initialize a thread with: threadID, name, counter start() and run()start() once for each thread, run() will not spawn a separate thread, but run in current thread 1234567891011121314class myThread(threading.Thread): def __init__(self, *args, **kwargs): super(myThread, self).__init__(*args, **kwargs) def run(self): print("run ... from start()")if __name__ == "__main__": demo = myThread() demo.start() demo.join() lock objectsa primitive lock does not belongto a certain thread when locked. by default, when constructed, the lock is in unlocked state. acquire(), will set the lock to locked and return the lock immediately(atom operator); if current lock is locked, then acquire() blocks (the thread) untill the occuping-lock thread calls release(). if multi-thread blocked by acquire(), only one thread will get the lock when release() is called, but can’t sure which one from the suspended threads condition objects12345678910threading.Condition(lock=None)acquire()wait()notify()release() thread A aquire() the condition variable, to check if condition satification, if not, thread A wait; if satisfied, update the condition variable status and notify all other threads in waiting. websocketbackgroundwebocket is better than http when the server need actively push message to client, rather than waiting client request first. clientusually webSocket client has two methods: send() to send data to server; close() to close websocket connection, as well a few event callbacks: onopen(): triggered when websocket instance get connected onerror(), onclose(), onmessage(), which triggered when received data from server. when constructing a webSocket instance, a connection is built between server and client. 1234567891011121314151617// client connect to server var ws = new WebSocket('ws://localhost:8181');ws.onopen = function()&#123; ws.send("hello server");&#125;// if need run multi callbackws.addEventListener('open', function(event) &#123; ws.send('hello server'); &#125;); ws.onmessage = function(event)&#123; var data = event.data ;&#125;;ws.addEventListener("message", function(event)&#123; var data = event.data ;&#125;); ws.send('client message'); onmessage()whenever the server send data, onmessage() events get fired. server onOpen(), triggered when a new client-server connection setup onMessage(), triggered when message received onClose(), onError(), implementationin c#, there is websocket-sharp, in python is python-websockets, in js is nodejs-websocket. as well as websocket protocol is satisified, server can write using websocket-sharp, and client in python-websockets. an example is in lg-simulator. the message type in websocket can be json or binary, so there should be json parse in c#(SimpleJSON), python(json module) and js (JSON). refersending message to a client sent message from server to client python websockets nodjs websocket c# websocket]]></content>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unet intro]]></title>
    <url>%2F2019%2F08%2F16%2FUnet-intro%2F</url>
    <content type="text"><![CDATA[unity3d manual High Level API(HLAPI) HLAPI is a server authoritative system, triggered from UnityEngine.Networking authority host has the authority over all non-player GameObjects; Player GameObjects are a special case and treated as having “local authority”. local/client authority for npcmethod1: spawn the npc using NetworkServer.SpawnWithClientAuthoritymethod2: NetworkIdentity.AssignClientAuthority network context propertiesisServer()isClient()sLOcalPlayer()hasAuthority() networked GameObjectsmultiplayer games typically built using Scenes that contain a mix of networked GOs and regular GOs. networked GOs needs t obe synchronized across all users; non-networked GOs are either static obstacles or GOs don&apos;t need to synchronized across players networked GO is one which has a NetworkIdentiy component. beyond that, you need define what to syncronize. e.g. transform ,variables .. player GO NetworkBehavior class has a property: isLocalPlayer, each client player GO.isLocalPlayer == true, and invoke OnStartLOcalPlayer() Player GOs represent the player on the server, and has the ability to run comands from the player’s client. spawning GOsthe Network Manager can only spawn and synchronize GOs from registered prefabs, and these prefabs must have a NetworkIdentity component spawning GOs with client authorityNetworkServer.SpawnWithClientAuthority(go, NetworkConnection),for these objects, hasAuthority is true on this client and OnStartAuthority() is called on this client. Spawned with client authority must have LocalPlayerAuthority set to NetworkIdentity, state synchronization[SyncVars] synchronzed from server to client; if opposite, use [Commands] the state of SyncVars is applied to GO on clients before OnStartClient() called. engine and editor integration NetworkIdentity component for networked objects NetworkBehaviour for networked scripts configurable automatic synchronization of object transforms automatic snyc var build-in Internet servicesnetwork visibilityrelates to whether data should or not sent about the GOs to a particulr clinet. method1: add Network Proximity Checker component to networked GOmethod2: Scene GOs saved as part of a Scene, no runtime spawn actions and communicationmethod1: remote actions, call a method across networkmethod2: networking manager/behavior callbacksmethod3: LL network messages host migrationhost: a player whose game is acting as a server and a “local client”remote client: all other playersso when the host left, host need migrate to one remote client to keep the game alive how it works: enable host migration. so Unity will distribute the address of all peers to other peers. so when host left, one peer was selected to be the new host (heart-keeping) network discoveryallow players to find each other on a local area network(LAN) need component , in server mode, the Network Discovery sends broadcast message over the network o nthe specified port in Inspector; in client mode, the component listens for broadcast message on the specified port using transport Layer API (LL API)socket-based networking in the endnetwork is basic cornerstone in server-client applications, Unet is deprecated at this moment, but still many good network resource can take in charge. e.g. websockets, nodejs, and what behind these network protocol or languages, e.g. async/coroutines are charming as well.]]></content>
      <tags>
        <tag>unity3D</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python generator and asynico]]></title>
    <url>%2F2019%2F08%2F16%2Fpython-generator-and-asynico%2F</url>
    <content type="text"><![CDATA[python iteratorin python, iterator is any object that follows iterator protocol. which means should include method: __iter()__, next(), if no next element should raise StopIteration exception. take an example, dict and list have implemented __iter()__ and __getitem__() methods, but not implemented next() method. so they are iterable but not iterator. generatorto support delay operations, only return result when need, but not immediately return generator funcion (with yield) generator expression Python generator in other languages is called coroutines yieldyield is almost of return, but it pauses every step after executing the line with yield, and till call next() will continue from the next line of yield. 12345678while True: sim.run(timeout, cb)def cb(): a = 1 yield print(a) a += 1 coroutinesexample: 12345678def cor1(name): print("start cor1..name..", name) x = yield name print("send value", x)cor_ = cor1("zj")print("next return", next(cor_))print("send return", cor_.send(6)) to run coroutine, need first call next(), then send() is called. namely, if not call next() first, send() will wait, and never be called. the thing is, when define a python generator/coroutine, it never will run; only through await(), next() call first, which then trigger the generator/coroutine start. asyncioasyncronized IO, event-driven coroutine, so users can add async/await to time-consuming IO. event-loop event loop will always run, track events and enqueue them, when idle dequeue event and call event-handling() to deal with it. asyncio.Task awaitawait only decorate async/coroutine, which is a waitable object, it works to hold the current coroutine(async func A) and wait the other coroutine(async func B) to the end. 123456789async def funcB(): return 1async def funcA(): result = await funcB() return resultrun(funcA()) multi-task coroutines1234567891011121314151617181920212223242526272829303132333435363738loop.create_task()run_until_complete() #block the thread untill coroutine completedasyncio.sleep() #nonblock event-loop, with `await`, will return the control-priority to event-loop, at the end of sleep, control-priority will back to this coroutine asyncio.wait(), #nonblock event-loop, immeditialy return coroutine object, and add this corutine object to event-loop``` the following example: get the event-loop thread, add coroutine objct(task) in this event-loop, execute task till end.```pythonimport asyncioasync def cor1(name): print("executing: ", name) await asyncio.sleep(1) print("executed: ", name)loop = asyncio.get_event_loop()tasks = [cor1("zj_" + str(i)) for i in range(3)]wait_cor = asyncio.wait(tasks)loop.run_until_complete(wait_cor)loop.close()``` ### dynamically add coroutine```shell loop.call_soon_threadsafe() # add coroutines sequencially asyncio.run_coroutine_threadsafe() #add coroutines async add coroutine sequenciallyin this sample, main_thread(_loop) will sequencely run from begining to end, during running, there are two coroutines registered, when thread-safe, these two coroutines will be executed. the whole process actually looks like run in sequencialy 123456789101112131415161718192021222324252627import asynciofrom threading import Threaddef start_loop(loop): asyncio.set_event_loop(loop) loop.run_forever()def thread_(name): print("executing name:", name) return "return nam:" + name_loop = asyncio.new_event_loop()t = Thread(target=start_loop, args=(_loop,)) #is a thread or coroutine ?t.start()handle = _loop.call_soon_threadsafe(thread_, "zj")handle.cancel()_loop.call_soon_threadsafe(thread_, "zj2")print("main thread non blocking...")_loop.call_soon_threadsafe(thread_, "zj3")print("main thread on going...") add coroutines asyncin this way, add/register async coroutine objects to the event-loop and execute the coroutines when thead-safe 1234567891011future = asyncio.run_coroutine_threadsafe(thread_("zj"), _loop)print(future.result())asyncio.run_coroutine_threadsafe(thread_("zj2"), _loop)print("main thread non blocking...")asyncio.run_coroutine_threadsafe(thread_("zj3"), _loop)print("main thread on going...") async callback vs coroutinescompare callback and coroutines is hot topic in networking and web-js env.]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dns server in lan]]></title>
    <url>%2F2019%2F08%2F10%2Fdns-server-in-lan%2F</url>
    <content type="text"><![CDATA[DNS server in LANto use domain name(e.g. gitlab.com) in LAN rather than IP, it needs every local host machine to store all key-values:: host-IP. if the LAN has many host machines, it will be difficult to maintain. Setting up DNS server will help to automatically map the ip to domain or reverse in the LAN. bind912345678910111213141516171819202122232425262728293031323334353637383940apt-get install bind9 ``` ### /etc/bind/named.conf.local```shellzone "gitlab.com" &#123; type: master; file "/etc/bind/db.ip2gitlab.com" ;&#125;; zone "101.20.10.in-addr.arpa" &#123; type: master; file "/etc/bind/db.gitlab2ip.com" ;&#125;;``` ### /etc/bind/db.gitlab2ip.com [dns zone file format](https://help.dyn.com/how-to-format-a-zone-file/)gitlab2ip zone file is mapping from domain to ip, as following sample, it works like: www.$ORIGIN --&gt; 10.20.101.119 ```shell; command$TTL 6000;@ refer to current zone file; DNS-server-FDNQ notification-email$ORIGIN gitlab.com@ IN SOA server email ( 2 ; 1d ; 1h ; 5min ; )@ IN NS serverwww IN A 10.20.101.119server IN A 10.20.101.119 /etc/bind/db.ip2gitlab.comip2gitlab zone file is from ip to domain mapping, 1234567891011$TTL 6000$ORIGIN 101.20.10.in-addr.arpa@ IN SOA server. email. ( 2 ; 1d ; 1h ; 5min ; )@ IN NS server119 IN A www.gitlab.com119 IN A server.gitlab.com nslookupnslookup www.gitlab.com #dns forward (domain 2 ip) nslookup 10.20.101.119 #reverse (ip 2 domain) settingsif the DNS setted above(DNS-git) is the only DNS server in the LAN, then this DNS works like a gateway, to communicate by domain name, every local host talk to it first, to understand the domain name. but in a large size company LAN newtwork, there may already has a DNS server hosted at IT department (DNS-IT), with a fixed IP e.g. 10.10.101.101, and all localhost machines actually set DNS-IT as the default DNS. DNS-git will work as a sub-DNS server. Inside the small team, either every localhost change default DNS to DNS-git, then DNS-git become the sub-network server. if every localhost still keep DNS-IT, there is no way(?) to use DNS-git service in LAN, and even make conflicts, as the DNS-git localhost machine will listen on all TCP/IP ports, every new gitlab.com access request (input as IP address) will get an output as domain name, but the others can’t understand this domain-name… what happened with two DNS server in LAN ? how email worksMail User Agent(MUA), e.g. Outlook, Foxmail, used to receive and send emails. MUA is not directly sent emails to end users, but through Mail Transfer Agent(MTA), e.g. SendMail, Postfix. an email sent out from MUA will go through one or more MTA, finally reach to Mail Delivery Agent(MDA), the email then store in some database, e.g. mailbox the receiver then use MUA to review the email in the mailbox ps, one day work as a IT admin ….]]></content>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[design scenarios in ADS simulation]]></title>
    <url>%2F2019%2F07%2F25%2Fdesign-scenarios-in-ADS-simulation%2F</url>
    <content type="text"><![CDATA[to design scenarios in self-driving test, one reference is: a framework for automated driving system testable cases and sceanrios, most other scenario classifications have almost the same elements: ODD, e.g. road types, road surfaces; OEDR, namely object and event detection and response, e.g. static obstacles, other road actors, traffic signature, environment conditions, special zones; and failure mode behaviors. in general, test cases can be grouped as black box test, in which the scenario parterners’ behavior is not pre-defined or unpredictable, e.g. random traffic flow(npcs) scenario, or white box test, where the npcs behavior is pre-defined, e.g. following user-define routings. white box testing is helpful to support performance metrics; while black-box testing is helpful to verify the system completeness and robust. as for ADS test, there are a few chanllenges coming from: heuristics decision-making algorithms, deep-learning algorithms, which is not mathematically completed the test case completeness, as the number of tests required to achieve statistically significant to claim safe would be staggering undefined conditions or assumptions a sample test scenario set maybe looks like: ODD OEDR-obj OEDR-loc maneuver in rump static obstacles in front of current lane in rump static obstacles in front of targe lane in rump dynamic obstacles in front of current lane scenarios_runnercarla has offered an scenario engine, which is helpful to define scenarios by test_criteria and behaviors, of course the timeout as well. test_criteria, is like an underline boundary for the scenario to keep, if not, the scenario failed. e.g. max_speed_limitation, collision e.t.c. these are test criterias, no matter simualtion test or physical test, that have to follow the same criterias; in old ways, we always try to find a way to evaluate the simulation result, and thought this may be very complex, but as no clue to go complex further, simple criterias actually is good. even for reinforcement learning, the simple criterias is good enough for the agent to learn the drive policy. of course, I can expect there are some expert system about test criterias. For simulation itself, there has another metric to descibe how close the simulation itself to physical world, namley to performance how well the simulator is, which is beyond here. behaivor is a good way to describe the dynamic processing in the scenario. e.g. npc follow lane to next intersection, and stop in right lane, ego follow npc till the stop area. OpenScenario has similar ideas to descibe the dynamic. to support behaivor, the simulator should have the features to control ego and npc with the atomic behaviors, e.g. lane-follow, lane-change, stop at intersection e.t.c. in lg simulator, npc has simple AI routing and lane-following API, basically is limited to follow pre-defined behaviors; ego has only cruise-control, but external planner is avialable through ROS. for both test_criteria and behavior, carla has a few existing atomic elements, which is a good idea to build up complex scenarios. OpenScenarionot sure if this is a project still on-going, carla has interface and a few other open source parsers there, but as it is a standard in popular projects, e.g. PEGUS, should be worthy to take further study.]]></content>
      <tags>
        <tag>simulation</tag>
        <tag>lg-sim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[model based design sucks]]></title>
    <url>%2F2019%2F07%2F18%2Fmodel-based-design-sucks%2F</url>
    <content type="text"><![CDATA[AV development includes perception, sensor fusion, location &amp; mapping, decision-making &amp; control (or motion planning), embedded, simulation and maybe many system-glue software tools. L3 planning &amp; control is now expert-based decision system, which basically defines rules to make decision, where model based design(mbd) is a helper. Waymo mentioned their hybrid decision-making solution, basically Machine Learning(ML) will take a big part of the situations, but still space to allow rule-based solution to take priority. when consider to ML decision making, mdb will become less useful. why model-basedTraditional OEMs follow vehicle-level safety requirements(ASIL-D) to develop vehicle products and components, usually can be represented as the V style development, from user requirs, system design, implmenent to test verification. to go through the whole V process take a rather long time, e.g. for a new vehicle model, it means 2~5 years. Commercial hardware and software(mobile apps) products which has lower level safety requirements, however, can iterate in a quicker frequency. the safety requirements drive the product development in a very different way, compared to common Internet products, which include more straight-forward programming skills and software architecture mindset. but to satisfy the additional, or should say the priority safety requirements, how to organize the code is less important than how to verify the functions is to satisfy the safety. so there comes the model-based design, the most-highly feature of which is to support system test and verify at pre-product period. of course, model-based design should be easily to build up prototype and visualize the system, which is the second feature of mbd, working similar like a microsoft vision e.t.c thirdly, from design to product, is auto code generation. which means once the design is verified, you don’t need to go back to write code again, but directly generate code from the design graph. model-based design toolchain is already a whole eco-system, e.g. system design, auto code generator, test tools. and all these tools should be first verified by ASIL-D standard. Internet AI companies once thought it would be easy to take over this traditional development by Internet agile development, while the reality is they still depends on model-based design at first to verify the system, then back to implement code again, which should be more optimized than auto-generated ones, which is one drawbacks of mbd, as mbd is tool-depended, e.g. Matlab, if Matlab doesn’t support some most updated libs, then they are not in the auto-code, and most time Matlab is far behind the stable version of libs outside. what mbd can’t dombd is born to satisfy safety requirments in product development. so any non safety required product won’t use mbd. and by nature, mbd is good at turning mathematical expressions to system languages, and logical relations to state flows, so any non-articulatable system is difficult to represent in mdb languages. in vehicle product development, engine, powertrain, ECU, brake system, ADAS, L3 motion planning, e.t.c have depends heavily on mbd. but also we can predict, L3+ applications arise, with image, cloud point based object detection, data fusion, SLAM, AI-driven planning, IVI, V2X, will hybrid mbd with many Internet code style. industry experience: a metaphysicssome friends say mass-product-experience makes him more value than new birds. since industry experience is not transparent, as there is a no clear bar to test the ability/value of the enginer, unlike developers, who can valued by their product, or skills, also the same reason make these guys who stay long in the industry sounds more valued, and they have more likey went through one or many mass product experience. but at most, industry product depends e.g. vehicle, on teamwork, even the team lead can’t make it by himself, unlike developer, a top developer can make a huge difference, much valued than a team of ordinary ones.]]></content>
  </entry>
  <entry>
    <title><![CDATA[autosar sucks]]></title>
    <url>%2F2019%2F07%2F15%2Fautosar-sucks%2F</url>
    <content type="text"><![CDATA[what is AUTOSARbasically it’s an micro-service architecture for vehicle EE system. Each micro-service is called software component(swc), and it has uniform interfaces, while of which the implementation is varied. as the goal of AUTOSAR said: share on the standard (interface), compete in the implementation. the inter-connect of micro-services is through virtual function bus(vfb), which works as a gateway, guiding data flow from port A, from micro-serviceA to port B, from micro-serviceB the benefits of AUTOSAR is obvious, to design the interface at system level first, if any changed need, it can be updated quickly. after the system architecture is fixed, then go to the implementation details. refer input description software components(micro-services) description, only define the data flow, interface functions system, system topology(interconnection among ECUs, and available data buses, protocols etc) hardware, the available hardware(processors, sensors, actuators etc) system configurationused to distributes the software component descritpions to different ECU ECU configurationthe basic software(BST) and run-time environment(rte) of each ECU has been configured, this is based on the dedication of the application software components to each ECU. generation of executablesin this step to implement the software components, then build. this can be automated done by tool-chains. all steps up to now are supported by defining exchange formats(xml) and work methods. basic softwareeach swc has well-defined ports, either provider port(PPort) or request port(RPort), the swc interface can either be a client-server interface or sender-receiver interface. with a PPort, the swc will impelment data generation; with a RPort, the swc will implement data read. communication manager (ComM), is a resource mananger to encapsulates communication related basic software modules. the actual bus states are controlled by the corresponding bus state manager, e.g. CAN/FlexRay/Lin bus. when ComM request a specific commmunication mode from the state manager, it will map the communication mode to a special bus state. network management modules (NM) works in bus-sleep mode and only support broadcast communication. diagnostic communication manager(DCM), a common API for diagnostic services. CAN driver performs the hardware access and provides a hardware-independent API to upper layers; it can access hardware resources and converts the given information for transmission into a hardware specic format and triggers the transmission. runtime environment(RTE)between basic software to upper application softwares, I think it’s mostly vfb. application software componentsfor now, e.g. ADAS, traditional EE.]]></content>
      <tags>
        <tag>AUTOSAR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[play with ros]]></title>
    <url>%2F2019%2F07%2F13%2Fplay-with-ros%2F</url>
    <content type="text"><![CDATA[ros filesystem toolsfirst check ROS_PACKAGE_PATH, where defines all ROS packages that are within the directories. 1234567891011rospack find [package-name]rospack list roscd [package-name]``` take an example, to locate `rosbridge_websocket.launch` ```shell rospack find rosbridge* #rosbridge_serverroscd rosbridge_servercd launch another tool to view ros-launch: roslaunch-logs write a .launch filelaunch files, which uses XML format, usually make a directory named “launch” inside the workspace to organize all launch files, and it provides a convenient way to start up multiple nodes and master, it processs in a depth-first tarversal order. usually launch files can be put a launch folder under a ros node project. 123roslaunch package_name launch_file#or roslaunch /path/to/launch_file an sample launch file: 123&lt;launch&gt; &lt;node pkg="package_name" type=" " name=" " output=" " args=" " /&gt;&lt;/launch&gt; args can define either env variables or a command. node/type There must be a corresponding executable with the same name. rvizrviz can help to playback sensor rosbag at lab. and also the visualization tool in algorithm/simulation development. someone(at 2014) said Google’s self-driving simulation has used rviz: a sample with rviz to visualize rosbag info: 12345# terminal 1 roscore # terminal 2 rosbag play kitti.bag -l rosrun rviz rviz -f kitti-velodyne rosbagthe sensor ros node will collect data in rosbag during physical or vitual test, then playback rosbag to develop or verify the sensing algorithms. or use to build simulation scene. a few common commands, and also rosbag support interactive C++/Python APIs. 1234rosbag record #use to write a bag file wit contents on the specified topicsrosbag info #display the contents of bag files rosbag play #play back bag file in a time-synchronized fashion kitti datasetare we ready for autonoous driving? – the KITTI vision benchmark suite, which is a famous test dataset in self-driving sensing, prediction also mapping and SLAM algorithm development. there are a few benchmars includes: stereo, basically rebuild the 3D object from multi 2D images. optical flow, used to detect object movement(speed, direction) scene flow, include other 3D env info, and objects from optical flow depth visual odometry object detection object tracking road/lane detection semantic evaluation catkin packagecatkin is ros package build/manage tool. mkdir -p ~/catkin_ws/src cd ~/catkin_ws/src catkin_create_pgk demo std_msgs rviz cd demo mkdir launch cat "&lt;launch&gt; &lt;node name="demo" type="rviz" -d="ls `pwd`" /&gt; &lt;/launch&gt; " &gt; demo.launch cd ~/catkin_ws catkin_make --pkg demo # add catkin_ws to cat "export ROS_PACKAGE_PATH=$ROS_PACKAGE_PATH:/path/to/catkin_ws/" &gt;&gt; ~/.bashrc]]></content>
      <tags>
        <tag>ros</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[play with Docker swarm/compose]]></title>
    <url>%2F2019%2F07%2F12%2Fplay-with-Docker-swarm-compose%2F</url>
    <content type="text"><![CDATA[Docker swarmdocker swarm is Docker nature cluster manager, with built-in DNS service found mechanism, and load-balancing mechanism. compare to k8s, is a light-weight and easy goers. create a swarm cluster12345678910111213141516export MASTER_IP=192.168.0.1docker swarm init --advertise-addr $&#123;MASTER_IP&#125; --name masterdocker swarm join --token tokens $&#123;MASTER_IP&#125; --name worker1 docker node ls # demote/ promote nodeID as managerdocker node demote/promote nodeID# rm node docker node rm worker1# stop swarm mode docker swarm leave create service123456789docker service create service-name # scale servicedocker service scael SERVICE=replicas docker service rm docker service inspect create overlay network12345docker network create --subnet=192.168.0.0/24 -d overlay ppss-netdocker network rmdocker network connect network-name docker-node in swarm mode, there are three network created by default: bridge0, the default network docker_gwbridge, local bridge used to connect containers hosted in the same host ingress, is a overlay network used in the swarm cluster however, in swarm mode, the default network for service is bridge, to across physical host, services need go through overlay network. load balancingIngress load balancingexpose Docker service to external network env Internal load balancingswarm mode has build-in DNS Docker composedocker compose is a manage/build tool to create application, which combine a bunch of micro-services, each of which can be ran as a Docker container. the docker-compose.yml configure file has to include each micro-service Dockerfile, and the application running scripts. service startup orderthe services defined in docker-compose.yml is not necessary depended to each other, so each serice can up individually, but of course they can has based on each other. docker-compose.ymlbest practice build path to Dockerfile, can be absolute path or relative (to .yml) path. Compose will buid the image based on contextsub-choice under build, point to the Dockerfile image the image will be used, if not locally, will pull from hub (vs Dockerfile) containe_name volumes path to attached volumes, in the format HOST:CONTAINER[:access mode] network_mode same as docker run --network init privileged command override launch command when service contianer start environment set env variables, in the format ENV:valueif only ENV, the value will be derived from host machine runtime: nvidia to suuport nvidia-docker e.g. 1234567nvsmi: image: ubuntu:16.04 runtime: nvidia environment: - NVIDIA VISIBLE DEVICES=all command: nvidia-smi stdin_open std io aviable tty virtual terminal sample yml from projecta sample yml for web app: 123456789services: web: build: . links: - "db: database" db: image: postgres a sample yml for general app CI: 12345678910111213141516171819202122232425262728services: build: build: context: ./Dockerfile image: docker-image container_name: build_app volumes: - ./build_scripts: /root/build commands: /root/build/build.sh run: context: ./Dockerfile image: docker-iamge container_name: run_app volumes: - ./run_scripts:/root/run environments: -DISPLAY -ROS_MASTER network_mode: host runtime: nvidia command: /root/run/run.sh test: #TODO docker-machinedocker-machine is a tool to build virtial host when hosted in one physical host or among multi-physical hosts. ros-dockerin self-driving software stack, ros is often used. and there is a need to deploy ros in docker. next time.]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[play with Docker]]></title>
    <url>%2F2019%2F07%2F12%2Fplay-with-Docker%2F</url>
    <content type="text"><![CDATA[Docker networking bridge: the default network driver, only in standalone containers; best when have multiple containers to communicate on the same host machine. host: for standalone containers, the Docker host use the host machine’s networking directly; best when need no isolated from the host machine. overlay: connect multi Docker containers, enable swarm services to communicate with each other, no OS-level routing; best when need containers running on different host machines to communicate. none: disable all networking the network base is open source project libnetwork containers can communicate through hostname, or through DNS(the now Docker engine has default built-in DNS server), but in early days, can use external dns, e.g. Blowb to host docker images, either pull to Docker Hub, or to create a private cloud by ownCloud, or docker save/export tools: 1234docker save -o /path/to/save/file image | gzipscp *.tar.gz remote_user@remote_hostnamedocker load *.tar.gz running GUI apps in Dockerthere is a benchmark GUI/openGL test in Linux glxgears. to test the host machine support ssh or container based apps, we can first do the following test: 1234567891011121314151617181920sudo apt-get install mesa-utilsglxinfo glxgears``` Docker by default is bash/text based, but `nvidia-docker` is a gui-supported Docker engine, which requires Nvidia OpenGL drivers and Nvidia Gpus of course. since the gpu hardware version and the docker engine version, please check the compatability at first. ## remote hosted apps * configure master and worker nodes communication by setting IP address in the same domain, and setting the master node IP address as the gateway IP address for all worker nodes, basically the master node will work as the swticher.* install xserver-common, xserver-utils, as Ubuntu by deafult doesn't have X-server. ```shellmaster:~/ ssh -X user@workerworker:~/ DISPLAY=:0worker:~/ ./gui_app for docker containers, there is also authority property need take care. Dockfilewhen the Docker container starts, usually we want to auto start a shell process, so by CMD or ENTRYPOINT defined in the Dockfile. for the base images, e.g. Ubuntu, busybox, are used as the base for upper applications, usually will use CMD at the end of Dockerfile; but if the Docker container is specially for a certain application, it’s usually using ENTRYPOINT. the last line of Ubuntu 16.04 Dockerfile: CMD &apos;/bin/bash&apos; usually in one Dockerfile, there is only one CMD or ENTRYPOINT, and it’s better written in exec format: CMD [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] since in shell format: CMD exectuable, param1, param2 Docker will trigger /bin/sh first by default, if not define a shell. and this shell is always the first process in this Docker container, which sometimes is not what we expected. when using both CMD and ENTRYPOINT in one Dockfile, the output will looks like append/pipe CMD command after ENTRYPOINT command. a sample of Dockerfile: 123456789FROM &lt;image&gt;:[&lt;tag&gt;] [AS &lt;name&gt;]ADD [--chown=&lt;user&gt;:&lt;group&gt;] &lt;src&gt; ... &lt;dest&gt;ADD [" ", ... " "]COPY &lt;src&gt; ... &lt;dest&gt;RUN &lt;command&gt;VOLUME /mount/nameCMD ["executable", "param1", "param2"]CMD command par1 par2 from docker image to dockerfilewe can easily pull images from hub, but when we try to build some images directly, there is also way to get Dockfile from existing docker image: dfimage 1sudo docker pull chenzj/dfimage mount host volume to containereither we can define in Dockerfile by VOLUME, which is create a volume name in the base image, or during runtime, docker run -v /host/volume/path:/container/volume/path, to bind a host volume to current container.]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[where are you in next 5 years 8]]></title>
    <url>%2F2019%2F07%2F09%2Fwhere-are-you-in-next-5-years-8%2F</url>
    <content type="text"><![CDATA[工薪和自由职业者/创业/老板，退休以后会有什么不同。也许对于工薪阶层，退休的生活也会如上班的：说不上的无奈感，没有痛快酣畅的体验过人生 — 离开了根本不享受的工作，也谈不上享受生活。就像《肖生克的救赎》被放出去的老头，离开了监狱，也融不进社会了。 有个表哥创业了，做了一个健身品牌，小有所成，进入持续创业，第二份是乡政企业办公软件。两份业很不一样。甚至表哥说，这份业估计一年后就结束，生活还在继续，下一份业在哪里，现在根本想不到，但是也不担心没有。 认识一个姑娘，88年，澳洲留学读了两年mba就12年回国，用我的眼光看，之后就没正紧上过班，走走玩玩也在这些年学了瑜伽，今年(2019年)开了一家店，32岁了过的跟20岁出头的姑娘一样，到底是没心没肺，还是把生活过成了别人羡慕的样子。她并不符合我的价值观，但是难道这样的人生不值得吗？ 年初在深圳，被这里的年轻人着实震惊了。大公司（华为）加班到凌晨，年轻的生命就像路旁的热带植被在绽放和燃烧。只是大表哥说了句，是被洗脑了。 老妈在好几个城市工作做，做保姆、帮厨、家政，走到哪里都得到顾客的喜欢，走到哪里都可以有饭吃，根本不担心没有技能，找不到工作。像一个自由职业者。 打工和创业当老板的mindset，似乎是本质的不同。 创业者/自由职业者，总会有出路，生活处处都通达。 对比下，打工者的心态。就是处处被堵，操的心一点不少，把脑袋削尖了跟黑压压的长江后浪推前浪的年轻人比拼，担心技术上比不过行家，担心项目被各种原因取消了，担心行业遇冷，担心被老板穿小鞋，担心30岁还没有不可替代的核心，担心35岁要开始讨生活了，担心工资比年轻人高容易被开，也开始担心身体健康、家人健康等等。 采取的解决办法也是围绕着这些压力了，人生没有朝向，谈不上洒脱和享受。 生活不止眼前的苟且，打工者真是委屈了心，把路走窄了，反而觉得这是唯一的出路。打工解决不了焦虑，必须转变。这世上，除了生死，都是小事。所以无所谓待业；找不到真正实现价值的事业，宁可像无业游民一样活着。 民企打工没有完善的制度，好的方面就是可以立山头，只要说动了领导，技术上可以大胆尝试，当然并不一定能得到支撑。 没有稳定的做产品的氛围，所以即便立了项目，也不一定能看到项目落地。对产品开发人员，就是不利于积淀。 这样的氛围下，见风使舵就是生存哲学。 可以联想到更广大的中小民企，更缺乏完善的产品流程和考评体系，老板一个人的话语权太大，打工者基本没有话语权。 出来自己干的人，哪些不同的品质？首先，压力的来源不该是跟成千上万人挤独木桥。一切可以明确定义的职位，比如，程序员、会计、工程师、个体网商，都挤满了人。而一旦陷入了这种思维，思考就会局限在削尖脑袋挤到这个行业/职位的头部。付出的代价/成本非常不成比例。简言之，就是洗脑了。 打工只适合初期的资本积累。所以，选择做舞蹈老师、瑜伽教练，咖啡馆老板的，慢慢都会生活和工作双赢，退休了也不愁不知道怎么经营生活。而单一大公司打工的，可能初期会在工作上得意，但是慢慢的生活和工作都会失去，而且退休了根本不会打理生活。 这些愿意出来自己干的人，更珍惜生活吧。所以不能接受将精力埋没在日复一日的工作中。这些自己出来干的人，都是被逼，当初没机会选择有保险的工作，只能自己走出一条道儿来。 刻意要避免打工，估计也是自找麻烦。所以平常心。 做技术的氛围到底什么环境/氛围适合做技术？北美的工作环境，相比国内，算是无忧无虑了，虽然有讨厌的印度人，但是如果有心总是有钱有时间捣鼓技术。不过反而，普通人在这样的环境下，是没有表现很强的科研热情。国内相比，待遇，工作的可爱度低，周围的大牛少了，但是反倒人因为生活和人的竞争感，反而会想多学点。 另一方面，国内的资本家似乎更缺乏对行业的敬畏心，资本家对这个行业就是个格外挣钱工具的心理，当然不会真正给这个行业带来真正伟大的技术推动。比如，当宝能系，恒大都砸出一叠钱说要造车，网上大张旗鼓的招聘，按照自动驾驶的各个模块：感知算法，运动规划，决策控制，地图定位开始招人的时候。一方面是哭笑不得，一方面是无奈，觉得身为工程师只是个棋子罢了，被一个工具/算法/模块给定义了。 相比这些只有钱的资本家的嘴脸，虽然汽车厂背后也站着资本家，似乎对汽车行业本身也更关心。当然，在中国，总是要面对钱，落地的现实。美国人可以谈vision, 3，5年不出产品，中国的资本市场基本不允许出现。所以，即使在国内的车厂做技术/研究，也是被量产推着。所以，整体氛围是浮躁，也就没办法专心下去]]></content>
  </entry>
  <entry>
    <title><![CDATA[computing chips in AV]]></title>
    <url>%2F2019%2F06%2F23%2Fcomputing-chips-in-AV%2F</url>
    <content type="text"><![CDATA[chips requirements in vehicleBosch : BMW: the next-generation vehicle EE platform can be easily modulized based on the topology of network composed by domain controllers and in-vehicle Ethernets. take an example with Singulato iS6, which has five domains: smart driving, powertrain, chassis, smart body, smart seat. each domain need support by a domain controller unit(DCU), the core of which is a powerful computing chip, which is usually more powerful than traditional ECUs. in average, L2 requires computing power about 10TOPS , L3 needs 60TOPS, L4 needs 100TOPS. computing chips product MDC600 Driver PX Pegasus EyeQ 4 BlueBox R-car H3 Journey2.0 Huawei Nvidia Mobileye NXP Renesas Horizon TOPS 352 320 2.5 10 main cores 8 * 晟腾310 16 ARM VMP LS2084A 4*Arm/A57 BPU2.0 other cores Ascend 310 2 TensorCore GPU S32V234 4*Arm/A53 FPGA AV-level L3+ L5 L3 L4(target) IVI L3 Camera support 16 10 8 8 8 4 Lidar support 8 6 function safety ASIL-D ASIL-B ASIL-D ASIL-D(target) ASIL-B ASIL-B products timelineTier1s/Tier2s timeline 2018 2019 2020 2021 Aptive level3 level4 Bosch level2 level3 level4 Conti level2 level3+ Autoliv level2 level3 level4 Intel level3 level4+ Nvidia level3 level4+ Chinese OEMs timeline 2019 2020 2021 2022+ changan level4 FAW level4 GAC level3 level5 Geely level3 level5 GWM level3 level4 SAIC level3 xiaoPeng level3 WeiMa level3 Nio level2 level4+ global OEMs timeline 2018 2019 2020 2021 2022+ Ford level2 level4 GM level2 level4 Fiat-Crysler level3+ Audi level2 level4 Mercedze level2 level3 level4 Toyota level2 level3 level4 Honda level3 level4 referenceMatrix 1.0 the five chip vendors from GPU to ASIC hauwei and the others global chips vendors L4 AI chips the evolution of EyeQ Mobileye tech NXP bluebox R-car H3 soc NXP function safety horizontal AI matrix2.0 cars, mobility, chip-to-city design and the Iphone4 2018-2019 汽车域控制器产业研究报告 汽车电子演化 Global L3 self-driving vehicle market insights 2019 self-driving car research report]]></content>
  </entry>
  <entry>
    <title><![CDATA[Lidar in AV]]></title>
    <url>%2F2019%2F06%2F22%2FLidar-in-AV%2F</url>
    <content type="text"><![CDATA[science, religion, music, universe as well as other sources of beauty, are what we humans should look for. – zj operational theorya pulse of light is emitted and the precise time is recorded. the reflection of that pulse is detected and the precise time is recorded. using the constant speed of light and the delay can convert into distance, with the known position and orientation of the sensor, the xyz position of the reflective surface can be calculated. components laser scanner/emitter and laser detector high-precision clock GPS and GPS ground station record xyz of the scanner IMU record angular orientation of the scanner field of view(FOV)azimuth with fixed vertical angle resolution, the neighboring laser emmiter-detector pair will create concentric circle, the distance between two neighboring concentric circle will grow with the distance from detected objects to Lidar. light source950nm wavelength producer is Si-based, which makes it cheaper than 1550nm, the InGaAs based, making it safer to human eyes as 950nm can burn retina and powerful. 1550nm is easier to be absorbed by water than 950nm, which makes it performance better in rainy days. the laser source emiss lines of pulse every frame, and a few photon return back to Photodetector(光电探测器), there are lots of env photons(noise), we can use narrow-band-filter to tick off some env photons, but not all of them, since the solar radiation is in the range from 905nm 50 1550nm. solid-statethere are two ways ongoing: MEMS based, phased array tech(相位阵列）. MEMS tech is using a micro scaning mirror, either rotate or vibrate to control laser direction. the drawback of micro-mirror is the in the process of relection, lots of laser energy is lost. phased array tech(Quanergy) integerated multi micro laser emission into one socket to control laser direction. and the drawback at this moment is the short detection distance. the traditional mechanical design Lidar(Velodye) usually has multi light emitters as well as multi corresponded light detectors. while SS-Lidar depends only on one single light emitter and the scanning mirror to control emission direction, which makes it cheaper. for example, each pair of mechanical emitter-detector cost 200 us dollar, so a 64 lines product will cost about 12800 us dollar, compared to MEMS socket about 200 us dollar each. detection distance(dd) &amp; angle resolution(ar)detection distacne with 10 % reflectivity vertical angle range(var) vertical angle resolution(va_res) company product dd(m) var va_res channels Hesai Pandar40 200 23&deg; 0.33&deg; 40 Robo sense RS-Lidar-32 200 40&deg; 0.33&deg; 32 Velodyne HDL-32e 100 41.3&deg; 32 Quanergy M8 150 20&deg; 32 Ibeo NSH_32 80 16&deg; 0.2&deg; 32 InnoVusion Cheetah 200 40&deg; 0.13&deg; 300 env effectswhat about weather effects? e.g. snow, dust, rain; what about env effects? e.g. temperature, system vibration. fusion with camerathe speed of productivizationwhen I first heard about InnoVusion, the founder Bao Junwei who was working at Baidu AI, then had the idea to produce Lidar around 2015, then he left Baidu and started InnoVusion, at the end of 2016, their first product Cheetah was born. this process is really speedy, one thought is the drive force either by capital market or industry needs is becoming so fast that every good chance from idea to product is becoming shorter in time; the other thought, only these highly effective persons will survive in this fast-iteration world. some other founders stories are here : the AI masters who left from Baidu referenceIbeo Next 3D SS-Lidar Innovusion Cheetah Lidar Velodyne HDL_32e product manual]]></content>
  </entry>
  <entry>
    <title><![CDATA[principles of GNSS positioning]]></title>
    <url>%2F2019%2F06%2F17%2Fprinciples-of-GNSS-positioning%2F</url>
    <content type="text"><![CDATA[novatel introduction GNSS architecturea) space segment the GNSS satellites, each of which broadcasts a signal that identifies ti and provides its time, orbit and status. b) control segment a ground-based network of master control stations, data uploading stations adn monitor stations. in case of GPS, 2 master control stations(one primary and one backup), 4 data uploading stations, and 16 monitor stations c) user segment the user equipment that process the received signals. GNSS propagationthe layer of atmoshpere that most influcences the transmission of GPS signals is the ionosphere(电离层), ionoshperic delays are frequency dependent; and the other layer is troposphere(平流层), whose delay is a function of local temperature, pressure and relative humidity. some singal energy is reflected on the way to the receiver, called “multipath propagation”. Antennaeach GNSS constellation has its own signal frequencies and bandwidths, an antenan must cover the signal frequencies and bandwidth. antenna gain is defined as the relative measure of an antenna’s ability to direct or concentrate radio frequency energy in a particular direction or pattern. A minimum gain is required to achieve a minimum carrier : power-noise-ratio to track GNSS satellites. GNSS error sourcescontributing sources error range satellite clocks +- 2m orbit errors +-2.5m inospheric delays +-5m tropospheric delays +-0.5m receiver noies +-0.3m multi path +-1m Resolving errorsmulti-constellation &amp; multi-frequencymulti-frequency is the most effective way to remove ionospheric error, by comparing the delays of two GNSS signals, L1 &amp; L2, the receiver can correct for the impact of ionospheric errors. multi-constellation has benefits: reduce signal acquisition time, improve position and time accuracy. D-GNSSin differential GNSS(D-GNSS), the position of a fixed GNSS receiver, refered as a base station, which sends the atmospheric delay related errors to receivers, which incorporate the corrections into their positoin calculations. differential positiong requires a data link betwen the base station and rovers, if corrections need to be applied in real-time. and D-GNSS works very well with base station-to-rover separations of up to 10km. Real time kinematic(RTK)it uses measurements of the phase of the signal’s carrier wave, in addition to the information content of the signal and relies on a single fixed reference station to provide real-time corrections, up to centimetre-level accuracy. the range to a satellite is calculated by multiplying the carrier wavelength times the number of whole cycles between the satellite and the rover and adding the phase difference. the results in an error equal to the error in the estimated number of cycles times the wavelength, so-called integer ambiguity search, which is 19cm for L1 signal. Precise Point Positioning(PPP)PPP solution depends on GNSS satellite clock and orbit corrections, generated from a network of global reference stations. GNSS + IMUthe external reference can quite effectively be provided by GNSS, and GNSS provides an absolute set of coordinates that can be used as the initial start point, as well, GNSS provides continuous positions adn velocities thereafter which are used to update the IMU/INS filter estimates. for additional combined sensors, such as odometers, cameras vision. challenges of GNSS in AVtalk from iMorpheus.ai 1) antenna 2) multipath mitigation 3) multi-band, multi-constellation signals 4) integrated navigation (camera )]]></content>
  </entry>
  <entry>
    <title><![CDATA[Manhanton SC review]]></title>
    <url>%2F2019%2F06%2F12%2FManhanton-SC-review%2F</url>
    <content type="text"><![CDATA[conjunctionsthe seven conjunctions can used to connect two independent clauses: For, And, Nor, But, Or, Yet, So comma only cant connect two sentences; but can connect two independent clauses using a semicolon(;) semicolon is often followed by a transition expression, (however, therefore, in addition), but these expression are not conjunctions, so must use semicolons, not commas to join. noun modifiersin the format: prepositon, part participle, presetn participle without commas. put the noun and its modifier as close together as possible “ comma which” is a nonessential modifier relative pronounswhich, cant modify people, who/ whom, must modify people whose, can modify either people or things where, can modify a noun place, can’t modify a metaphorical place, such as situation, case … when, can modify a noun event or time Noun Modifier markersany -ing that are not verbs and not separated from the rest of the sentence by comma will either be a noun, or a modifier of another noun. any “comma -ing” is adverbial modifiers adverbial modifierin the format of: prepositional phrase, present participle with commas , past participle with commas the adverbial modifier must modify a certain verb or clause at a right position, not structurally closer to another verb or clause participle modifierswhen using particples, the information present earlier in the sentence leads to or results in the information presented later in the sentence . subordinatorssubordernate clause provides additional info about the main clause. common subordinatetor markers: although, before, unless, because, that, so that, if, yet, after, while, since, when. and using only one connected word per “connection” . . which vs -ingwhenever using which, must refer to a noun, can’t modify a whole clause. so if need to modify the whole clause, use an adverbial modifier(either -ing, or past) quantitycountable modifiers: many, few, fewer, fewest, number of , numerous uncountable modifiers: much, little, less, least, amount of , great more, enough, all works with both countable and uncountable. parallelismcomparable sentence parts must be structurally and logically similar comparisoncomparison are a subset of parallelism, which requires parallelism between two elements, but also require the two compared items are fundamentally the same type of thing like, unlike, as, than, as adj as, different from, in contrast to/with like vs aslike is used to compare nouns, pronouns or noun phrase, never put a clause or prepositional phrase after like. as can used to compare two clauses.]]></content>
  </entry>
  <entry>
    <title><![CDATA[how simulation helps for autonomous vehicle]]></title>
    <url>%2F2019%2F06%2F06%2Fhow-simulation-helps-for-autonomous-vehicle%2F</url>
    <content type="text"><![CDATA[prefarerecently joined another Chinese auto OEM, still work in simulation platform for autonomous vehicle. this experience is a different pratice from GM or Ford. at first, it’s the mindset update, once for a long time, or even take for granted that I was looking for to be treated greatly, like in NA companies, what the company can offer me. it’s about freedom and responsibility. To have the freedom/right to make a difference in project direction, contents, and methods to achieve sth, first prove that I can take it, which is responsibility. “ you can you up” that kind of phylosphy is very common and actually I’d love it in some point. beta versionthere are ways to make a virtual simulator. traditional tool vendors include Matlab/Simulink, Prescan, VTD, ANSYS e.t.c. and as the beta version of simulator tool-chain, they are common in most OEMs in China. while have to say, in China, cause most software tools, from engineering tools to modern HR managment tools to product managment tools are kind of in practice process in Chinese companies as I can see, rather than as a matured segment pluged-in the company’s DNA. e.g. the product development tools are pretty in trial and error, occasionaly some PLM vendors come to introduce their products. and actually except the CAD/CAE tools, which were introduced in China since 80’s, and which sounds matured right now, the simulation tools for autonomous vehicle is pretty new-things, from map, sensor to all kinds of internal algorithms. the beta version of simulation tools has its DNAs as well. first they are charged by license fee, which is a pretty old way like IBM days. not many modern Internet mobility companies live in this way anymore. secondly, they can’t update frequently, once or twice a year with a new version of the product is common, which maybe OK in CAD/CAE tools, since the engineering analysis process are pretty matured and few exceptional requirements jump out in daily work, and there update is sometime by the vendor itself, which is not a required update, maybe optimized the algorithms, maybe added a third-party function, and sometime is from the users conference, usually the tool vendor will organize this kind of user conference to group users together, one way to get some connections, as well as to get some end-user requirements to fix the next version product, which is also a drawback compared to open source tool community, in where the user is more active. thirdly, the beta version tools are excluding the users someway, who can’t actually manage the tool as part of its whole functions. assuming simulation will be the key to make a difference in future auto product, then this excluding is unacceptable. on the other side, if autonomous simulation tool are at most aided to develop new product, then it becomes a piece of chicken neck either to take or to throw. from a product view, what is the role of simulation tool or as a product itself, where it is blooming point alpha versionwhy simulation tool become a role in autonomous, except the traditional vendors, mostly coming from Internet companies, e.g. Google, Uber, Baidu, Tecent etc. they are pretty strong to make a software product, including the simulation software in auto industry. interesting, these Internet companines never go to make a CAD/CAE software, but they actually prefer to support the cloud/HPC infrastructures for CAD/CAE simulation. these big mobility companines make their simulation tools now and well cowork with their existing IT development methodlogy, software product ideas as well as all ICT infrastures, which make these tools sounds huge great, and which also make themselves as the core role, rather than the OEMs, few are free or not yet open-sourced. and there are a few popular open source simulation tools, e.g. Carla, Airsim, LG simulator e.t.c. I actually get a chance with Carla at GM research team, which even now still great, but does’t study deeper, and for me it is a great tool for AI training, which should be the key role of simulation tool in future, since L3 or below, really cares little environment info, and its rule-based decision has no need for a large chunk of virtual test. that’s also a difference between Chinese and NA team, NA teams have the trend to invest in new tech even without immeditaly money back, but Chinese companies would prefer in immediate invest. lg simulatorUnreal never tried, but Unity looks pretty easy to use, while still need sometime to get familiar with the editor and play with scenes. lg simulator has give a great reference to build the virtual city and make autonomous vehicles running. the real problem actually for most team is how to make this tool useful in product development. we can think about the tool strucutre itself, like cloud running features, implement measure/verified methods, and build scenes database as we can think about or from the sort of system engineering port. as for a L3 or below usage, the whole meaning of simualtion is not driven, but at most additional support of product development, as I can see. so what simulation for ? L4+, which leads to AI, or data driven product development! from this point, the simlation itself is not the core, but the AI, so think about that in career development.]]></content>
      <tags>
        <tag>simulation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[robust control theory]]></title>
    <url>%2F2019%2F05%2F10%2Frobust-control-theory%2F</url>
    <content type="text"><![CDATA[this is a review from cmu refer state variable methodany Nth order differential equation describing a control system could be reduced to N 1st order equations, these equations could be arranged in the form of matrix equations. define x as system state, y as output, u as input: modern control methods(ODEs) can handle multiple-input-multiple-outputs, and they can be optimized, and they allow to design performance and cost model. effects of uncertaintyobservabilitythe ability to observe all of the parameters or state variables in the system controllabilitythe ability to move a system from any given state to any desired state stabilitythe bounded response to any bounded input robust control theory might be stated as a worst-case analysis, to bound the uncertantiy. metircshow to model the behavior of the test system is one most difficult challenge in design a good control system. adaptive controlset up observers for each significant state variable. at each iteration loop, the system learns about the changes in the system parameters, and getting closer to the desired. while the method may suffer from convergence issues H2 or H-infinityH2 control seeks to bound the power gain of the system, H-infinity seeks to bound the energy gain of the system. gains in power or energy indicate operation of the system near a pole in the transfer function. parameter estimationby establishing boundaries in the frequency domain that cannot be crossed to maintain stability. Lyapanovthe only universal tech for assessing non-linear systems, the method focus on stability. Lyapanov functions are constructed, which are described as energy-like functions, to model the behavior of real system.]]></content>
      <tags>
        <tag>control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Carmaker 8.0]]></title>
    <url>%2F2019%2F05%2F10%2FCarmaker-8-0%2F</url>
    <content type="text"><![CDATA[this is from IPG open house Shang Hai scenario generation taskdata recordtracking vehicles, roads, tobstacles obj: lane, road, barries, GPS input, vehicle position/orientation, fixed ID, type the goal of recoding is for road building, which will be used in replay. road buildGPS input + lane mark info + vehicle location –&gt; vehicle trajectory replayrun config, input as tranversal and longitudial position traffic vehicle location, speed rearrangeinput as : traffic vehile info + ego info , list of traffic vehicle info traffic vehicle manage: 1) manuevor control: free move 2) spawn control: lati + longi --&gt; 23 cases 3) support external plugins + manuevor trigger Synthetic Scenariojunction assistant road type + traffic rules + scenario –&gt; support road topology modification support different envs: day of time, weather, scenario editor to support opendrive import standardizationPEGASUS + ASAM simulation standards roads, scenarios, simulation interfaces Opendrive –&gt; road topology opENScenario –&gt; maneuver &amp; anction abstract definitions Open simulation interface –&gt; interface developed for PEGASUS limitationspre-define route for vehicle ? the ego car has AI maneuvor ? Vitual Prototypeincluding gearbox loss mode, gas mode, through look-up table including hybrid powertrain architectures: automatic gearbox + parallel hybrid including powertrain masses(engine, tank, gearbox, battery, motor) including trailer data set generator including damping top mount Simulation testsupport:: ADAS/AD, POWERTRAIN, Vehicle Dynamics steering system visual casefor less steering will overall comfort and vehicle dynamics reference measurements(steering-in-loop simulator) -&gt; model parameter id + softare + ECU integration –&gt; parameterization &amp; validation -&gt; training how the steering system worksopen loop to get mechanical characteristics(stiffness, friction..) system performance with or without EPS 1) ideal(basis) model vs physical model how to cowok the physical model with autopilot control model ? test bedto support electrification, durability, balancing, driveability, powertain caillbratio, connected powertrain AI training with synthetic scenariodecion making trajectory planning image perception q: how to make sure AI robost ? –&gt; what CarMaker can do for AI? 1) obj annotation (vehicles, pedestrains) –&gt; auto annotation 2) semantic segmentation e.g. IPG Movier for auto semantic segmentation Q: what’s the hardware for ? Cloud &amp; CPU/GPU for Parallelizationq: how to parallel in docker ? 1) test case in each CPUs 2) even for single test run(with multi sensors, multi cars ) resources &amp; distribution CPU: vehile model, drivel model, envs, ideal sensors GPU; visual, camera RIS, radar ris, lidar rs Test run in prallelsensor setup(10 ultra, 5 Radar, 1 Lidar, 1 Camera) host pc (with test manager) + 4 virtual machines output: key figures, reports, statistics, queries open archi for scalable processing( on-premise and cloud) big data anaysis with DaSense by NorCom how it works ? external scheduler mananger, PBS HPC light to support local PC parallel new features in 8.0virtual test driving 8.0 simulink lib (through Simscape) Scenario Editor: vege geenration, animated 3D objs, new models(vehicles, trailers, trucks, bus, buildings, houses, street furniture, pedestrains) visulize road surfaces .. ipg movie fisheye distortion from external file new sensor models(Lidar RSI) q: what’s the difference of open source tool vs commericial ? Lidar RSIIdeal perfect world –&gt; ground truth HiFi –&gt; false positives &amp; negatives raw data –&gt; RSI supporting Lidar type: moving laser &amp; photot diode moving mirrors solid state flash input features : Laser beam, including custom beam pattern, Raytracing rays Scene Interaction, including atmoshpere attenuation, color or material or surface or transparent dependency detection, including threashold, multiple echoes per beam, separability output features: sending &amp; receiving direction of every beam light intensity of every beam time &amp; lenght of light pluse width number of interactions User Case : Nio Pilotby sun peng casesinter-city, parking, closed space, crowded space sensors: 3 front camera, 4 surround camera, 4 mm RADARS, 12 Ultra, 1 driver monitor camera higway pilot in June perception: camera, radar, ult, hd map, location planning : path planning, maneuvor decsion, system control cloud &amp; AI simulation usage FDS -&gt; cases -&gt; SIL platform –&gt; cases -&gt; regression test, abstraction &amp; instantiation ; scene reconsturction(in-house) / close loop SIL ; traffic model training(to do) integration -&gt; HIL what about vd ? –&gt; co-work with simulation and physical test, the cover percentage of simulation is about 80%, the left is from data platformupload nodes -&gt; cloud med usa API server -&gt; fleet mgmt log stash –&gt; elastic search –&gt; Kibana &amp; Admin (tensn and spark ) I think they are collecting data, and this data for scene building and simulation usage in future data visulazationHIL lane model simualtion on HIL fusion simulation on HIL automation testjenkins master –&gt; jenkins slave (agent IPG) –&gt; cloud goalsimulation server &lt;—&gt; data center parallel sim core + simulation monitor (data exchange service) data processing + labelling + case management + traffic model training (replay, SIL, REMODEL, Visuliazation )]]></content>
      <tags>
        <tag>simulation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PID control]]></title>
    <url>%2F2019%2F05%2F04%2FPID-control%2F</url>
    <content type="text"><![CDATA[closed loop systemset point is the desired or command value for the process variable. at any given moment, the difference between the process variable and the set point is used by the control system algorithm to determine the desired actuator output to drive the system. closed loop system, the process of reading sensors to provide constant feedback and calculating the desired actuator output is repeated continuously and at a fixed loop rate. control system performance is measured by applying a step fuction as the set point command variable, and then measuring the response of the process variable. rise time is the amount of time that the system takes to go from 10% to 90% of the steady-state/final. percent overshoot is the amout that the process variable overshoots the final value, expressed as a percentage of the final value. settling time is the time required for the process variable to settle to within a certain percentage(5%) of the finla value steady state error is the final difference between the process variable and set point disturbance rejection is the measure of how well the control system is able to overcome the effects of disturbances. often there is a disturbance in the system that affects the process variables or the measurements of these variables, it’s important to design a control system that performs satisfactorily during the worst case conditions. nonlinear system , in which the control parameters that produe a desired response at one operating point might not produce a satisfactory response at another operating point. deadtime is the delay between when a process variable changes, and when that change can be observed. loop cycle the interval of time between calls to a control system, system that change quickly or have complex behavior requires faster control loop rates. PID theoryproportional responseerror the difference between the set point and the process variable. the proportional gain(K_g) determins the ratio of the output response to the error signal. e.g. the error term has a magnitude of 10, and the K_g is 5, then the proportional response is 50. increasing K_g will increase the speed of the control system response, but if K_g is too large, the process variable will oscillate(why?) integral responsethe integral component sums the error term over time. the result is that even a small error term will cause the integral component to increase slowly. the integral response will continually increase unless the error is zero, so the effect is to driven the steady-state-zero to zero. integral windup when integral action saturates, still without the controller driving the error signal toward zero derivative responsethe response is portortional to the rate of change of the process variable. increasing the derivative time will cause the control sytem to react more strongly to changes in the error, and react more quickly. in practice, most control system use very small derivative time, since the derivative response is highly sensitive to noise. turningwhich is the process of setting the optimal gains of P, I, D to get an ideal response. trial and errorset I, D as zero, and increas P gain. Once P has been set to obtain a desired fast response, I starts to increase to stop the oscillatins to achieve a minimal steady state error. once P and I have been set, the D is increased untill the loop is acceptably quick to its set point. filteringassume a sinusoidla noise with frequency w, the direvative is: so in practice it’s necessary to limit the high frequency gain of the derivative term, either by adding a low pass filtering of the control signal, or implement the derivative term in a cut-off way. Udacity Self driving Car project 9 int main() { uWS::Hub h ; PID pid ; pid.Init(pinit, iinit, dinit); h.onMessage( // cte, the error { double diff = fabs(pid.p_error - cte) ; if( diff &gt; 0.1 &amp;&amp; diff &lt; 0.2) thr = 0.0; else if( diff &gt; 0.2 &amp;&amp; speed &gt; 30) thr = -0.2; pid.UpdateError(cte, dt); steer_value = -pid.TotalError(speed); } } void PID::UpdateError(double cte, double dt) { d_error = (cte - p_error) /dt ; p_error = cte ; i_error = integral(cte * dt); } double PID::TotalError(double speed) { return (Kp - 0.0032 * speed) * p_error + Ki * i_error + (Kd + 0.002 * speed) * d_error ; } in real self driving control system, usually divide as latitual and longitual control. and in different scenarios, there are plenty of PID controls. in the future will expose: highway autopilot, auto parking, urban L4 referencePID theory explained PID control from Caltech udacity pid control]]></content>
      <tags>
        <tag>control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apollo 2.0 Localization 源码]]></title>
    <url>%2F2019%2F05%2F03%2FApollo-2-0-Localization-%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[there are two localization methods: RTK and multi-sensor fusion(MSF). RTK using GPS and IMU inputs, MSF using GPS, IMU and Lidar sensor and HD map as inputs. the output is the localization estimated object instance. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657Status Localization::Start()&#123; localization_ = localization_factory_.CreateObject(cofig_.type()); localization_-&gt;Start(); return Status::OK();&#125;Status RTKLocalization::Start()&#123; AdapterManager::Init(FLAGS_rtk_adapter_config_file); timer_ = AdapterManager::CreateTimer(ros::Duration(duration), &amp;RTKLocalization::OnTimer, this); AdapterManager::GetGps(); AdapterManager::GetImu(); tf2_broadcaster_ = new tf2_ros::TransformBroadcaster() ; return Status::OK();&#125;void RTKLocalization::OnTimer(const ros::TimerEvent &amp;event)&#123; AdapterManager::Observe(); PublishLocalization(); RunWatchDog(); &#125;void RTKLocalization::PublishLocalization()&#123; LocalizationEstimate localization ; PrepareLocalizationMsg(&amp;localization); AdapterManager::PublishLocalization(localization); PublishPoseBroadcastTF(localization);&#125;void RTKLocalization::PrepareLocalizationMsg(LocalizationEstimate *localization)&#123; const auto &amp;gps_msg = AdapterManager::GetGps()-&gt;GetLatestObserved(); Imu imu_msg = AdapterManager::GetImu()-&gt;GetLatestObserved(); ComposeLocalizationMsg(gps_msg, imu_msg, localization);&#125;void RTKLocalization::ComposeLocalizationMsg(const localization::Gps&amp; gps_msg, const localization::Imu &amp;img_msg, LocalizationEstimate* localization)&#123; // add header // set measurement time // combine gps and imu auto mutable_pose = localization-&gt;mutable_pose(); if(gps_msg.has_localization())&#123; const auto &amp;pose = gps_msg.localization(); if(pose.has_position())&#123; // update mutable_pose &#125;; if(pose.has_orientation()) &#123; //update mutable orientation &#125;; if(pose.has_linear_velocity())&#123;&#125;; &#125; if(imu.has_linear_acceleration())&#123; //update mutable_pose acc &#125;; if(imu.has_angular_velocity())&#123;&#125;; if(imu.has_euler_angles())&#123;&#125;;&#125; MSFvehicle localization based on multi-sensor fusion 12345678910111213141516171819202122232425class MSFLocalization &#123; void InitParams(); void OnPointCloud(const sensor_msgs::PointCloud2 &amp;message); void OnRawImu(const drivers::gnss::Imu &amp;imu_msg); void OnGnssRtkObs(const EpochObservation &amp;raw_obs_msg); // ... void PublishPoseBroadcastTF(const LocalizationEstimate&amp; localization);&#125;Status MSFLocalization::Start()&#123; AdapterManager::Init(FLAGS_msf_adapter_config_file); Status &amp;&amp;status = Init(); AdapterManager::GetRawImu(); AdapterManager::ADDRawImuCallback(&amp;MSFLocalization::OnRawImu, this); AdapterManager::GetPointCloud(); AdapterManager::AddPointCloudCallback(&amp;MSFLocalization::OnPointCloud, this); // ... return Status::OK();&#125; there is addtional local_map module, which will be review later.]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apollo 2.0 Prediction源码]]></title>
    <url>%2F2019%2F05%2F03%2FApollo-2-0-Prediction%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[Predictionprediction inputs are: obstacles from perception module nad localization from localization module. outputs are obstacles will predicted trajectories. there are three classes in prediction modules: * container store input dat from subscribed channelds, e.g. perception obstacles, vehicle localization * evalutor predicts paths and speed separately for any given obstacles * predictor generate predicted trajectories for obstacles. e.g. lane sequence(obstacle moves following the lanes), free movement, regional moves(move in a possbile region) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// 模块入口APOLLO_MAIN(apollo::prediction::Prediction);Status Prediction::Init()&#123; predicition_conf_.Clear(); adapter_conf_.Clear();common::util::GetProtoFromFile(FLAGS_prediction_adapter_config_filename, &amp;adapter_conf_); //Initial managers AdapterManager::Init(adapter_conf_); ContainerManager::instance()-&gt;Init(adapter_conf_); EvaluatorManager::instance()-&gt;Init(prediction_conf_); PredictorManager::instance()-&gt;Init(prediction_conf_); AdapterManager::GetLocalization(); AdapterManager::GetPerceptionObstacles(); AdapterManger::AddPerceptionObstaclesCallback(&amp;Prediction::RunOnce, this); AdapterManger::AddLocalizationCallback(&amp;Prediction::OnLocalization, this);AdapterManger::AddPlanningCallback(&amp;Prediction::OnPlanning, this); return Status::OK();&#125;void Prediction::RunOnce(const PerceptionObstacles&amp; perception_obstacles)&#123; ObstaclesContainer* obstacles_container = dynamic_cast&lt;ObstaclesContainer*&gt;(ContainerManager::instance()-&gt;GetContainer(AdapterConfig::PERCEPTION_OBSTACLES)); obstacles_container-&gt;Insert(perception_obstacles); EvaluatorManager::instace()-&gt;Run(perception_obstacles); PredictorManager::instance()-&gt;Run(perception_obstacles); auto prediction_obstacles = PredictorManager::instance()-&gt;prediction_obstacles(); for(auto const&amp; prediction_obstacle : prediction_obstacles.prediction_obstalces())&#123; for(auto const&amp; trajectory:prediction_obstacle.trajectory()) &#123; for(auto const&amp; trajectory_point : trajectory.trajectory_point()) &#123; if(!IsValidTrajectoryPoint(trajectory_point))&#123; return ; &#125; &#125; &#125; Publish(&amp;prediction_obstacles); &#125;void Prediction::OnLocalization(const LocalizationEstimate&amp; localization, ObstaclesContainer* obstacles_container)void Prediction::OnPlanning(const planning::ADCTrajectory&amp; adc_trajectory, ADCTrajectoryContainer* adc_trajectory_container) Prediction interface has defined: RunOnce() and Pulish(). the Prediction class has defined listener’s callback: OnLocalization, OnPlanning, and Start(), Stop() and implemented interface functions. For both EvaluatorManager and PredictorManager, there are Init() and Run() functions, and there are bunch of apis from container class. Evaluator1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859void EvaluatorManager::Init(const PredictionConf&amp; config)&#123; //... switch(config.obstacle_type())&#123; case PerceptionObstacle::VEHICLE :&#123; vehicle_on_lane_evaluator_ = obstalce_conf.evaluator_type(); break; case PerceptionObstacle::BICYCLE:&#123; cyclist_on_lane_evaluator_ = obstlce_conf.evluator_type(); break; case PerceptionObstacle::PEDESTRAIN:&#123; break; &#125; case PerceptionObstacle::UNKNOWN:&#123; default_on_lane_evaluator_ = obstacle_conf.evalutor_type(); break; &#125; &#125;&#125;Evaluator* EvaluatorManager::Run(const perception::PerceptionObstacles&amp; perception_obstacles)&#123; ObstaclesContainer* container = dynamic_cast&lt;ObstaclesContainer*&gt;(); Evaluator* evaluator = nullptr ; for(const auto&amp; perception_obstacle : perception_obstacles.perception_obstalce()) &#123; int id = perception_obstalce.id(); Obstacle* obstacle = container-&gt;GetObstalce(id); switch(perception_obstalce.type()) &#123; case PerceptionObstacle::VEHICLE: &#123; if(obstacle-&gt;IsOnLane())&#123; evaluator = GetEvaluator(vehicle_on_lane_evaluator_); &#125; break; &#125; case PerceptionObstacle::BICYCLE:&#123; if(obstacle-&gt;IsOnLane())&#123; evaluator = GetEvaluator(cyclist_on_lane_evaluator_); &#125; break; &#125; // ... if(evaluator != nullptr) &#123; evaluator-&gt;Evaluate(obstacle); &#125; &#125; &#125;&#125; and there are a few different evaluators: (multilayer perception approach) MLP and RNN(deep neural network), both will discuss in details in future. Predictorpredictor will generate trajectories, a few apis: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172virtual void predictor::Predict(Obstacle* obstacle) = 0;void predictor::TrimTrajectories(const Obstacle, const ADCTrajectoryContainer* );static predictor::Trajectory GenerateTrajectory(const std::vector&lt;apollo::common::TrajectoryPoint&gt;&amp; points) ;void PredictorManager::Init(const PredictionConf&amp; config)&#123; // ... switch(obstacle_conf.obstacle_type()) &#123; case PerceptionObstacle::VEHICLE: &#123; if(obstacle_conf.obstacle_status() == ObstacleConfg::ON_LANE)&#123; vehicle_on_lane_predictor_ = obstacle_conf.predictor_type(); &#125;else if(obstacle_conf.obstacle_status() == ObstacleConf::OFF_LANE)&#123; vehicle_off_lane_predictor_ = obstacle_conf.predictor_type(); &#125; &#125; break; // ... &#125; void PredictorManager::Run(const PerceptionObstacles&amp; perception_obstacles)&#123; ObstaclesContainer* obstacles_container = dynamic_cast&lt;ObstaclesContainer*&gt;(AdapterConfig::PERCEPTION_OBSTACLES) ; ADCTrajectoryContainer *adc_trajectory_container = dynamic_cast&lt;ADCTrajectoryContainer*&gt;(AdapterConfig::PLANNING_TRAJECTORY); Predictor* predictor = nullptr ; for(const auto* perception_obstacle : perception_obstacles.perception_obstacle()) &#123; int id = perception_obstacle.id(); PredictionObstacle prediction_obstacle ; Obstacle* obstacle = obstacle_container-&gt;GetObstacle(id); if(obstacle != nullptr) &#123; switch(perception_obstacle.type()) &#123; case PerceptionObstacle::VEHICLE: &#123; if(obstacle-&gt;IsOnLane())&#123; predictor = GetPredictor(vehicle_on_lane_predictor_); &#125;else&#123; predictor = GetPredictor(vehicle_off_lane_predictor_); &#125; break; &#125; // ... &#125; if(predictor != nullptr) &#123; predictor-&gt;Predict(obstacle); if(obstacle-&gt;type() == PerceptionObstacle::VEHICLE)&#123; predictor-&gt;TrimTrajectories(obstacle, adc_trajectory_container); &#125; for(const auto&amp; trajectory : predictor-&gt;trajectories()) &#123; prediction_obstacle.add_trajectory()-&gt;CopyFrom(trajectory); &#125; &#125; &#125; prediction_obstacle.mutable_perception_obstacle()-&gt;CopyFrom(perception_obstacle); prediction_obstacles_.add_prediction_obstacle()-&gt;CopyFrom(prediction_obstacle); &#125; the predictor has a few types:: regional based, free move, lane sequence e.t.c, which defined as subclasses, will discuss more in future. Containercontainer manager class has a few subclass as adc_trajctory container, obstacles contianer and pose constainer, which should be discussed in details later. Adapter1234567891011121314151617181920212223242526272829// in adapter_manager.cc case AdapterConfig::LOCALIZATION: EnableLocalization(FLAGS_localization_topic, config);case AdatperConfig::PERCEPTION_OBSTACLES: EnablePerceptionObstacles(FLAGS_perception_obstacle_topic, config); // in messsage_adapters.h using PerceptionObstaclesAdapter = Adapter&lt;perception::PerceptionObstacles&gt;; using LocalizationAdapter = Adapter&lt;apollo::localization::LocalizationEstimate&gt;;// in adapter_manager.h static voi Enable##name()&#123; instance()-&gt;InternalEnable##name(topic_name, config);&#125;; static name##Adapter *Get##name()&#123; return instance()-&gt;InternaleGet##name();&#125;static void Add##name##Callback(name##Adapter::Callback callback)&#123; instance()-&gt;name##_-&gt;AddCallback(callback);&#125;template&lt;class T&gt; staic void Add##name##Callback(void(T:: *fp)(const name##Adapter::DataType &amp;data), T* obj)&#123; Add##name##Callback(std::bind(fp, obj, std::placeholders::_1));&#125;]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apollo2.0 Control 源码 (3)]]></title>
    <url>%2F2019%2F05%2F01%2FApollo2-0-Control-%E6%BA%90%E7%A0%81-3%2F</url>
    <content type="text"><![CDATA[the input of Apollo control module includes: chassis info, localization info, and planning info ,the output is steering angle, acc, throttle. 12345678910111213141516171819202122232425262728293031323334// 模块入口#define APOLLO_MAIN(APP) int main(int argc, char **argv) &#123; google::InitGoogleLogging(argv[0]); google::ParseCommandLineFlags(&amp;argc, &amp;argv, true); signal(SIGINT, apollo::common::apollo_app_sigint_handler); APP apollo_app_; ros::init(argc, argv, apollo_app_.Name()); apollo_app_.Spin(); //check previous blog(1) return 0; &#125; Status Control::Init()&#123; init_time_ = Clock::NowInSeconds(); common::util::GetProtoFromFile(FLAGS_control_conf_file, &amp;control_conf_); AdapterManager::Init(FLAGS_control_adapter_config_filename); common::monitor::MonitorLogBuffer buffer(&amp;monitor_logger_); controller_agent_.Init(&amp;control_conf_); AdapterManager::GetLocalization(); AdapterManager::GetChassis(); AdapterManager::GetPlanning(); AdpaterManager::GetControlCommand(); AdapterManager::GetMonitor(); AdapterManager::AddMonitorCallback(&amp;Control::OnMonitor, this); return Status::OK();&#125; conf_file: /modules/control/conf/lincoln.pd/txt, which is derieved from /modules/calibration/data/mkz8/calibration_table.pd.txt the topics in control modules are : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125config &#123; type: LOCALIZATION mode: RECEIVE_ONLY&#125;config &#123; type: PLANNING_TRAJECTORY mode: RECEIVE_ONLY&#125;config &#123; type: CHASSIS mode: RECEIVE_ONLY&#125;config &#123; type: CONTROL_COMMAND mode: PUBLISH_ONLY&#125;config &#123; type: MONITOR mode: DUPLEX&#125; ``` basically, control module will receive topic about /localization, /planning, /chassis, and publish /control_command the controller_agent is an interface class, so can support user defined controller algorithms, which only need to configure through `control_conf` ```cStatus ControllerAgent::Init(const ControlConf* control_conf)&#123; RegisterControllers(control_conf); InitializeConf(control_conf); for(auto &amp;controller : controller_list_) &#123; if(controller == NULL || !controller-&gt;Init(control_conf_).ok()) &#123; return Status(ErrorCode); &#125; &#125; return Status::OK();&#125;void ControllerAgent::RegisterControllers(const ControlConf *control_conf)&#123; for(auto active_controller : control_conf-&gt;active_controllers()) &#123; switch(active_controller)&#123; case ControlConf::MPC_CONTROLLER: controller_factory_.Register( ControlConf::MPC_CONTROLLER, []()-&gt;Controller * &#123;return new MPCController();&#125;); break; //case LAT_CONTROLLER //case LON_CONTROLLER &#125; &#125; &#125; Status Control::Start()&#123; //sleep for advertised channel to ready std::this_thread::sleep_for(std::chrono::millisecons(1000)); timer_ = AdapterManager::CreateTimer(ros::Duration(control_conf_.control_period()), &amp;Control::OnTimer, this); common::monitor::MonitorLogBuffer buffer(&amp;monitor_logger_); return Status::OK(); &#125; void Control::OnTimer(const ros::TimerEvent &amp;) &#123; double start_timestamp = Clock::NowInSeconds(); ControlCommand control_command ; Status status = ProduceControlCommand(&amp;control_command); double end_timestamp = Clock::NowInSeconds(); status.Save(control_command.mutable_header()-&gt;mutable_status()); SendCmd(&amp;control_command); &#125; Status Control::ProduceControlCommand(ControlCommand *control_command) &#123; Status status = CheckInput(); Status status_ts = CheckTimestamp(); Status status_compute = controller_agent_.ComputeControlCommand( &amp;localization_, &amp;chassis_, &amp;trajectory_, control_command); return status; &#125; Status ControllerAgent::ComputeControlCommand( const localization::LocalizationEstimate *localization, cosnt canbus::Chassis *chassis, const planning::ADCTrajectory *trajectory, control::ControlCommand *cmd)&#123; for(auto &amp;controller : controller_list_) &#123; controller-&gt;ComputeControlCommand(localization, chassis, trajectory, cmd) ; &#125; return status::OK(); &#125; ControllerAgent::ComputeCmd() is the interface, the real control cmd will be computed inside each specified controller modes. there are few controller modes: MPC, Lattice e.g in Apollo. in next few days continue work on localization, prediction, and decision modules in Apollo 2.0]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apollo2.0 Routing源码(2)]]></title>
    <url>%2F2019%2F05%2F01%2FApollo2-0-Routing%E6%BA%90%E7%A0%81-2%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// 模块入口APOLLO_MAIN(apollo::routing::Routing) int main(int argc, char** argv)&#123; google::InitGoogleLogging(arg[0]) ; google::ParseCommandLineFlags(&amp;argc, &amp;argv, true); signal(SIGINT, apollo::common::apollo_app_sigint_handler); apollo::routing::Routing apollo_app_ ; ros::init(argc, argv, apollo_app_.Name()); apollo_app_.Spin(); return 0;&#125;``` `appollo_app_.Spin()` can be found [here](https://zjli2013.github.io/2019/04/28/apollo-planning-源码/)```c apollo::common:Status Routing::Init()&#123; const auto routing_map_file = apollo::hdmap::RoutingMapFile() ; navigator_ptr_.reset(new Navigator(routing_map_file)) ; common::util::GetProtoFromFile(FLAGS_routing_conf_file, &amp;routing_conf_); hdmap_ = apollo::hdmap::HDMapUtil::BaseMapPtr(); AdapterManager::Init(FLAGS_routing_adapter_config_filename); AdapterManager::AddRoutingRequestCallback(&amp;Routing::OnRoutingRequest, this); return apollo::common::Status::OK();&#125;/* DEFINE_string(routing_adapter_config_filename, "modules/routing/conf/adapter.conf", "the adapter config filename")*/void AdapterManager::Init()&#123; //... for(const auto &amp;config :: configs.config()) &#123; case AdapterConfig::ROUTING_REQUEST : EnableRoutingRequest(FLAGS_routing_request_topic, config); break; case AdapterConfig::ROUTING_RESPONSE: EnableRoutingResponse(FLAGS_routing_response_topic, config); break; case AdapterConfig::ROUTING_MONITOR: EnableMonitor(FLAGS_monitor_topic, config); break; //... &#125;&#125; where define EnableRoutingRequest() ? 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//apollo/modules/common/adapters/adapter_manager.h #define REGISTER_ADAPTER(name) static void Enable##name(const std::string &amp;topic_name, const AdapterConfig &amp;config) &#123; instance()-&gt;InternalEnable##name(topic_name, config); &#125; template&lt;class T&gt; static void Add##name##Callback( void(T::*fp)(const name##Adapter::DataType &amp;data), T *obj)&#123; Add##name##Callback(std::bind(fp, obj, std::placeholders::_1)); &#125; tempalate&lt;class T&gt; static void Add##name##Callback(void (T::*fp)(const name##Adapter::DataType &amp;data))&#123; Add##name##Callback(fp); &#125; // apollo/modules/common/adapters/message_adapters.husing RoutingRequestAdapter = Adapter&lt;routing::RoutingRequest&gt; ;using RoutingResponseAdapter = Adapter&lt;routing::RoutingResponse&gt;;// apollo/moduels/common/adapters/adapter.h typedef typename std::function&lt;void(const D&amp;)&gt; Callback ;// apollo/modules/common/adapters/adapter.h template &lt;class D&gt; void Adapter&lt;D&gt;:OnReceive(const D&amp; message)&#123; last_receive_time_ = apollo::common::time::Clock::NowInSeconds(); EnqueueData(message); FireCallbacks(message);&#125;void AddCallback(Callback callback)&#123; receive_callbacks_.push_back(callback);&#125;tempplate&lt;class D&gt; void Adapter&lt;D&gt;::FireCallbacks(const D&amp; data)&#123; for(const auto&amp; callback : receive_callbacks_) &#123; callback(data); &#125;&#125; Dreamview and Planning modules have message publish API. e.g. 1234567891011121314151617SimulationWorldUpdater::(WebSocketHandler *websocket, SimControl *sim_control, const MapSerivce *map_service, bool routing_from_file) : sim_world_service_(map_service, routing_from_file), map_service_(map_service), websocket_(websocket),sim_control_(sim_control)&#123; // ... websocket_-&gt;RegisterMessageHandler("SendRoutingRequest", [this][cosnt Json &amp;json, WebSocketHandler::Connection *conn) &#123; RoutingRequest routing_request, bool succed = ConstructRoutingRequest(json, &amp;routing_request); if(succed)&#123; AdapterManager::FillRoutingRequestHeader(FLAGS_dreamview_module_name, &amp;routing_request); AdapterManager::PublishRoutingRequest(routing_request); &#125; &#125; ApapterManager class is used to make sure the connection among each module in ROS message type. Routing class has GPS and IMU input and generate routing and velocity info as output. Navigator class is using A start algorthm with hd map, start point and end point as input to generate a navigation route.]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里(7)]]></title>
    <url>%2F2019%2F04%2F30%2F%E6%9C%AA%E6%9D%A55%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C-7%2F</url>
    <content type="text"><![CDATA[噱头团队必须在有危机意识的企业中成长。靠投资活的团队，往往只展现其繁荣的一面，对内对外；而实际公司的产品、市场定位，甚至内部员工都不知情。对员工缺乏诚实，对市场客户也不会诚实。 民企文化近距离观察了一家民企（长城汽车），意识到民企都不容易迈过国际化的坎儿。为什么需要国际化？因为资本市场是赢者通吃。行业内国际企业进入，本土行业要想生存，必须主动走出去。 民企的老总个人印记太深。集权的管理问题，符合中国文化传统，但与现代管理理念相差甚远。强调军事化管理的集体制，是无法调动个人主观积极性的，对于底层车间工人可能有效，但如此又会造成企业管理上的双轨制，产生内部紧张。另外，集体制会助长一些骄横的个人气息，狭隘自大，而不利于个人内在品质的培养。一个代表先进生产关系和生产力的团队，是不会容忍形式主义的。 国内不错的企业要么狼性，要么艰苦奋斗。其实鼓励创业氛围没有问题，但是文化上很容易“右倾”，把规则理解的太死。 另外强调艰苦奋斗又不集权的华为，给员工持股，也许才是现代优秀企业该有的特征。把员工当作“合伙人“，自然调动了员工的积极性，而不是为某个老板打工。 回国前的想法是参与一个团队的成长，而不是在一家公司打工。所以除了物质利益，现在人更渴求在工作中的身份认同：”合伙人“。 套路国内的套路：先放话。 改革开放前，国家层面严禁私有制，结果江浙的小农户搞了“分田到户”私有承包，后来却全国推广了。面对新情况，国家领导也在摸索，但又要给广大普通人一致的声音。所以先放话。在日后的实践中，慢慢修正，甚至会产生与放话的内容完全相反的实践。至于普通人，如果把放话的内容听的太真，跟领导较劲儿，就是不懂套路。 有些企业做了匪夷所思的规定，还名正言顺地称为“企业文化”，对于明显不符合人情逻辑的条例，也就是这类“放话”，企业领导并不知道怎么管理，员工明白就好，该怎么来还是怎么来；但如果因此想挑战企业领导的规定，就是不懂套路。 提拔 or 压制国内有个说法叫”站错队“：不怨能力，是没跟对人。在美国职场，管理层都至少表现比较”亲民“，另外工薪层也基本生活无忧，所以两者相安，比较容易相信对方；相对，国内的职场还没有成熟的系统，就会有”站错队“的风险。另外大家都有生活压力，难免成了隐性竞争对手，互不信任，所以国内的职场被压制可能多过受提拔。这当然是陋习。 偶然看了密西根的地图，一股亲切感就涌上来。在美国的大环境会把善意当作默认的配置，工作生活上有意无意都会受到他人的帮助或有意无意地帮助别人。善意比较容易表达出来；相比，国内的环境，职场上、生活上，都有一些压制感。 比如生活上的压抑感。在北美的同学都嘴上说，回国好啊，吃的多么多么好。实际上，外面的东西都不敢吃，忌讳比如肉干净吗、油干净吗、放了不该放的调料吗。办点事，老是担心哪里被骗了都不知道。社会系统不成熟，就会有这样的隐形成本。 写在最后写完上面的内容4天后，才有机会再次打开。工作到没时间读书是对人最大的消耗。希望自己尽快适应国内工作生活节奏，而又不受制于这样的工作生活状态。 某个人的回忆 ps 军训的时候，每天跑8km，站军姿一个半小时，在这样的环境下反而更激发我去思考，如何把高标准习成标配。 管理之路我以为自己是细节导向的，国内这个工作环境会占据太多个人时间，叫人没时间思考大方向的问题。高标准成为标配吧。 越来越看到技术是平的，相比，工作流程，系统建设才是企业愿意花钱的地方。写程序、推公式、做运营、甚至做基础研究，到一定阶段都是极易取代的。“单腿走路”是high risk的。相比，任何一个领域的产品人，至少有一条粗腿，同时还有很多不错的小腿。把各个环节、各个岗位有机组合起来的，就是管理。而在工作中需要有意培养系统（管理）意识。 现在的民企，管理水平还是很差的。文化层面上，虽然在推一些流程，但是也有一些制度性的限制。一方面，一边高举打破旧思维搞创新，一边又做很多行为细节上的约束，在员工心理建立墙。工作流程的建立，不是做几次讲座、参加几个培训，领导提倡几次，不是喊出来的，而是像培养一个工作习惯。需要反复练习，慢慢融入到企业文化，变成员工自然而然的行为。 日常层面上，任务管理、时间管理都比较缺乏。老板会希望员工加班，但并没有定义很好的工作内容，即缺乏任务管理，缺乏日常的考核机制。然后责任不明晰，缺乏有效的工作模式，比如出现一个问题，一伙儿人都围上来了。或者一个会议讨论停不下来。当然，大的层面还是工作的流程没有建立好。 今后的工作，要经常跳出来思考系统建设，而不要跟年轻人拼体力上。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Apollo 2.0 Planning 源码(1)]]></title>
    <url>%2F2019%2F04%2F28%2Fapollo-planning-%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104// 模块入口APOLLO_MAIN(apollo::planning::Planning)int main(int argc, char **argv)&#123; google::InitGoogleLogging(argv[0]); google::ParseCommandLineFlags(&amp;argc, &amp;argv, true); signal(SIGINT, apollo::common::apollo_app_sigint_handler); apollo::planning::Planning apollo_app_ ; ros::init(argc, argv, apollo_app_.Name()); apollo_app_.Spin(); return 0;&#125;///////////////////////////////////////////////////////int ApolloApp::Spin()&#123; ros::AsyncSpinner spinner(callback_thread_num_); auto status = Init(); status = Start(); spinner.start(); ros::waitForShutdown(); Stop(); return 0;&#125;///////////////////////////////////////////////////////Status Planning::Init()&#123; hdmap_ = apollo::hdmap::HDMapUtil::BaseMapPtr(); apollo::common::util::GetProtoFromFile(FLAGS_planning_config_file, &amp;config); if(!AdapterManager::Initialized())&#123; AdaapterManager::Init(FLAGS_planning_adapter_config_filename); &#125; AdapterManager::GetLocalization(); AdapterManager::GetChassis(); AdapterManager::GetRoutingResponse(); AdapterManager::GetRoutingRequest(); if(FLAGS_enable_prediction) AdapterManager::GetPrediction(); if(FLAGS_enable_traffic_light) AdapterManager::GetTrafficLightDetection(); ReferenceLineProvider::instance()-&gt;Init(hdmap_, config_.qp_spline_reference_line_smoother_config()); RegisterPlanners(); planner_ = planner_factory_.CreateObject(config_.planner_type()); return planner_-&gt;Init(config_);&#125;/* DEFINE_string(planning_adapter_config_filename, "modules/planning/conf/adapter.conf", "The adapter configuration file")*////////////////////////////////////////////////////////////////void AdapterManager::Init(const std::string &amp;adapter_config_filename)&#123; AdapterManagerConfig configs; util::GetProtoFromFile(adapter_config_filename, &amp;configs); Init(configs);&#125;//////////////////////////////////////////////////////////////void AdapterManager::Init(const AdapterManagerConfig&amp; configs)&#123; if(Initialized()) return; instance()-&gt;initialized_ = true; if(configs.is_ros())&#123; instance()-&gt;node_handle_.reset(new ros::NodeHandle()); &#125; for(const auto &amp;config : configs.config()) &#123; case AdapterConfig::CHASSIS: EnableChassis(FLAGS_chassis_topic, config); break; case AdapterConfig::LOCALIZATION: EnableLocalization(FLAGS_localization_topic, config); break; // ... &#125; &#125; /* DEFINE_string 宏 FLAGS_chassis_topic -&gt; /apollo/canbus/chassis FLAGS_localization_topic -&gt; /apollo/localization/pose*/ where is EnableChassis ? in /apollo/modules/common/adapters/message_adapters.h 12345using ChassisAdapter=Adapter&lt;::apollo::canbus::Chassis&gt;;using GpsAdapter = Adapter&lt;apollo::localization::Gps&gt;;using PlanningAdapter = Adapter&lt;planning::ADCTrajectory&gt;; in /apollo/modules/common/adapters/adapter_manager.h 1234#define REGISTER_ADAPTER(name) public static void Enable##name(const std::string &amp;topic_name, const AdapterConfig&amp; config) &#123; &#125; continue tomorrow]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动驾驶职位介绍]]></title>
    <url>%2F2019%2F04%2F08%2F%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%81%8C%E4%BD%8D%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[光庭科技（武汉）智能网联汽车车载终端产品，汽车IT公司，自主研发、产品设计、制造和销售。e.g. 自动驾驶控制器，远程无线通信终端，立体相机、地图传感器、”光谷梦4.0”自动驾驶系统 职位：感知算法工程师 要求：图像/点云目标检测、跟踪；slam算法；Camera/Lidar/Radar传感器融合算法；系统集成和调试 加分项：ROS开发经验，视觉SLAM算法，深度学习算法 职位：控制算法工程师 要求：控制系统设计、信号处理、动态系统建模； 时域/频域控制器设计和稳定性分析；汽车速度和方向控制 职位：测试主管 要求： 产品测试方案、管理。软件测试及自动化测试工具 华砺智行（武汉）智能基础设施平台，为智能驾驶、智慧城市提供软硬件解决方案。产品：智能网联汽车终端、交通行业、无线通信、云平台、交通应用等 天迈科技（郑州）城市公交运营、管理及服务提供综合解决方案。车联网产品：智能公交调度系统、远程监控系统、智能公交收银系统、充电运营管理。 职位：视频算法工程师 要求： 驾驶员行为状态检测算法，机器学习检测效果 赢彻科技物流 纵目科技环视ADAS解决方案 职位： 车身控制算法工程师 要求： 车身横向、侧向控制算法设计、仿真、测试。自动控制理论，车辆底盘控制，车身动力学，Carsim, Simulink 西井科技（上海）职位： 仿真平台开发工程师 要求： 设计仿真平台架构，传感器仿真模型，3D物理引擎(unreal, unity)，熟悉开源仿真平台(carla, airsim)，构建场景环境 小鹏汽车（广州）职位： SLAM专家 要求： 视觉定位算法；与感知、地图模块结合做3d视觉系统； 职位：HD Map工程师 要求： 大规模多层高精地图框架，地图API, 地图数据质量评估，地图数据格式 职位：运动控制算法专家 要求：转向、制动、动力系统的主动控制，对底盘、执行器的控制需求；车道保持、自适应巡航、自动变道等功能运动控制，现场调试 职位：规划与控制专家 要求：算法开发、测试，提出硬件设计和集成要求 职位： 雷达算法工程师 要求： 算法仿真验证、数据分析、算法嵌入式平台实现、维护 职位：计算机视觉 要求：场景元素识别；道路、停车场环境寓意分割；算法验证 职位：传感器融合 要求： 毫米波、超声波、摄像头、激光雷达测试开发； 感知数据处理、实时地图构建及应用；目标实时跟踪与预测 职位：项目经理 要求： l2-l4产品解决方案项目管理 宇通汽车（深圳）职位： 控制工程师 要求： 轨迹跟踪、车辆控制、 仿真优化 职位：行为决策工程师 要求：行为决策算法开发，碰撞预测、行为预测、驾驶经验库、交通安全规则库等，基于强化学习的跟踪(?) 职位：首席工程师 要求：自驾客车架构规划、设计；核心算法开发和测试；自动驾驶技术跟踪。熟悉人工智能、机器视觉、深度学习、高精地图、定位等， 熟悉滤波算法、轨迹规划 纽励科技职位：ADAS产品经理 要求：产品客户需求调研、项目收集整理， 与主机厂沟通技术方案，产品改进。熟悉AEB, ACC, LKA, FCW, 自动泊车，熟悉汽车电子软硬件设计、嵌入式开发标准，熟悉传感器 光束汽车职位： 控制算法 华人运通职位： 控制工程师 要求： 运动控制算法的设计开发、仿真、优化，测试； 基于驾驶员操作数据的车辆运动控制算法 恒大汽车自行开发，现有汽车平台及产品 职位： 仿真验证经理 要求：汽车电子网络、诊断开发测试、 研发中心结构： 造型中心，动力总成中心， 车联网中心，整车工程中心，自动驾驶中心 上海电气轨道公交智能系统 职位： 控制算法工程师 要求：车辆控制实时数据采集、理论模型、系统仿真和实测验证算法 Magna Steyr 麦格纳斯太尔汽车技术职位： L4 ADAS电子专家 要求： ADAS SE团队 职位： GNSS测试验证首席工程师 牧月科技职位：仿真系统工程师 要求： 负责仿真系统开发、测试，及各种传感器的仿真软件库，利用已有数据构建仿真场景；开发自动化仿真分析平台 景驰科技（文远知行）职位： 仿真算法工程师 要求： 开发场景自动化生成算法、人机交互仿真软件工具集、可扩展计算框架 腾讯CSIG事业部伟世通职位： 软件测试 要求： ADAS软件测试、竞品分析 博世（苏州）职位： 控制算法工程师 要求： ADAS自动泊车系统 亿伽通（吉利系）职位：算法工程师要求：定位产品场景、需求分析； 参与搭建自动驾驶数据智能服务平台；熟悉基于视觉的slam方案 奥迪中国（北京）车联网团队 职位： 测试工程师 要求： ADAS/HAD功能测试 职位：仿真工程师 要求： SiL环境搭建，adas功能仿真测试 斑马智行（阿里、上汽）智加科技（苏州）职位： 传感器标定工程师 要求： 传感器选型、数据读取、内外参标定、在线检测 职位：感知算法工程师 要求： 对图像、点云等的静态场景要素检测和追踪；融合传感器数据后的目标检测和跟踪；服务地图自建、预测规划决策系统 职位： 地图定位工程师 要求： 开发多传感器融合算法以提高地图精度，自动化地图的大规模采集、生成、标注、校政。 职位： 决策规划工程师 要求： 预测、决策、规划等系统 职位： 仿真工程师 要求： 利用真实数据或合成数据搭建动态环境的仿真框架；设计场景，为感知、规划、预测等模块开发仿真测试接口；开发基于仿真测试的自动化分析平台；构建可扩展的仿真框架 魔视智能（上海）极目智能职位： 车辆决策算法工程师 零跑科技职位： 算法工程师 要求： 定位算法模块，基于GPS/IMU的航伟推算算法，视觉定位算法 阿里巴巴（人工智能实验室）职位： 仿真算法工程师 吉利Volvo上海研发中心中智行（南京）l4 解决方案 同元软控（苏州）国产CAD/CAE产品供应商 华为2012华为海思（上海）职位： 软件测试 一汽红旗职位： 车联网构架设计师 要求： T-box端-云构架，v2x 潜在人选： 车联网企业： 斑马、亿伽通、博泰、百度车联网团队、飞驰镁物、哈曼、四维图新、梧桐车联 新势力： 蔚来、小鹏、威马 职位：软件算法、系统工程师 潜在人选： 上汽人工智能实验室 英伟达 福特-argo ai momenta, plusai 职位： 数据挖掘工程师 要求： 大数据建模分析，用户数据、车辆数据、驾驶员数据等 潜在人选： 浪潮 曙光 华为云 阿里云 百度 华域汽车滴滴无人车美团无人车职位： 仿真工程师 一汽大众职位：决策算法工程师 veoneer职位：adas软件开发工程师 易航智能职位： 控制算法工程师 三一无人驾驶无人码头 职位： 算法工程师 易高美职位： hpmap仿真工程师 要求： 大批量仿真数据自动化生成；设计分布式仿真系统底层架构，3D引擎编辑器工具链开发；实现仿真系统场景渲染；对大批量及各种随机虚拟数据自动化脚本工具开发。OpenGL/Direct3D图形接口; unreal, unity引擎]]></content>
      <tags>
        <tag>self-driving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[particle filter (2)]]></title>
    <url>%2F2019%2F04%2F03%2F%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2-2%2F</url>
    <content type="text"><![CDATA[前一篇 介绍贝叶斯滤波的数学原理和蒙特卡洛定位的算法。本篇将介绍基于序贯重要性采样。粒子滤波的思想就是采用一个加权粒子分布去近似后验概率分布p(x) 蒙特卡洛积分定义一连续随机变量X, 其概率密度分布函数为 p(X); 定义Y=f(X)， 则随机变量Y的数学期望： 实际中，概率密度分布p(X)未知，如何保障所采样的点服从p(X) 直接采样通过对均匀分布采样，实现对任意分布的采样。 任何未知概率密度分布的累积概率函数cdf都映射在[0-1]区间，通过在[0-1]区间的均匀采样，再函数z = cdf(y)求逆，即是符合真实 y的概率密度分布的采样点。 但如果cdf()函数未知或无法求逆，直接采样不可行。 接受-拒绝采样用一个已知概率分布函数q(X)去采样，然后按照一定的方法拒绝某些样本，达到近似p(X)分布: p(x_i) &lt;= k p(x_i) 该采样的限制是确定参数k。 重要性采样在一定的抽样数量基础上，增加准确度。未知p(x), 在已知概率密度分布的q(x)上采样{x_1, x_2, … x_n}后估计f的期望： 定义新的随机变量： 关于原随机变量Y在未知概率分布p(x)下的期望，转化为新的随机变量Z在已知概率分布q(x)下的期望。已知概率分布，即知道如何采样。这里 p(x)/q(x) 就是权值。 so the posterior expectations can be computed as: as the importance weights can be defined as: the problem is we can’t get p(x|z) , but a loosed (unnormalized) importance weights as: then do normalized from it: so the posterior expectation is approximated as: sequential importance sampling(SIS)consider the full posterior distribution of states X_{0:k} given measurements y_{1:k} : consider the sequential of q(x): then the unnormalized importance weights can be as: namely: the problem in SIS is the algorithm is degenerate, that variance of the weights increases at every step, which means the algorithm will converget to single none-zero (w=1) weight and the rest being zero. so resampling. sequential importance resampling(SER)resampling process: 1) interpret each weight as the probability of obtaining the sample index i in the set x^i 2) draw N samples from that discrete distribtuion and replace the old sample set with the new one. SER process: 1) draw point x_i from the q distribution.2) calculate the weights in iteration SIS and normalized the weights to sum to unity3) if the effective number of particles is too low, go to resampling process]]></content>
      <tags>
        <tag>control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[partical filter]]></title>
    <url>%2F2019%2F04%2F02%2Fpartical-filter%2F</url>
    <content type="text"><![CDATA[localizing the vehicle involves determing where on the map the vehicle is most likely to be by matching what the vehicles see to the map. Markov localization or Bayes Filter for localization is the generalized filter. thinking of the robot location as a probability distribution, each time the robot move, the distribution becomes more diffuse(wide). by passing control data, map data, observation into the filter will concentrate(narrow) the distribution at each timestep. state spacex = f(x, v) (1) z = h(x, w) (2) v, w is the process noise, measurement noise respectfully, and each is in the normal Gaussian distribution. Bayes filter derivation (b) consider the multiply rule of probability: p(a, b) = p(a|b) p(b) lhs of equation(b) is: given x_k, assuming z_k is independent from all previous measurements z_{1:k-1}: Markov Localizationin which the true state x is unobserved, and the measurements z is observed.assuming 1st order Markov, the probability of current true state: p(x_k | x_{0:k-1}) == p(x_k | x_{k-1}) (3) similarly, the measurement is only dependent on current state, which is a stochastic projection of the true state x_t, : p(z_k | x_{0:k}) = p(z_k | x_k) (4) (3) is referred to as motion model, and (4) as measurement/observation model. the classical problem in partially observable Markov chains is to recover a posterior distribution from all avilable sensor measurements and controls in all timesteps. Especially, for the localization problem here is to obtain the system current state posterior p(x_k | z_{1:k}) based on the all existing measurements, which can be solved by Bayes Filter. ps. the propability distribution of current state is also depend on other known inputs, e.g. map data, control data. predictionfrom Bayes filter equation, p(x_k | z_{1:k-1}) need get first, which is the prediction step. physically, it used to estimate the system state based on all previous measurements. consider x_{k-1} as the random variable, the integration of pdf p(x_k, x_{k-1} | z_{1:k-1}) about x_{k-1} is p(x_k | z_{1:k-1}) consider the multiply rule: by 1st order Markov assuming, the first item in integral can reduced: p(x_k | x_{k-1}) is determined by the system, which obey the same distribution of process noise. p(x_{k-1} | z_{1:k-1}) is known, as the posterior state at timestep k-1. this is where the recursive process. updateusing equation (b) to update the current posterior state. the denominator of (b) is a constant coeffient. p(z_k | x_k) is the likelihood paramter, decided by measurement.]]></content>
      <tags>
        <tag>control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无迹卡尔曼滤波]]></title>
    <url>%2F2019%2F03%2F31%2F%E6%97%A0%E8%BF%B9%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%2F</url>
    <content type="text"><![CDATA[非线性系统： x = f(x, w) （1） z = h(x) + v （2） 随机信号 w, v分别是过程噪声和观测噪声 CTRV 状态方程对于const turn rate and velocity magnitude (CTRV )场景： x = (px, py, v, phi, \dot{phi}) 固定速度和转动速率约束，即： 考虑 dv/dt == 0 , dphi^2\dt^2==0 且\psi是时间的函数, 上述第一项即： 从原状态空间到预测空间，由方程（1),（2）可见，过程噪声w是状态x的非线性项；而z关于观测噪声v是线性的。ukf实际采用增广状态变量sigmax = [x, w]. 过程噪声w包括径向加速度和角加速度 [w_a, w_phi]， 且w不是时间的函数 , 对上述第一项可展开： 预测空间对比扩展卡尔曼 ekf采用一阶线性化近似。无迹卡尔曼ukf，将原状态空间的特征采样点(sigmax)映射到预测空间，采用预测空间里的状态变量f(sigmax)的均值、方差的加权推广作为先验状态估计x^- 和先验误差P^-。 其中权值表述： $$ w = lamda / ( lamda + ns) when i==1 $$ $$ w_i = 0.5/(lamba + ns) when i!=1 $$ $$ X^- = sum(w_i * f(sigmax) ) $$ $$ P^- = sum(w_i * (f(sigmax) - x).^2) $$ 观测空间将原状态空间的特征采样点(sigmax)映射到观测空间，采用观测空间里的状态变量h(sigmax)的均值、方差的加权作为先验观测值Z^- 和观测值先验误差S^-，使用与预测空间同样的权值。 $$ Z^- = sum(w_i * h(sigmax) ) $$ $$ S^- = sum(w_i * (Z^- - z).^2) + R $$ 卡尔曼滤波表示： 后验估计（真实状态变量值）与先验估计（预测空间的状态变量值）的差异，可表示为真实观测值与观测空间里的先验观测值的差异的增益 K。 $$ x - x^- = K (z - Z^-) $$ （3） 可见，卡尔曼增益K在衡量状态误差与观测误差之间的相关性。定义预测空间与观测空间的相关系数： T = sum(w_i * (X^- - x)(Z^- - z)) K = T / S^- （4） ukf算法有（4）， （3） 分别更新卡尔曼增益和状态变量， 预测空间里的先验误差更新由： P = P - KSK^t ps: 在非线性的处理上，线性化或者布点采样都是常用的思路。也是ekf与ukf的区别。 link1 link2]]></content>
      <tags>
        <tag>control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CarND(term2): Extended Kalman Filter]]></title>
    <url>%2F2019%2F03%2F31%2FCarND-term2-Extended-Kalman-Filter%2F</url>
    <content type="text"><![CDATA[sensor measurementRadarradio detection andranging, using radio waves to measure the distance to objects as well as their velocity and angle. Radar used a lot in preventing collions, parking assistance, cruise control. and Radar isn’t affected by weather conditions. while Radar can’t tell an object’s shape correctly. and Radar can’t detect objects if they are out of their line of sight. the Radar measurement data in EKF proejcct is 3D position and velocity vector (ro, theta, ro_dot) in polar coordinates. Lidarlight detection and ranging, using near-infrared light to scan objects and create 3D map of the enviroment. it’s 360-degree view and can track movements and their directions. but it also depends on weather conditions and can’t detecting the speed of other vehicles well. the Lidar measurement data in EKF proeject is 2D position vector (x,y) in Cartesian coordinate system. compare Radar vs Lidar server - client networkUdacity simulator communicate with EKF controller through websocket. in simulators(Udacity carsim, Carla) running time, the message channel between simulator server and external controller need to be open all the time. so the simulator feed the controller with sensor data, and the controller feedback simulator with controlling data. uwebSocketbuilt once uwebSocket, webSocket protocol providing full-duplex communication channel between server and client through a singlt TCP connection. it allows the server to send content ot the client without being first requested by the client, and allowing messages to be passed back and forth while keeping the conenction open. sensor raw dataevery data slice includes a either Lidar or Radar raw measurement and a ground truth measurement. The state variable x is described in Cartesian coord, so for Radar measurement processing, there is a coordinate transfer from Cartesian to polar, and which lead it nonlinear, requiring Extended Karman Filter. EKF controllerthe client side is the EKF controller, which process the sensor measurement. define system state `x_` , state priori covariance `P_`, state transition matrix `F`, process covariance `Q_`, measurement gain matrix &apos;H&apos; measurement covariance &apos;R_&apos; kalman filter gain &apos;K&apos; as discussed in previous blog: 12345678910111213141516171819 void KalmanFilter::Predict()&#123; x_ = F * x_ ; P_ = F * P_ * F.transpose() + Q_ ;&#125;void KalmanFilter::Update(const Vector &amp; measurement)&#123; if(EKF) h = toPolar(x_); y = measurement - h ; else y = measurement - H * x_ ; K = P_ * Ht / (H * P_ * Ht + R_); x_ = x_ + K * y ; P_ = (I - K*H) * P_ ; &#125;]]></content>
      <tags>
        <tag>control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卡尔曼滤波]]></title>
    <url>%2F2019%2F03%2F30%2F%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%2F</url>
    <content type="text"><![CDATA[状态方程状态变量 $x$ 满足, 其中 u为控制变量： $$ x = A x_prev + B u_prev + w_prev $$ (1) 观测变量 $z 满足： $$ z = H x + v $$ (2) 随机信号 $w$ 和 $v$ 分别表示过程激励噪声和观测噪声，假定相互独立，且服从正态分布。 定义状态变量的先验估计 $x^-$， 即基于之前状态对当前状态的预测值； 定义后验估计 $x^$，即已知当前观测值所计算的当前状态变量。 定义先验误差 $e^-$, 后验误差 $e^$, 满足： $$ e^- = x - x^- $$ (3) $$ e^ = x - x^ $$ (4) 卡尔曼滤波表示： 后验估计（观测值所推导的状态变量值）与先验估计（预测的状态变量值）的差异，可表示为观测值与以先验估计为输入的观测值的差异的增益 K。 $$ x^ - x^- = K ( z - H x^- ) $$ (5) K可由先验误差的协方差 P、观测噪声的协方差R 和观测增益H表示: $$ K = P^- H^t / ( HP^-H^t + R ) $$ (6) 可见： 1） 当R 趋于0时， k 趋近于 h 的逆，此时 x^ = x。即当观测误差很小，观测值趋近真实值。 2） 当P趋于0时，即预测值趋近真实值。 算法设计卡尔曼滤波器用反馈控制估计过程状态（变量）： 滤波器估计某一时刻的状态（时间更新/预估），然后以（含噪声的）测量变量获得反馈（测量更新/校政）。 时间更新，当前时步状态先验估计 x^- 及先验误差协方差近似P^-: $$ x^- = A x^-_prev + B u_prev $$ (7.1) $$ P^- = A P^_prev A^t + Q $$ (7.2) 其中 $ P(w) ~ N(0, Q) $ 测量更新，使用(6)更新卡尔曼增益K, 使用（5)更新后验状态变量x^和当前步先验误差协方差值 P^： $$ P = ( I - KH ) P^- $$ (8) 控制器调参测量误差一般可观测得到；而过程误差q需要通过与一个已知误差的在线滤波器对比调整系数。调参一般是离线过程。一般当过程误差和卡尔曼增益会快速收敛并保持常数。但测量误差受环境影响不易保持不变。 扩展卡尔曼当观测值与系统状态变量 或 系统本身是非线性关系，方程(1), (2)变非线性函数。 $$ x = f(x_prev, u_prev, w) $$ (1.2) $$ z = h(x, v) $$ (2.2) link]]></content>
  </entry>
  <entry>
    <title><![CDATA[paper reading-Carla an open urban driving simulator]]></title>
    <url>%2F2019%2F03%2F30%2Fpaper-reading-Carla-an-open-urban-driving-simulator%2F</url>
    <content type="text"><![CDATA[CARLA used to support training, prototyping, validation of self-driving models, including perception and control. CARLA is usded to study the performance of three approaches, 1) classic modular pipeline that comprises a vision-based perception module, a rule-based planner, and a maneuver controller; 2)a deep network that maps sensory input to driving commands via imitaion learning; 3) end-to-end reinforcment learning. all approaches make use of a high-level topological planner. the planner takes the current position of the agent and the location of the goal as input, and use A* algorithm to provide a high-level plan. this plan advises the agent to turn left/right, or keep straight at intersections, but not provide a trajectory neither geometric info. which is a weaker form of common GPS. simulation engineCARLA simulates a dynamic world and provide a simple interface between the world and an agent that interacts with the world. CARLA is designed as a server-client system, where the server runs the simulation and renders the scene, the client API is responsible for interaction between the agent and the server via sockets. the client send commands and meta-commands to the server and receives sensor readings in return. the comands control the vehicle and includes steering, accelerating, and braking. meta-commands controls the behavior of hte server, e.g. resetting hte simulation, modifying the sensor suite. environmentthe static 3D world, such as buildings, traffic signs, and the dynamic objects such as vehicles, pedestrains. the behavior of non-player characters is based on standard UE4 vehicle model(PhysXVehicles), and extended with a basic controller to govern NPC’s behavior: lane following, respecting traffic lights, speed limits, and decision making at intersections. pedestrainspedestrains navigate the streets according to a town-specific navigation map, which conveys a location-based cost, which is designed to encourage pedestrains to walk along sidewalkd and marked road crossing, but allows them to cross road at any point. sensorscamera parameters include 3D location, 3D orientation with respect to the vehicle’s coordinate system, field of view, and depth of field. the semantic segmentation pseudo-sensor provides 12 semantic classes: road, land-marking, traffic sign, sidewalk, fence, pole, wall, building, vegetation, vehicle, pedestrain, and other. a range of measurements associated with the state of the agent. ? measurements concerning traffic rules include the percentage of vehicle’s footprint that impinges on wrong-way lanes or sidewalks, as well as states of the traffic lights and speed limit at the current location of the vehicle. CARLA provides access to exact location and bounding boxes of all dynamic objects. autonomous drivingthe agent interacts with the environment over discrete time steps. at each time step, the agent gets an observation, which is a tuple of sensory inputs, and must produce an action, which represents steering, throttle, brake. modular pipelinethe pipeline includes: perception, planning, continuous control. local planning is critical based on visual perception. perceptionusing semantic segmentation network based on RefineNet to estimate lanes, road limits, and dynamic objects. and a classification model is used to determine proximity to intersections. the local plannercoordinates low-level navigation by generating a set of waypoints, near-term goal states that represents the desired position and orientation of the car in near future. the rule-based state: 1) road-following, 2) left-turn, 3) right-turn, 4) intersection-forward, 5) hazard-stop. transitions between states are performed based on estimates provided by the perception module and on topological info provided by the global planner. continuous controllerusing PID controller, which inputs current pose, speed, a list of waypoints, and outputs steering, throttle, and brake. carla release9.11) enable client to detect collisions and determine lane changes : sensor.other.collision, sensor.other.lane_detector, 2) access to the road network, waypoints nearby current vehicle and define user navigation algorithms: Map 3) support new map created from external RoadRunner/VectorZero, in OpenDriven map standard 9.21) simulation of traffic scenarios by Scenario Runner. e.g. following leading vehicle, stationary object crossing, dynamic object crossing, opposite vehicle running red light, vehicle turn right/left etc 2)upgraded ROS bridge 3) vehicle navigation from client side: BasicAgent, navigate to a point given location while dealing with other vehicles and traffic lights safely; RoamingAgent, drives around making random choices when presented to multiple options 9.31) new town and new pedestrains 2) no rendering mode, a 2D map visualization tool that display vehicles, traffic lights, speed limits, pedestrains, road, etc. help to improve the server framerate 3) traffic light class in client TrafficLightState 4) new sensors. ObstacleDetector, a simple raycast sensor to detect something in front of the ego vehicle and what is it; and GlobalNavigationSatelliteSystem, attach to ego vehicle and get its geolocation, which is based on the geo-reference define in OpenDriven file associated with each map. 9.41) allow client side to change physics properties of vehicle or their components in runtime WheelsPhysicsControl 2) logging and playback system, which includes a camera-following mode to follow a target actor while replaying the simulation, and can replay situations from different viewpoints. and the logging query engine allow users to query different types of events. 3) random streaming port, which makes it possible to stream sensor data in a secondary port 4) import maps, replace maps as tar.gz files in “ExportedMaps” folder]]></content>
      <tags>
        <tag>self-driving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paper reading-autonoVi: AV planning with dynamic maneuvers and traffic constraints]]></title>
    <url>%2F2019%2F03%2F29%2Fpaper-reading-autonoVi-AV-planning-with-dynamic-maneuvers-and-traffic-constraints%2F</url>
    <content type="text"><![CDATA[this is the advanced driver module in Vi-sim simulation platform. this driver module algorithm pipeline: 1) a route plan by graph-search over the network of roads 2) rules based guiding trajectories generation(traffic and lane following rules) 3) set of candidate trajectories(control inputs) generation and evaluated by vehicle dynamic model and collision free model 4) most feasible trajectory evaluated through optimization vehicle state spacethe full state of a vehicle updates: X = (x, y, v, theta, throttle, steering, behavior) the vehicle updates its plan at a fixed palnning rate dt; at each pllaning step, the vehicle computes a target speed v and target steering theta to be achieved by the control system S(u, X) determine if a set of control is feasible, given current state of the vehicle, S(u, X) will return false if the given input u cause a loss of traction or control. sensing and perceptionthe sensing module provide an approximation of the center line of lane, closet point on the lane center to the ego-vehicle, and friction coefficient. route choice and behavior statebehavior set includes merging, right turn, left run, keep straight. the behavior state of the vehicle is described as a finite-state machine(turn left, turn right, merge left, merge right), which restrict potential control decisions and adjust the weight of the cost function. guiding paththe ego-vehicle computes a set of waypoints along the current lane at fixed time intervals. how to create the path based on waypoints collision avoidancedefine obstacles domain for each neighbor of the ego-vehicle, which is defined as all controls that could lead to collision. the obstacles domain and the set of dynamic infeasible domain form the boundary of collision-free space for the ego-vehicle. trajectory samplingthe exact obstacle domain is not computing time efficent, instead here use a sampling strategy around theta and v to determin a feasible control. each sample is referred to as a candidata control u_c. trajectory cost functiononce the set of suitable control candidates has been computed, the most feasible control will be selected by minimizing the cost function for each sample point i : C = sum_i{ C_path(i) + C_cmft(i) + C_mnvr(i) + C_prox(i) 1) path cost, defined as success at tracking its path and the global route. 2) comfort cost, C_cmfg = ||vel_acc|| + ||theta_acc|| 3) maneuver cost, penalize lane changes C_mnvr = lane_change 4) proximity cost, prevent the ego vehicle from passing close to neighbors. control inputone PID controller to driven current speed to match the target speed; another PID controller drives the current steering angle to match the target.]]></content>
      <tags>
        <tag>self-driving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里(6)]]></title>
    <url>%2F2019%2F03%2F29%2F%E6%9C%AA%E6%9D%A55%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C-6%2F</url>
    <content type="text"><![CDATA[广州之行投自动驾驶，隐约有些不安全感。比如，激光雷达、定位l3+的HDMap, 仿真环境，l4全栈解决方案。一旦l3+以上的自动驾驶方案短期内不会量产，做这些方向的创业团队、产品投入都会”死“。 国内所有行业的激烈竞争，造成包括互联网公司、主机厂、创业团队，都强烈需要落地产品。而对于一个遥遥无期，5～10年，甚至更久以后才能实现的产品的技术研发投入是所有公司不能承受之轻。相反，对能快速落地的产品、应用场景，也是各方发力的地方。比如， l1 ~ l2.5 的 adas 产品。 在国外主机厂侧重研发；国内主机厂侧重落地。也是回国感受到的落差。朋友讲，国内主机厂对供应商的依赖很重。长城的情况大概就是，l3以下的adas产品软硬件全栈由供应商提供，主机厂自身连标定/调参都不参与😓。主机厂的趋势是慢慢自己做，也是国内汽车人的机会吧。相比，福特，通用都是15+万员工的规模，国内自主品牌主机厂的员工规模在1/20。另外，国内主机厂员工大多是dre角色。 行业走近了，都是深水。想轻轻松松工作，就是表面划水。对行业风险缺乏判断、或者不能承受行业风险的，想挑容易的活儿，那在哪行都待不长。所以啊，年轻人就要在压力环境下活着。 汽车创业了解到一些汽车行业的创业者，比如做汽车云、车载服务、以及自动驾驶软硬件方案。可能因为创业方向本身只是依托于自驾、智能网联车等具体应用场景，其所创的技术只是一些在其他领域成熟或新的技术。比如，云计算、移动操作系统、视觉AI、5G等技术在汽车载体上的转化应用。 所看到的汽车领域的创业，更像是一个技术转化。而源于汽车领域自身的新创意，似乎只在博世、大陆等成熟技术积淀的企业里面逐步推进的。而且行业内新应用场景的标准定义也是由这些大厂主导的。比如，AutoSar, LTE-V2X, ADAS前装需求定义等等。 没有行业积淀的汽车创业，看着叫心悬，这样都敢玩！他们最好的命运可能是被主机厂收购，但是更大可能是被互联网巨头挤掉，无声无息。从广州回家的路上想到：自己曾经也是有梦想的人，面对现在的市场环境，也许能去个大平台做点事，就聊以自慰了。 创业 本是个挺好的事儿，但必须有强大的信念，觉得这事儿一定能成。只想搞个概念移植，那是注定要凉。]]></content>
  </entry>
  <entry>
    <title><![CDATA[paper reading- autono Vi-Sim simualtion platform]]></title>
    <url>%2F2019%2F03%2F28%2Fpaper-reading-autono-Vi-Sim-simualtion-platform%2F</url>
    <content type="text"><![CDATA[introduction of Vi-Sim data generation, allowing exports of traffic data and virtual sensor data on the vehicle, which can be used in training DL by generating automatically labelled classification and control data dynamic traffic conditions, with varying vehicles, pedestrians, lighting, weather rapid scenario construction simulation modules Vi-Sim is divided into 8 extensible modules. roads represented by center line, #lanes, directions, surface friction. the roads can quick constructed by drawing splines on the landscape road network provides connectivity information of road and traffic infrastructure. the road network provides routing and localization purpose. infrastructure represents traffic lights, signage, and any entities that will modify the behavior of vehicles on the road. environment represents the time of the day, weather, rain conditions, road friction etc. non-vehicle traffic basically pedestrains and cyclists in the map. both are following safe traffic rules. data capture this module used for logging data of the environment as well as sensor data from ego vehicle driving modulesvehicle represented as a physical-driven entity with specific tire, steering, sensor parameters. the vehicle has 3 components: * control is provided with steering, throttle, brake inputs; * dynamics is implemented in Nvidia physX engine; * perception component is a ray-cast with configurable uncertainty, detection time, classification error rate, and sensor angle/range. a vehicle can equip multiple sensors. the perception component provides interface to a generic camera interface and Monte Carlo scanning ray-casts, which can be extended to Lidar/camera based NN claassifiers. driver driving decision module, who fuses information from road network and vehicle’s sensor to make decisions. currently there are 3 driver models * lane-following driver, which employs control command like lane-keeping ADAS * manual driver, allows a human drive the vehicle * autonoVi driver, use optimization-based maneuvering with traffic constraints to generate advanced behaviors limitations lack of calibaration configuration to replicate specific sensors driver modules are limited to hierarchical, rule-based approaches real traffic conditions thoughts1) sensor components in simulation, e.g. Lidar, camera, Radar 2) sensor calibration in simulation, Apollo and Carla may has some good suggestions 3) multi-agents environment 4) distributed framework to ensure real time multi-agents simualtion]]></content>
      <tags>
        <tag>self-driving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paper reading-distributed simulation platform for autonomous driving]]></title>
    <url>%2F2019%2F03%2F28%2Fpaper-reading-distributed-simulation-platform-for-autonomous-driving%2F</url>
    <content type="text"><![CDATA[to test newly developed algorithms, due to the massive amount of simulation data, need a distributed simulation platform based on Spark distributed framework. simulation based on synthetic data, used in control and planning simulation based on real data playback, used to test function and performance of different components in autonomous driving system, each functional module in ROS is deployed as a node, the communication between nodes rely on the messages with well-defined formats. so the test of each module is independent, we can develop simulation module for each functional module. anatomy of simulatorthere should be a dynamic model of the car, a vehicle dynamic model; then the external environment is needed, which includes static and dynamic scenes. the simulator can decompose external environment into basic elements, and rearranges the combination to generate a variety of test cases, each simulating a specific scenario. e.g. the position, speed, next step command of the barrier vehicle can give different basic elements. ROS based simulatorto use the real traffic data to reproduce the real scene requires a distributed simulation platform. ROSBAG, record from Topic and replay message to Topic. the Record function is to create a recording node in ROS, and call the subscribe method to receive ROS message to all Topics, and then write the message to Bag file. the Play function is to establish a play node, and call the advertise method to send message in bag to specified Topic. Spark distributed platformthe Spark driver launch different simulation applications, e.g. localization algorithms, object recoginization algorithms, vehicle decision-making and control algorithms etc, then allocate resources to each Spark worker, who first reads the RosBag data into memory and launches a ROS node to process the incoming data. the interface between Spark and ROS is through Linux pipe, basically data written to the write end of the pipe is buffered by the kernel until it is read from the read end of the pipe. two problems: 1) Spark only support text-based data consuming; 2) Spark memeory to ROBag Binary data streamingthe core Spark data structure is resilient distributed dataset (RDD). to process and transform binary data into a user-defined format and transform the output of Spark computation into a byte stream, even further to a generic binary file(HDFs) 1) encode and serialize the binary files(image, lidar input data) to form a binary byte stream 2) de-serialize and decode the binary stream, according to interpret byte stream into an understandable format and perform target computation 3) the output then be encoded and serialized before passed in RDD partitions(e.g. HDFs), and returned to Spark driver data retrieval through ROSbag cachetwo things: reading from memory through ROSbag play, and writing to memory through ROSbag record. solution: design a memoryChunkedFile class, derived from ChuckedFile class, to read/write memory rather than files.]]></content>
      <tags>
        <tag>self-driving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paper reading-perception, planning, control and coordination for autonomous vehicles]]></title>
    <url>%2F2019%2F03%2F27%2Fpaper-reading-perception-planning-control-and-coordination-for-autonomous-vehicles%2F</url>
    <content type="text"><![CDATA[Lidar Perception point cloud based approach, directly use the raw sensor data for further processing. usually applied a voxel-based filtering to reduce the number of points feature based approach, extract parametric features out of the point cloud, and represent the environment usign extracted features (out of date) grid based approach segmentation algorithmscluster points into multiple homogeneous groups. edge based method region based method, cluster neighborhood points based on certain criteria model-based / parametric method, graph based method detection algorithmscategorize each cluster into different objects, the information in each cluster is mainly from spatial relationship and Lidar intensity of the points. Visionusually deal with road detection and on-road object detection. Road Detectionlane mark detection lane line feature extractionbasically identify the pixels that belong to lane line marks. fitting the pixels into different models estimate the vehicle pose(lateral position and moving orientation) based on the fitted model. road surface detectioninform the self-driving car on the location of free space where it can drive without collision. usually three ways: feature based detection first identify the feature points or patches in the original image; based on the identified features, either model fitting or segmentation kind of algorithms will be applied to identify the road surfaces. feature based learningfirst extract a set of features associated to pixels or image patches, then train a classifier based on the features to assign a road or non-road label to the pixels or patches deep learning on-road object detectionmainly concerns vehicle and pedestrain object classes, and mainly with deep learning based approaches, whose pipeline usually like: 1) the proposal bounding boxes needs to be generated around the input image 2) each proposal box will be sent through the CNN network to determine a classification and fine tune its bounding box location Fusionsensor fusion betweeen Lidar and camera is necessary to make the best use of these devices and achive a robust environment perception result. Localizationthe problem of estimating the ego vehicle’s pose, can divided into 2 sub-problems: pose fixing problem and dead reckoning problem. pose fixing problem is to predict a measurement given a pose, e.g. a map. dead reckoning problem, the state is related to the observation by a set of differential equations, and these equations has to be integrated in order to navigate. map aided localization algorithm use local features to achieve highly precise localization. e.g. SLAM. a key event in smoothing based SLAM is loop closure, basically when features that have not been seen for a while are observed again from the sensor readings. when a loop closure is detected, the error caused by imperfect odometry can then be removed, and a substantial portion of the map can be updated. approaches to semantic mapping can be categorized into three ways: object based, appearance based, and activity based. appearance based semantic mapping, interpret sensor readings to construct semantic information of the environment object based semantic mapping, use the occurrence of key objects to build a semantic understanding of the environments, where object recognition and classification is important. activity based semantic mapping, relies on information about the activities of the agents around the ego vehicle. Planningmission planningperformed through graph search over a directed graph network, which reflects road/path network connectivity. behavioral planningdecision making to ensure the vehicle follows any road rules and interacts with other agents in a conventional, safe manner. motion planningthe process of deciding on a sequence of actions to reach a specified goal, typically based on avoiding colisions on a sequence of actions to reach a specified goal. combinatorial planning, builds a discrete representation of the real environment, and finds a complete solution. sampling-based planning, utilizes a collision checking module to conduct discrete searching over samples drawn from the configuration space, which rely on random sampling of continuous spaces and the generation of a feasible trajectory graph where feasibility is verified through collision checking of nodes and edges to connect these nodes . planning in dynamic environments decision making structures for obstacle avoidance to monitor regions along the intended path for potential obstacle collisions, where these regions are labeled as “critial zones”, and checking against the trajectories of all nearby vehicles to determine a “time to collision”. planning in space-time control space obstacle represnetations rather than checking for collisions directly in robot’s configuration space, directly plan in the control space by prohibiting certain control actions which are predicted to lead to collision. incremental planning and replanninga means of incrementally generating sub-goals, a new plan is generated as often as a new sub-goal is defined. iteratively replanning to generate new solution trajectories presents a potential opportunity to carry over knowledge from previous planning iterations to subsequent planning iterations. Controlfeedback controle.g. proportional-integral-derivative(PID) controller, the limitation of feedback-only control, is has delayed response to errors. model predictive controltrajectory generation combined trajectory generation and tracking separate trajectory generation and tracking 1) trajectory generation, to find an entire control input, which corresponds to some desired state trajectory. can be dealed as a two point boundary value problem, with a starting state and a final state. 1.1) sensor based trajectory generation 1.2) dynamic based trajectory generation trajectory tracking geometric methods pursues a point along the path that is located at a certain lookahead distance away from the vehicle’s current position. the input is waypoints, rather than smooth curves. model based methods kinematic model based controllers performs well at low speed applications, but the error increase as the vehicle speed and curvature rate of path increases. dynamic model based controllers performs well for higher speed driving applications summarythis is an overview of self-driving car system. From a job-hunting view, the algorithms details are more important and better with some practial experience. paper link]]></content>
      <tags>
        <tag>self-driving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[book reading: 第一本无人驾驶技术书]]></title>
    <url>%2F2019%2F03%2F27%2Fbook-reading-%E7%AC%AC%E4%B8%80%E6%9C%AC%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%E6%8A%80%E6%9C%AF%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[一些方案 基于gps/imu融合的定位 基于视觉的定位 基于点云的定位 基于视觉的物体识别与跟踪 基于lidar的物体识别与跟踪 基于点云的定位简化概率问题：已知t_0 时刻的点云信息，以及t_1时刻，无人车位置的先验概率分布，求无人车位置的概率分布。 贝叶斯法则: $$ P(X_t) = P(Z_t | X_t ) \cdot \ \vec(P(X_t)) $$ $\vec(P(X_t))$ 是汽车当前位置的概率分布； $P(Z_t | X_t)$ 是当前位置下观察的点云概率分布。 ROS based systemROS是基于消息传递通信的、分布式多进程框架。Topic发布、接受是一种异步通信方式； Service服务是一种利用同步通信请求/回复交互的分布式系统。 传感器Lidar 环境感知： 通过雷达扫描汽车周围的环境3d模型。运用相关算法比对上一帧和下一帧，从而匹配环境中的其他车辆或行人 slam定位：实时扫描地图，与高精地图中的特征物比对，实现导航及精准定位。 供应商： Velodyne 毫米波雷达 可用工作频段 24ghz、77ghz, 波长 1 ～ 10 mm。77ghz物体分辨率较24ghz提高2～4倍，测速和测距精度提高3~5倍。电磁波频率越高，距离和速度的检测解析度越高。 供应商 射频芯片：24ghz成熟供应（博世、飞思卡尔）； 77ghz没有对中国开放 雷达数据处理芯片：恩智浦，意行半导体 摄像头 高动态 中低像素 适合温度范围 -40 ～ 80 度 防磁抗振 寿命长 计算平台 计算单元与计算负载 GPU执行卷积任务最有效，DSP执行特征提取最有效。 移动端soc架构 I/O子系统与前端传感器交互；DSP负责图像处理流以进行特征提取；由GPU进行目标识别和其他深度学习任务；由一个多核CPU完成规划、控制和互动的子任务；由FPGA进行动态重构以分时共享的方式完成传感器数据压缩上传，物体跟踪和流量预测等。计算部件和I/O部件之间通过共享内存进行数据通信。 系统安全安全问题：强磁场干扰IMU；假大功率的gps信号；干扰激光雷达，在无人处周围放置强反光物；干扰高精地图的更新； ros系统劫持、通信修改。obd-2入侵， 充电桩入侵，车载cd入侵，蓝牙入侵。 Spark与ROS的分布式模拟平台基于合成数据的模拟；基于真实数据回放的模拟。 高精地图]]></content>
      <tags>
        <tag>self-driving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里(5)]]></title>
    <url>%2F2019%2F03%2F26%2F%E6%9C%AA%E6%9D%A55%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C-5%2F</url>
    <content type="text"><![CDATA[深圳之行来深圳前只说见见朋友。而工作在这里的年轻人，都在讲述着让我陷入焦虑的故事。对国内的年轻人、行业选择、工作地点有了接近真实的了解。可能是朋友的分布在各个地方不同，深圳的年轻人最明显的感受是干劲儿足，而且对未来充满了希望。没有白走的路。 在美帝的最后一段时间有接触面试：阿里巴巴人工智能实验室、美团无人车、硅谷的自动驾驶创业团队， zoox, cruise, tosimple, drive.ai，当时只是隐约感觉不适应。工作内容、技能要求，都不在我的雷达里。 过去来讲，是选择性忽略行业常识。对互联网人、金融人的自信、高薪比较回避，也因为在国外相对生活无忧、高新安逸的环境。而这些行业领头平台对个人成长的价值，也被忽略了。当用常识去看世界，对互联网平台的成长是非常感兴趣。 实际上，平均2～3年，在华为、腾讯等国内领军互联网平台的年轻人可以独立带项目了。相比，在国外工作的头三年，大部分还在为身份焦虑，或者假期哪里玩而晒朋友圈。 站在快速迭代的行业ceo角度讲，比如互联网、金融行业，都越来越校招，使用应届毕业生，其创造的价值和投入的成本确实更划算。除非有突出和无法复制的核心。所以，工作中一定要保持积累，否则离开了平台，又缺少技能积累，很快失去竞争力。甚至一些“曲线救国”的规划，会抓紧到大平台“偷学”完成，然后迅速跳更期望的位置。 回国的朋友说，国内同龄年轻人，对行业技能的积累，比国外回来的要高一两个层次。记得两年前的一个中午，站在上海张江高科园区，看到黑压压的年轻人涌出办公楼，那一刻的感受是蛮绝望的。当时我兴庆不在国内工作。没意识到危机，就先被危险吓到～ 行业分布在美帝的几年，对美帝的产业分布会有些认识。比如，汽车制造业在密西根；石油能源行业在德州；互联网、智能、医疗等在硅谷湾区、波士顿等。对国内的行业分布，也是这次来深圳才慢慢听到的。之前对地域的认识只有南北方区别。北京是互联网、外企聚集地；上海和周边也是外企聚集地，特别汽车行业的产业链全覆盖；深圳广州从加工制造起身，电子硬件产业链完整，也有腾讯、华为等互联网链条。成都、重庆、武汉也集中一批软件产业和汽车制造业。 为什么一定要到行业top的公司去实践，想必是一个常识。虽然平台是带不走的，这里面的管理文化、优秀的同事、解决的行业问题、做的产品、见到的客户等等都会烙印在个人身上。 为什么一定要去top的行业去实践，也是同样的常识。对于没有明确兴趣和职业方向的年轻人，top的行业是能帮忙迅速完成鉴定的。这就比如年轻人当初一定要去一线大城市，就是想去见识下，不为别的。是骡子是马，在top行业溜一圈就有底了。 产品方向过去几年在技术岗没有明显成就，所以快30岁就考虑转产品岗位了。这真是一个危险的信号。以前的解读是，反正技术不会干一辈子，总要横向扩展。实际上，相比单纯的技术方向如果不出众，转产品岗只会越走越低。因此，年纪的危机感也加重了。不再像一个初出茅庐的年轻人愿意去挑战，而是求安稳的心态。越缺少专注，越容易被不重要的问题困扰。记得刚回来找工作，我对薪资待遇是念念不忘，而不在能做成什么事。现在要保持一个学习者同时给企业创造价值的心态。 很多朋友了解我不到30岁，都觉得有些问题考虑的过早了。应该继续放手搏一搏。长线来讲，现在还是积累的年纪。选择回避，去一个非技术的岗位、去一个二线城市、甚至换一个不知深浅的行业，都不是明智的。 也不要思考这种弱智问题，诸如，35岁以后干什么呢？ 以前很擅长谈vision, 没有行业积累，不能低下去踏实做事情，会是回国就业的障碍。 阿里巴巴，2018年统计员工9万，其中85%月薪资在20k ～ 50k 腾讯，2017年统计员工人数4万，其中88%月薪资在20k~50k 百度，2017年统计员工人数近4万，其中70%月薪资在20k~50k 滴滴，2019年统计员工人数为1.3万，其中71%月薪资在20k～50k 华为，2017年统计员工人数为17万，其中55%月薪资在20k~50k 国内top行业和行业top公司的薪资情况，half million的样子。回归常识吧。 参考，OEM自动驾驶研发工程师薪资（年） 长城（保定） 30万 + 长安（重庆） 40万 + 吉利（杭州） 40万 + 广汽（广州） 35万 + 公司性质主机厂、外资供应商、国内领头互联网企业、创业团队，也都有接触到了。从最开始很happy有主机厂的offer，到慢慢听到猎头讲，国内oem的尿性。领头互联网内部也是小股作战，同时又有平台的优势；优秀的创业团队比较吸引资本，都非常棒。相比，外资供应商是体系成熟、个人自由、但核心内容不一定在大陆，所以适合做跳板，迅速完成积累然后跳出。]]></content>
  </entry>
  <entry>
    <title><![CDATA[未来5年在哪里(4)]]></title>
    <url>%2F2019%2F03%2F18%2F%E6%9C%AA%E6%9D%A55%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C-4%2F</url>
    <content type="text"><![CDATA[如果衣食无忧、每天和快活的人在一起，谁还去努力奋斗呢。 – by me 从北京回来写下： 在一线城市的优势就是对政策的敏感，能够提早做布局。相比，三四线更多是跟随和被影响。 上层可以很自然向下扩展，但下层很难上去。不论是产品、概念、服务。比如，提到英语培训机构，会想到 新东方，而不会是 钟祥少儿英语。 新东方从北京发展到武汉市自然而然的；但定位在钟祥的少儿英语，甚至不会逆向到荆门。其他包括互联网服务业、造车新势力等等。 起点决定了发展上升空间。 底层会有很多直接面向终端人群的利弊。比如一个家电品牌在某村的供销商，必然跟当地人很熟。只是在中国只要发展到一定阶段，就会碰到“天花板”。造成这些地方（下层）的生意，处于一个“长不大也饿不死”的阶段。 底层的生意也容易受壁垒保护，毕竟除了概念、体制可以低成本渗透，企业运营管理要做到三四线是相当沉重的。所以，三四线可以肆无忌惮地复制品牌，或者同类小生意林立。比如，服装、餐饮等等。 之前考虑，为什么中国没有像Mc Donald’s, Wendy’s的全国连锁店，只看到了模式复制的成本。其实更深应该是两国体制不同。美国整体运营环境较为平坦；而中国是有上下层的。政治模式决定经济模式，所以，美国的品牌容易横向扩展，标准在哪里都可以建立；而中国的品牌都需要从上到下，游戏规则只会在上层建立。 一些不安分的理想青年，大约不想只参与游戏，被割韭菜，所以才要努力在现有的格局上捅窟窿。昌兄说，有朝一日，中国如美国，都是百年老店了，年轻人也就会逆来顺受，接受命运的安排，兴许人生也更轻松。 在三四线做点小生意，不求闻达于诸侯，苟且偷生不好吗？ 作为一个无产者，犯不上考虑这些问题。作为有产者，如何保证财产安全，就是底层忧虑的根源。对个人产权缺乏保护是任何奋斗且成功的青年都会担心的。而唯一的途径，就是通过更多途径赚钱，积累财富。雪球越滚越大，像一只无形的鞭子。受益者也许是国家，兴百姓苦，亡百姓苦。]]></content>
  </entry>
  <entry>
    <title><![CDATA[apollo planning module]]></title>
    <url>%2F2019%2F03%2F03%2Fapollo-planning-module%2F</url>
    <content type="text"><![CDATA[决策规划, 根据导航信息及车辆的当前状态，在有限时间内，计算出一条合适的轨迹。 Lattice Planner Sample candidate trajectories Assign costs Select lowest-cost trajectory, to satisfy constraint &amp; collision check in each cycle Output Frenet 坐标定义沿路面的一条光滑参考线（路面中心线）， 汽车位置坐标由纵向偏移量，横向偏移量表示。纵向坐标，由汽车质心在参考线上的投影点，到参考线起点的长度s表示；横向坐标， 由投影点到汽车质心的距离l表示。 在Frenet坐标下，汽车的朝向、速度、加速度可由横、纵向偏移量的一阶导、二阶导表示。另外，由车辆动力学控制方程可知，横向的偏移量变化率与纵向速度相关，即横向运动是由纵向运动诱发的。 生成轨迹分别由初始（当前）横向、纵向的状态信息（位置、速度、加速度），和下一时刻的横向、纵向状态信息（位置、速度、加速度），可以得到Frenet坐标下的一个轨迹点。通过一系列的时间点t0，t1, … tn, 可以得到一系列的轨迹点p0, p1, … pn， 即形成一条轨迹。 轨迹集合采样轨迹采样，本质上是通过在解空间随机布点，然后贪婪搜索，筛选最低cost function value的点，即为可行轨迹。 横向偏移量由动力学方程可由纵向偏移量唯一决定，即从控制方程上，每个scenario可以变成由纵向偏移量当唯一自变量的控制方程，从而保证有唯一最优解或者无解。但实际上，由纵向偏移量唯一表示的控制方程，需要满足不同scenario的约束条件，即一个带约束条件的优化问题。随机算法是一个鲁棒性很强的方法，而且大部分情况下，至少可以得到一个可行解。apollo中横向轨迹、纵向轨迹分别采样。 apollo的横向轨迹的采样，由内部自定义三种横向偏移量： -0.5， 0， 0.5； 同时，设计到达这些横向偏移量的纵向位移： 10， 20， 40， 80. 两层循环即可得到一个轨迹集合。由此定义的横向轨迹不一定符合运动学、动力学约束。 apollo的纵向轨迹的采样，考虑巡航、跟车、超车、停车等scenario。 比如，巡航场景，通过两层循环采样。外层循环将速度从零到速度上限按等间距均匀遍历，内层循环由1s-8s均匀遍历。对于停车的场景，末状态的速度、加速度是0. ST-Graph横坐标是时刻，纵坐标是障碍车的车头、车尾的位置。st图将描绘障碍车进入当前车道到离开当前车道所占据的位置区域。自驾车必须确保与障碍车所占据位置有重合。跟车，即在被占区域的下方，或超车，即在被占区域的上方。 由跟车或超车，即可采样各个时刻的纵向偏移量，然后同理横向偏移量采样，形成纵向轨迹集合。 轨迹cost轨迹规划的约束条件： 达到目的、 横向偏移代价（尽量沿着道路中心行驶）、碰撞代价、符合交规、平稳舒适（横向加速度代价）、 纵向加速度代价（激烈加速）、向心加速度代价（转弯场景）。 对于换道场景，对当前车道、目标车道的参考线做一次采样，并找到最低cost的轨迹。可以设计换道cost 。 EM plannerexpectation maximum(EM) ， 最大期望算法，在概率模型中寻找参数最大似然估计或最大后验估计，其概率模型依赖于无法观测的隐性变量。 首先，计算期望（e)， 利用对隐藏变量的现有估计值，计算其最大似然估计值；然后，最大值m， 最大化在e步上求的的最大似然值来计算参数的值。step-2 的参数被用于下一个 step-1 的计算。 em planner 会迭代对路径path、速度velocity进行优化。基于当前步的path和对其他运动物体的预测，优化当前步的速度；然后基于这个新的速度，再优化path。迭代直到收敛。 EM Planner 相比 Lattice Planner，利用了更多当前时间步的状态信息，在每个local timestep 都得到一个可行解。一系列的局部解，相当于弱形式，如果解空间存在可行解，局部解的迭代最终也将给出系统的可行解 12345bool Plan(const common::vehicle_state::VehicleState &amp;vehicle_state, const bool is_on_auto_mode,const double publish_time, std::vector&lt;common::TrajectoryPoint&gt;]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[apollo control module]]></title>
    <url>%2F2019%2F03%2F02%2Fapollo-control-module%2F</url>
    <content type="text"><![CDATA[纵向控制通过控制刹车、油门实现对车速的控制。由一个级联控制器和标定表构成。级联控制器包括：位置PID闭环控制器， 速度PID闭环控制器。标定表即速度-加速度-刹车-油门命令 标定表。 123456Status LonController::ComputeControlCommand( const localization::LocalizationEstimate *localization, const canbus::Chassis *chassis, const planning::ADCTrajectory *planning_published_trajectory, control::ControlCommand *cmd) 其输出是油门\刹车命令cmd. 位置PID闭环控制器输入变量：期望位置 + 当前实际位置 输出变量：速度补偿量 速度PID闭环控制器输入变量：速度补偿 + 当前位置-速度偏差 输出变量：加速度补偿量 速度-加速度-刹车/油门命令 标定表输入变量： 加速度补偿量 + 规划驾速度， 车速 输出变量： 油门／刹车控制量 1234void LonController::ComputeLongitudinalErrors( const TrajectoryAnalyzer *trajectory_analyzer, const double preview_time , SimpleLongitudinalDebug *debug) CarSim/Simulink 与 apollo 连结 ？ 横向控制通过调节方向盘转角实现对航向的控制，由一个前馈控制器和反馈控制器组合，实现车辆动力学模型和lqr 模型。 12345Status LatController::ComputeControlCommand( const localization::LocalizationEstimate *localization, const canbus::Chassis *chassis, const planning::ADCTrajectory *planning_published_trajectory, control::ControlCommand *cmd) 前馈控制器输入变量：道路曲率 输出变量：方向盘前馈控制量 前馈控制变量实现补偿道路曲率对稳态误差的影响。 反馈控制器输入变量： 期望航向角 输出变量： 方向盘反馈控制量 12345678910111213141516171819// update status matrix void LatController::UpdateStateAnalyticalMatching( SimpleLateralDebug *debug)// cal lat error double LatController::ComputeLateralErrors( const double x, const double y, const double theta, const double linear_v , const double angular_v, const TrajectoryAnalyzer &amp;trajectory_analyzer, SimpleLateralDebug *debug)void LatController::UpdateMatrix() void common::math::SolveLQRProblem() // gain matrix steer_angle = steer_angle_feedback + steer_angle_feedforward 控制模块更新cmd 后，发送给canbus模块。 MPC 模型mpc的实现依赖于过程的动态模型。对时域内，每个当前时刻进行优化，求取每个时刻的最优控制解，从而得到整个时域的优化解。 基于线性化的预测 实际系统状态-控制方程（系统状态变量、控制变量）具有时域非线性（二次项等）。首先对该系统进行线性近似，从而可以实现通过当前状态变量、控制变量对下一个时刻系统状态和控制的预测。 滚动优化 设计符合约束条件的目标函数，为状态、控制的离散的能量二次方程。优化的目标就是在每个时刻，寻找最优控制变量，使得目标函数最小。MPC相当于给出每个时刻的局部最优控制值，整个时域的最优控制就是一个控制序列。 控制更新 每个优化的输出即每个时步的控制增量，用于更新当下时步的系统控制变量。 LQR 模型lqr 给出时域系统的全局最优解， 其目标函数是积分函数。相比较，lqr 相当于加权伽辽金方法，给出弹性体的全局最优解，对应强形式的控制方程，解空间非常狭小，甚至不存在。而mpc 相当于有限元方法，给出弹性体每个单元的局部解，对应弱形式的控制方程，每个单元的解空间span可张。理论上弱形式在极限情况给出的解就是强形式的解。 数值方法上，lqr相当于直接法，mpc相当于迭代法。直接法给出控制方程的唯一真实解；迭代法给出近似解，但近似解可收敛到系统的真实解。 车辆模型运动学模型给出车辆纵向控制变量，即车辆模型刚体质心参考点及航向角的控制方程；动力学模型给出车辆横向控制变量。 由运动学和动力学模型给出了整车状态-控制方程，采用mpc or lqr 算法求解控制变量，即实现车辆控制。]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自驾初创团队]]></title>
    <url>%2F2019%2F02%2F26%2F%E8%87%AA%E9%A9%BE%E5%88%9D%E5%88%9B%E5%9B%A2%E9%98%9F%2F</url>
    <content type="text"><![CDATA[name CEO start-date business funding 宏景智驾 刘飞龙(北美oem 背景） 2018-8 L4 Tier1供应商 天使轮 ¥4千万 赢彻科技 翟学魂 2018-9 临港物流 - 拓疆者 隋少龙（北美背景） 2018-4 无人挖掘机 - 驰加科技 王明彦（国内oem 背景） 2017-10 低成本后装解决方案 - plus ai 刘万千 2017-3 AI 方案？ - 纽励科技 徐雷（特斯拉） 2017—1 视觉方案 ？ - momenta 曹旭东 2016-7 视觉方案 c轮 $2亿 极木科技 祁卫（亿航） 2016-7 机器人方案 - 驭势科技 吴甘沙(intel) 2016 芯片方案 ？ - 易航智能 陈禹行（吉大） 2015-8 汽车动力学、控制软件 b轮 ¥2.2亿 青飞智能 孙一飞（上交） - 园区短距自驾解决方案 天使轮 ¥千万 环宇智行 李明（武大） 自驾解决方案 天使轮 ¥千万 行深智能 安向京 - 自驾整体解决方案 - 畅行智能 张祖峰（苏州清华） 物流自驾解决方案 天使轮 ¥千万 图森未来 陈默 2015-8 自驾卡车、视觉、ai d轮 $0.95亿 纵目科技 唐锐 2013 环视adas供应商 c轮 ¥1亿 禾多科技 倪凯（清华） 2017-3 L3.5 自驾解决方案 a轮 $千万 牧月科技 杨庆雄（景驰） 2018-6 - 天使轮 ¥5千万 深兰科技 陈海波 2012-8 ? a+轮 ¥3亿 武汉光庭 朱敦尧 2011 智能网联 上汽并购 主线科技 张天雷 2017-3 港口卡车（天津港） - 创昂tron 邓恒（国内oem 背景） 2018-4 天使轮 $百万 畅风加行 ？ 2018-4 L3 解决方案 - 小马智行 楼天城 2016-12 ai ? a+ 轮 智行者 韦忠亚 2015-5 ？ b+ 轮 鹰驾科技 郑智宇 2015-8 adas 视觉软硬件 a轮 ¥千万 盟识科技 邱长伍 - 矿山、港口、园区自驾解决方案 - 飞步科技 何晓飞（滴滴） 2018-4 车载 ai 系统 - 文远知行 韩旭（百度） - - a+$千万 拿森电子 陶喆 2016-3 汽车电控 b轮 $1亿 auto brain 李明喜 2017-5 自驾解决方案（长城合作） - 踏歌智行 余贵珍（北航） - 特定场景自驾解决方案 pre-a ¥千万 云天励飞 陈宁 - 视觉 ai 芯片 - 有光科技 朱积祥 - 图像方案 - 云洲智能 张云飞 - 无人船 c轮 ¥4亿 西井科技 谭黎敏 - ai 芯片，智能医疗、港口 自动驾驶供应链中，ai算法、芯片、传感器（毫米波、激光、相机）等创业公司不计，传统主机厂、供应商内部相关团队不计。独立的小股团队也不计。直接把自动驾驶解决方案当作团队vision的有将近40家公司，分别来自电子芯片、人工智能、传统主机厂等不同背景。 每年估计烧十几个亿，市场还是有很多热情。2019年也许会死掉很多。另外，对市场时机需要敏感。几个海龟团队都是2018年才开始的。]]></content>
  </entry>
  <entry>
    <title><![CDATA[apollo self driving car]]></title>
    <url>%2F2019%2F02%2F22%2Fapollo-self-driving-car%2F</url>
    <content type="text"><![CDATA[apollo self-driving carHD mapswhere to define the center-meter fidelity level maps, with landmarks, and even the height dimension as well, which is used as the guiding map and also the global coordinate to locate the vehicle in the world. how to keep HD maps updated is a big invest. Baidu has hundreds service cars in China to collect the natural highway HD map data, even still to 2020. LocalizationSLAM tech is a robot maping the world at the same time localizing itself in the world. with HD maps prepared first, self-driving car only need localize itself at every timestep. and the common idea is comparing a few local landmarks with the corresponding global landmarks, then transfer the local vehicle position to its global position in HD map. LiDAR localization either based on the cloud points matching from continous timestamps; or calculating the error between the LiDAR points with the HD maps points; or based on the Karman filter, which give the highest possibility of the location of vehicle always accessible but not easy to construct, especially requiring HD maps. Visual localizationbased on particle filters, which give the most likely location of the vehicle. Perceptiondetection &amp;&amp; classificationwhere the object located, and classify it. detection CNN to find the object in the image; then use classification CNN to classify it. or use a combined CNN to detect and classify at same time. trackingtracking helps when detection failed; also tracking helps to identity the object, so when objects overlapped in the image, still can tell which is which. Predictionmodel based predictiondata driven predictionafter prediction, then generate the trajectory. Planningtransfer the world to graph, and find routings in the graph. routing is a global path, comparing to trajectory is the local way. Vehicle controllinear quadratic regulator PID model predictive control]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[where am I in next 5 years(6)]]></title>
    <url>%2F2019%2F02%2F14%2Fwhere-am-I-in-next-5-years-6%2F</url>
    <content type="text"><![CDATA[things changes faster than I expected, already in China for two weeks. The first few days with family, that was precious, and the last spring festival at home was 5 years ago. life in ChinaI am searching jobs in China now, the recruiters are highly efficient. almost in 24hrs, a reply return; and they prefer wechat rather than email, Liepin ranther LinkedIn. on the other hand, I went to tongji hosptial at Wuhan for a doctor appoitment, but stay in the line waiting most time. People are everywhere, even in a third-class city, like my hometown, I can only find two caffe shop to sit down and use the free wifi; the skyline buildings are everywhere, the evening scene feel like in the movie: Matrix. in USA, I never touch the real China, which is attracting but concerned. As many of my friends in USA, we always talking about China, but never take a move back to China. Till the moment I stand in China, things are real. In general, if people looking for a stable middle-class, USA definitely is a better choice, with few career pressure, 8 hrs a day, 20 days PTO a year, large house with green grass, and income is about 2 times highly as the same engineering in China, without considering 996 in China. people back to China must think big, rather than looking for a stable income. self-driving in ChinaOEM level: Changcheng, Audi, Volkswagen, Geely, xiaopeng, Beiqi, and many names I few heared before, are investing in new energy, intelligent vehicles, and self-driving cars and expending, the market sounds high to hell. there are about 500 start-ups focusing on component supply chain, e.g. in-vehicle sensors, network communication infrastructure, platform solutions, AI algorithms to chips design, specilized scenario applications(mining, seaport, airport, logistics, warehousing, city bus, city-cleaners) it was the best of times, it was the worst of times. many start-up companies will die so sure, but the market is the training course for next BIG. The second half of 2018 is my time to feel everything is accelerated.]]></content>
  </entry>
  <entry>
    <title><![CDATA[review planning pipelien in self-driving car]]></title>
    <url>%2F2019%2F01%2F22%2Freview-planning-pipelien-in-self-driving-car%2F</url>
    <content type="text"><![CDATA[when understanding planning in self-driving car, all other modules make sense. perception layerwith all concepts such as lidar, radar, camera, sensors, GPS, CAN, data fusion, computer vision, SLAM, AI-based detect/tracking algorithms, they are all perception related, which helps the car to understand itself and the surroundings. perception first helps the car understand itself located in the world; then helps to predict the intensions of other vehicles/pedestrains around. behavior layerthere are two steps here: behavior predictation, behavior planning. based on the prediction info of other agents intensions from previous timestep/configuration, the behavior predictation module predicts current behavior of the self-driving car, which usually implemented either by model based methods or data driven method (AI-trained), and which output all the possible feasible maneuvers for current timestep. the car choose only one maneuver at each timestep/configuration, and the behavior planning module is used to weight all the feasible maneuvers from the behavior predictation, and find the most-likely maneuver, which usually is implemented based on a cost function with constraints. the all possible manuever is also called trajectory planning, and the choosen manuever is also called motion planning, which is locally-space and time-depended. control layersince motion planning, then send the command to vehicle control actor and update the car physically. end-2-end motion planningdeep learning is also used to demo end2end motion planning. e.g. from camera output to vehicle control output frame to frame, while many situations may not be trained in the model, so not that realisty. path/routine planningthe motion planning pipleline above is happening every timestep for self-driving car and locally. at high-level is path/routine planning, bascially given the start point and destination point. there are a few algorithms, like global graph search, random tree, incremental graph search. reinforcement learing in simulatorprevious blog, it is also popular to learn behavior in simulation environement and train with reinforcement learning. simulation in self-drivinghow simulation tool chain can accelerate self-driving development ? usually simualtion enviornemnt can help to verify and test the perception, behavior, control algorithms. if working with reinforcement learning, a virtual simulator is required also. what else ?]]></content>
      <tags>
        <tag>self-driving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[where am I in next 5 years]]></title>
    <url>%2F2019%2F01%2F21%2Fwhere-am-I-in-next-5-years%2F</url>
    <content type="text"><![CDATA[I love reading autographies, especially these with life-time stories. it’s a window to explore kinds of life possibilites. and recently I readed &lt;&lt; the only girl &gt;&gt;, Robbin at Rolling Stone magazines in 1970s. I remember some toughts about life meanings. there are three kinds: 1) to crazy satisfy yourself, or pursue a free life, live the way you really want. e.g the freelancer; 2) ladder up to the higher social class, which is happening a lot in developing countries, and to be rich and success is the guarantee; 3) enjoy the beauty of nature and civilazation in this short period of life time. the last two meanings can not be done immediately, to crazily or desperately satisfy self, is then the easiest way to fight against the fucking environment. like when getting tired of this busy world, I behave more like a hippie, no big deal to anything, and kill time on drama, books, or drugs or sex. the hippie lifestyle is momental, it creats a space to seperate from daily troubles; after a few days or nights, I am back to real life, continuing on career stuff. so take it, the reality for me, as born in Asian developing country, with traditional family responsibility, is pursuing career, and of course it helps to take a break, and enjoying hippies, but don’t take it as the life-time way. mostly of my inner voices are: either busy to live or busy to die; problem-solving skills, leadership; to figure out how the economy machine functions in small and big roles; and be the smart guy in market and at office. and no need with another degree or matured platform to be problem-solvingable or leadershipable. these skills are in everywhere.]]></content>
  </entry>
  <entry>
    <title><![CDATA[未来五年在哪里（3)]]></title>
    <url>%2F2019%2F01%2F13%2F%E6%9C%AA%E6%9D%A5%E4%BA%94%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C%EF%BC%883%2F</url>
    <content type="text"><![CDATA[在美国开长途，沿途总是遇见wendy’s, McDonald’s， auto zone 等等连锁店。也不论在toledo等中部小城市，还是芝加哥、纽约等国际大城市，这些店也到处都有。甚至costco商店在底特律，布法罗，哥伦布，三番等城市的商品布局摆放都是一摸一样的。 中国很少有这种在从三四线城市到一线城市都连锁的服务品牌，除了重资产的，比如商业地产（万达广场）， 酒店，大型购物超市，阿里仓库，烟草局等。而餐饮、汽修、家装等等基本都是地方特色。 美国各州相对很独立。比如，buffalo wings 从布法罗开到了全美国， kfc 从肯塔基开到了全美国。把产品、模式从一个地方复制到另一个地方很容易实现。而且不仅产品本身，其背后的供应链也同样反映了平坦。所以是整个体系很成熟，容易实现规模效益，从而正向强化成功。 中国地方性壁垒比较高，一个成功的产品、模式难轻松复制。好处是发展了很多地方性的特色服务。当然对资本而言是不经济的。好奇麦当劳进入中国，是怎么做供应链的，如何在不规范、体系不成熟的环境下复制美国模式的。 一个顶级成功的商业，应该是建立系统。然后，利润就在系统运营和管理中流淌出来。就像顶级的造车公司，是能够把研发、生产、销售等subsystem打通，建立一个造车system的公司。这也是matured market 的特征，系统成熟运作，利润稳定地流淌。emerging market 就处于探索建立系统的过程。一般是从新科技、产品subsystem出发，因为商业模式、市场等subsystem比较好从已经有的成熟体系里借鉴，然后融合、打通。 中国出现了两种特有的经济行为：外卖和淘宝。他们的价值就是打通原本因为地域壁垒很难复制的产品。互联网不存在地域壁垒，所以有效最优化资源，并实现规模效益。相比，美国不存在大范围的外卖、淘宝，因为产品复制在地域上很容易。 商业的本质，就是建立一套稳定运转的系统，不论中美。因为美国的资本、商业环境、企业规范、地方行政等方面的成熟，即游戏规则已经稳固且清晰，成功是更容易的。相比，在中国，就需要官商结合重资本打开局面。因此在中国，赚大钱更容易。]]></content>
  </entry>
  <entry>
    <title><![CDATA[未来五年在哪里(2)]]></title>
    <url>%2F2019%2F01%2F04%2F%E6%9C%AA%E6%9D%A5%E4%BA%94%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C-2%2F</url>
    <content type="text"><![CDATA[很多汽车公司（供应商，oem）都有大量职位招：CAN, control, EE 背景的同学。相反，cae产品却很少出现在供应商的招聘中。先给个预判：汽车电子处于草莽时代。 汽车电子首先，汽车电子，不论硬件、软件，都很面向产品。出来的直接可以拿到市场去卖。另一方面，这类产品又很成熟，不论是电子元器件，汽车网络，摄像头等传感器，控制算法都不是新技术，而更像是一次“组合创新”。基于已有部件，在新的应用场景，重新组合，研发需求本身很低。 即使前沿应用场景，比如自动驾驶，其实是需要重新定义很多基础部件，包括通信协议、ai算法、传感器的融合、路面设备等，必然有很多研发需求，但目前的自动驾驶大多在现有adas基础上的迭代，还没有真正的超越。 另外，因为各个终端产品的差异性，市面上会出现各种解决方案。这是一个行业的草莽时代，各家都自立山头。还没有出现几家鼎力。 由于应用层的需求，而不是研发需求，汽车电子需要熟练的软件/硬件工程师去拼装产品。这也是ME, EE偏控制方向的同学，在汽车行业就业的窗口期。 汽车CAECAE行业的黄金期是80年代到2000年，当时也是各个解决方案在市场上乱飞，甚至应用商（汽车厂等)内部都有独立的CAE开发团队，而且做了很牛逼的产品。那个年代，CAE算是emerging tech，很吸引投资和就业。到现在，这样的窗口期已经结束，北美几大CAE厂商三足鼎立： 加州msc, 密西根altair, 匹兹堡ansys，还有一些小众厂商。还有就是更大只的PLM厂商： 西门子， 达索等。 整个行业处于成熟期。产品的研发、市场、客户都已经形成了模式。应用行业也基本稳定，民用品：汽车、电子消费品、医疗等，军用：航空航天、核物理等。当然，未来会拓展新的应用领域，以及需求新产品研发。 虽然整个行业处于成熟期，进入门槛很高了。实际上，如果不在大厂里，CAE还是辅助的。有接触到设计制造供应商，都会使用CAE分析，但经常是Design engineer 顺带跑一下cae分析。也可以看到供应商更需要直接面向产品的设计工程师，而不是CAE应用工程师。 大厂里面有CAE研究部门，尝试新的分析场景；CAE支持部门，包括部署、debug、用户支持；以及CAE工程师。]]></content>
  </entry>
  <entry>
    <title><![CDATA[未来五年在哪里]]></title>
    <url>%2F2018%2F12%2F21%2F%E6%9C%AA%E6%9D%A5%E4%BA%94%E5%B9%B4%E5%9C%A8%E5%93%AA%E9%87%8C%2F</url>
    <content type="text"><![CDATA[两种思维不同的思维反映了不同的背景。面对一个商业想法，一个对细节严肃的工程师，首先会估量自己对讨论的领域是否足够熟悉，然后会考虑技术上是否可行。马云说他是阿里巴巴最不懂技术的人，却能够把握大方向；相反往往问题没解决，责怪技术不够的，会陷入用一个技术解决另一个技术的游戏中，而远离了大方向。 张首晟教授有分享在知识信息爆炸的时代该怎么学习。因为最优秀的书籍也已经多到一个人一辈子都读不完了，所以重要的是抓住道(principle)。举个极端的例子，如果地球毁灭，最后一个人只能带1kb 的信息离开地球，这个信息该是什么，能够保证人类文明不断流。 道与术的例子， 日本远海70公里发生了地震，多久巨浪会到达海岸线？一个细节严苛的学流体力学仿真的研究生，要跑一套cfd程序，考虑近似若干边界初始条件，然后花一两天调试，运行得到结果。而一个有物理常识的人，大概能估计几个跟巨浪速度有关的变量，就可以得到一个近似结果。 我的经验就是太容易过分依赖技术（本质是一种惰性，想回避该用人脑的分析），而忘了新技术的初衷，单纯为了迭代技术本身而发展新技术。世界正在把太多新技术太快速地塞进普通人的视野，不要说思考这些新技术到底意味什么，甚至这些新技术到底将如何产业化，进入普通人的生活都没有清晰。 很长时间，我对工程仿真技术有这样的误区。我已经忽略了cae本身要解决的问题，而只是沉迷在cae技术本身。或者说是，从研究（学生）到商业（职场）的角色没有转变过来。而商业需要的是一个能够打穿新技术与产品落地。他／她可以不懂技术的深度，但一定有广度和连结，能觉察到新技术可以在哪里发挥价值。 越来越感受到，思维会决定行为，而具有倾向性的行为会强化思维。一个没有商业经验的且受过高等技能教育的年轻人，会倾向强调技术，放大其重要性，而看不到其在企业经济大方向上的不合理性。比如，工程师倾向使用最新的技术在产品开发中，但企业定位可能是一款廉价快速打入市场的产品。这时候，在工程师看来完美的合理方案，于企业战略就是不合理的。 emerging market emerging matured platform start up large business management new matured involving multi-role single-role stability high-risk low-risk working env shining mediocre creativity knowledge-based manage-based 由新知识引导的商业创新，比如，生物科技，基因技术，新材料，量子计算，人工智能，核物理等，首先很容易给人带来极大的快感，特别让年轻人欲罢不能， 更吸引投资、社会关注。但是，emerging tech存在周期性：从新技术到产品的会有很长的空窗期。历史上电气时代（1830 - 1860年），在爱迪生开发电灯泡之后，数百家电气公司迅速出现，但20年内大部分被淘汰掉；铁路公司，PC电脑, 汽车制造，消费电子，包括最近的AI、自驾车行业，都体现着emerging market的规律：早期快速极度膨胀，然后几年内极度跌落，再慢慢恢复理性。 经济学上还有一个概念，就是新技术带来的就业远不能弥补旧行业消失造成的失业。时间久了，emerging market也会成为matured market。 团队选择emerging tech非常强调先发优势。很多新知识引导的商业创新，源自学校研究第一梯队，他们本身就是某项技术的原创团队，具有极强的先发优势，而且因为新知识门槛，被市场快速复制的可能性极低。比如，机器人、柔性可穿戴电子器件、基因检测、ai等等。 普通人的第一次团队选择，大多是在高校完成的。聪明人跟对了团队，在博士期间就可以引领emerging market。《新资本论》中提到，21世纪，劳动力与资本收益分配是1:7。 稍微感受下，知识经济将明显比资本有更高收益。 如果错过了第一次团队选择，普通人很难有后发优势，没钱没技术。所以，除非对该领域具有极强的认同感，执着的信仰，否则想在创业团队谋一份职业，可能是误判。 现实与认知之间的矛盾，往往反映一个人还没成熟。我的假象包括：创业团队更锻炼人，更吸引眼球，站在风头浪尖上，然而没有一个因素是因为我对该领域的认同感，不过是渴望它附加的虚荣。另外，个人成长动力不该仅源自环境，而必须是内心。环境在满足基本需求后，只叫人不讨厌它了；但爱上它，必须是从心发出的。 普通人的第二次团队选择，大多是在职场完成的。明确自己该要什么，不该要什么，也许对普通人选择matured market更友好。一方面是老生常谈的大企业病：成熟平庸。但是，创新成功并不都是声名噪动，也有闷声发大财。 成熟行业或成熟企业的创新，也许不是产品／技术本身，而在体系和管理。 而在一个成熟行业待不少于5年，才有可能了解系统，接触到管理。写到这里，不由又盘算要不要读MBA了，正好遇到明年经济糟糕。 – 感谢Selena]]></content>
  </entry>
  <entry>
    <title><![CDATA[insertion sort list]]></title>
    <url>%2F2018%2F12%2F18%2Finsertion-sort-list%2F</url>
    <content type="text"><![CDATA[leetCode 147, this is another example when solving problem, either find a good data structure, which looks like stand in a higher dimensions with additional DOF, or define the routines on the given data structure. additional set123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657struct classcomp &#123; bool operator() (const ListNode* lhs, const ListNode* rhs) const &#123;return lhs-&gt;val &lt; rhs-&gt;val;&#125;&#125;; class Solution &#123;public: set&lt;ListNode*, classcomp&gt; labels ; ListNode* insertionSortList(ListNode* head) &#123; while(head)&#123; labels.insert(head); head = head-&gt;next ; &#125; head = labels[0]; ListNode* node = labels[0]; for(int i=1; i&lt;labels.size(); i++)&#123; node-&gt;next = labels[i]; node = node-&gt;next ; &#125; return head; &#125;&#125;;``` basically `unordered_set` has ordering by default, so how about first inserting each element from the list to set, then set the next pointer for each element, which take additional space, but did the job straight forward ## single linked listsince single linked list can't access the previous node, so define prev pointer and current pointer, and every insertaion needs do the compare from frist element to the previous element. ```c public class Solution &#123; public ListNode *insertionoSortList(ListNode *head)&#123; ListNode *result = nullptr ; if(head == nullprt || head.next = null)&#123; result = head ; return result ; &#125; ListNode *dummy = new ListNode(0); ListNode *cur = head ; ListNode *next, *pre; while(cur != nullptr)&#123; next = cur-&gt;next ; pre = dummy ; while(pre.next != nullptr &amp;&amp; pre.next-&gt;val &lt; cur-&gt;val)&#123; pre = pre.next ; &#125; //return the position where cur node should insert cur-&gt;next = pre-&gt;next ; pre-&gt;next = cur ; cur = next ; &#125; &#125;]]></content>
      <tags>
        <tag>leetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lru cache design]]></title>
    <url>%2F2018%2F12%2F18%2Flru-cache-design%2F</url>
    <content type="text"><![CDATA[leetCode 146, for non-CS background, this kind of problem is out of mind, what is talking about, but I know at all it is about design a data structure to meet some requirements: first a hash map data structure for key, value operations, e.g. get(), put(); but normal hash map can’t track the most recent used element, so either find a data structure, which can track the recently used element in the hash map; or define some routines to do it. a data structure to catch recently usedeach get(), put() will operate on one special element, and that element should be marked as recently used, and the next element is the least recently used element, which can be overlapped in next operation. stack and queue have some property to track the most recent element, but they are one-way, not straight forward to manage two neighbor elements. double linked list is a good choice to track both the most recent element and the least most recent element. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950struct node&#123; int val ; int key ; Node *pre ; Node *next ; public Node(int value_, int key_) &#123; val = value_; key = key_ ; pre = NULL ; next = NULL; &#125;&#125;class LRUCache &#123; public: list&lt;node*&gt; recent ; unorder_map&lt;int, node*&gt; hash ; int capacity ; LRUCache(int cap): capacity(cap) &#123;&#125; node* get(int key)&#123; if(hash.find(key) != hash.end())&#123; recent.remove(hash[key]) recent.push_front(hash[key]); return hash[key] ; &#125; return -1 ; &#125; void put(int key, int value)&#123; if(hash.find(key) != hash.end())&#123; recent.remove(hash[key]); hash[key] = value; recent.push_front(hash[key]); &#125; node* tmp = new node(value, key); hash[key] = tmp ; recent.remove(tmp); recent.push_front(tmp); if(hash.size() &gt;= capacity)&#123; node *tmp2 = recent.back(); hash.erase(tmp2-&gt;key); &#125; &#125; &#125; routines to track recently usedin this way, the hash is still needed, but define routine to set the latest updated node always at front. void setHead(Node* n){ n.next = head ; n.pre = null ; if(head != null) head.pre = n ; head = n ; if(end == null) end = head; //only 1 element } when solving real problems, either has a good understand about special data structure, which can solve this problem, or the raw way is to implement routines based on the exisiting input]]></content>
      <tags>
        <tag>leetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[where are you in next 5 years (4)]]></title>
    <url>%2F2018%2F12%2F18%2Fwhere-are-you-in-next-5-years-4%2F</url>
    <content type="text"><![CDATA[in the nude“only after the tide has faded, know who is in the nude”, the wise said. distributable resources(social relationship, capital, core tech, market vision) define the success of a man in the world. in normal days, people feel good about themselves, like they can take things in control and looks well, these factors are ignored, and crisis is under the hood; when situation changes, the crisis appears, then people feel unexpected surprise. how crisis is so close along with us for a long time, and get igored for a long time ? another word, “standing in the wind, elephant can fly to the sky”, when market is good, normal people make money from stock. it is not him become smart, worse he is not prepared for the coming bad times. with no money, no social relations, and no core tech standing in the world, I should be humble to dirt, rather talking like everything is in control. for many young immigrants, the breakthrough is through education and learn core skill, and earn money and gradually being successful. this evening, watched Jack Ma speech, some ideas: in age 20 - 30, the best thing is to find a good boss and learn from him or the team; from age 30 - 40, try something for yourself, either to start a business or go for a art design degree; from 40 - 50, make sure to do things yourself most good at. life is previous and short, no waste. find out my contribute to the world, and do it. the right thingI shared my thoughts with a friend, that I thought AV was the future in vehicle industry, so even though I had no experience in control algorithms, computer vision, sensor hardware, I will find a way in; on the other side, I am pretty familiar with CAE software products development and applications, how I choose ? my friend said, neither. job is to make money. Have to say, I am still focusing too much on the strong or the weak, the right or wrong. spent too much energy to find the line to divide future into simple right choic and wrong choice. I remembered another good friend, said I am used to pre-define or label people, but forget every similar story could have many different reasons and results. if there is a unique right thing in the world, then everyone must follow it, but since the right thing/decision for each person is different, even though the world drive people to be similar, I should realize in every situation, every step, it is ok to have a different right thing, rather than holding on the unique rightness]]></content>
  </entry>
  <entry>
    <title><![CDATA[word break with dfs & bfs]]></title>
    <url>%2F2018%2F12%2F17%2Fword-break-with-dfs-bfs%2F</url>
    <content type="text"><![CDATA[leetcode 139 dfstraversing string ,and split it one char by next, if find the splited left word in dict, recursiving the remaining right substring. in this way, it will make sure find a solution, but also traversing all unnecessary possibilites. and need design a class variable to stop the dfs() 123456789101112131415161718192021bool found = false;bool dfs(string s, unordered_set&lt;string&gt;&amp; dict)&#123; string left, right ; if(s.size() == 0 || s.empty()) &#123; found = true; return found; &#125; for(int i=0; i&lt;=s.size(); i++) &#123; left = s.substr(0, i); right = s.substr(i, s.size()-i); if(dict.find(left) != dict.end()) &#123; dfs(right, dict); &#125; &#125; return found;&#125;; bfsas mentioned above, dfs goes to all possibile branches, which is not required. bfs is like a greedy way, to find out one working solution and done. also need keep track of searched left sub-strings, since the remaining right sub-strings may can not find a matched from dict, then the left sub-string is not acceptable, so the left sub-string need move one char further. 1234567891011121314151617181920212223242526272829303132 bool bfs(string s, unordered_set&lt;string&gt;&amp; dict)&#123; int len = s.size(); string s1, q, left, right; queue&lt;string&gt; sq; sq.push(s); string tmp ; while(!sq.empty())&#123; tmp = sq.front(); sq.pop(); int i=0; s1 = s ; while(!s1.empty() &amp;&amp; i &lt;= s1.size()) &#123; if(tmp == s)&#123; left = s1.substr(0, i); right = s1.substr(i, s1.size()-i); &#125;else &#123; left = s1.substr(0, i+tmp.size()); ///will dead-cycle right = s1.substr(i+tmp.size(), s1.size()-i-tmp.size()); &#125; i++; if(dict.find(left) != dict.end()) &#123; q += left ; sq.push(left); s1 = right; i = 0; &#125; &#125; &#125; this bfs is not good design.a pretty simple sample code]]></content>
      <tags>
        <tag>leetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[where are you in next 5 years(3)]]></title>
    <url>%2F2018%2F12%2F15%2Fwhere-are-you-in-next-5-years-3%2F</url>
    <content type="text"><![CDATA[at the end of 2018, I was lay off from GM after only a couple months. in the middle of 2018, I was planning to jump out from the traditional CAE group, to try the new autonomous vehicle. but a few things I missed when made that decision: the big environmenthow long will the traditional automotive industry stay in a sort of good shape? the answer now is not long. from Oct 2018, both Ford and GM publicly lay off over 15%, the vehicle market at both China and NA were significantly down from Q3; even earlier the manufactoring robotic supplieres mentioned their 2019 order were almost zero. so layoff employees is almost destined. in economics, consumer index is the reason, employment/unemployment rate is the result, not the reverse. and there is a delay from consumer index to unemployment, about 3 ~ 6 month. so when the market is done, a few month later, the unempolyment rate will show up. an even biggger environment, the trade war among China and USA raise the cost, fear both the consumers and the investors. not only vehicle market, consumer electronics (e.g. apple), internet(e.g. Google), retail industry(e.g. Alibaba) and as closely related to lologistics, chips, energy, housing, finance all go down. the winter is coming. Sooo scary, hate Trump ! new techs in vehiclefor a new stuff in market, the trend is starting extremely high, then low to bottom, then go to rationality. it happens to AI, autonomous vehicle(AV) market. starting from 2016, billions invest in, every step looks promising, revolution. but 2 years now, even Waymo said it can’t make AV in bussiness. on one way, OEMs can’t be lay behind, so buy startups, on the other way, they can’t all invest in AV, which has no mature market strategy. the more invest, the higher risk. for people who want to eat this AV cake, cold cold. how to face the changing worldhave to say, there is no well-prepared in this quick-changing world, plan is always out of date. at the middle of year, I thought I made the right decision to leave Ford CAE group, join GM autonmous group, now put myself in a poor situation. I usually have a mind, that this thing has to, must to be done in this and only this way; this situation is and has to be dealed in this and only this way; this person is and has to be treated in this and only this way. I know this is the poor mindset. they say “poor people make poor decisions”, thanks to warn me up. e.g. * so high risk to depend on one single incoming resource * no stable business, keep humble and keep foolish * be positive, be felxible.]]></content>
      <tags>
        <tag>automotive software</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[clone graph with dfs & bfs]]></title>
    <url>%2F2018%2F12%2F14%2Fclone-graph-with-dfs-bfs%2F</url>
    <content type="text"><![CDATA[leetcode 133, the solution is traversing the graph, so either bfs or dfs should work. dfs works like a stack, take an example when processA is running, inside processB is invoked, then processA hang, and go into processB; as processB is running, inside processC is invoked, then processB hanged, and go into processC. e.t.c. so the finished order is from innerest to outest. dfs will always traverse all possibilities, then return to the outest layer. in clone graph, e.g. #0, 1, 2 #1, 2 #2, 2 dfs process as: first deal with first line, element 0, which trick element 1, so process goes to second line, and trick element 2, so go to third line(done), then return to second line(done), then return to first line, trick second neighbor 2 go on… bfs works different, during processA running, processB is invoked, will still run processA to done, but also push processB to job queue, which will run next time. so using bfs, usually has a queue variable to mark unfinished jobs. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263 struct UndirectedGraphNode &#123; int label ; vector&lt;UndirectedGraphNode *&gt; neighbors ; UndirectedGraphNode(int x) : label(x) &#123;&#125;;&#125;;//graph traversing by bfsUndirectedGraphNode *cloneGraph(UndirectedGraphNode *node)&#123; if(!node) return nullptr ; UndirectedGraphNode *c_node = new UndirectedGraphNode(node-&gt;label); queue&lt;UndirectionGraphNode*&gt; qlist ; unorder_map&lt;int, UndirectionGraphNode*&gt; map; map[c_node-&gt;label] = c_node ; qlist.push(c_node); UndirectedGraphNode cur, neighbor = nullptr; while(!qlist.empty())&#123; cur = qlist.front(); qlist.pop(); for(int i=0; i&lt;cur-&gt;neighbors.size(); i++)&#123; neighbor = cur-&gt;neighbors[i]; if(map.find(neighbor-&gt;label)) &#123; c_node-&gt;neighbors.push_back(neighbor); &#125;else&#123; UndirectedGraphNode *tmp = new UndirectedGraphNode(neighbor-&gt;label); c_node-&gt;neighbors.push_back(tmp); map[tmp-&gt;label] = tmp ; qlist.push(tmp); //bfs idea: find new node, push it to queue and deal // with it in next while loop &#125; &#125; &#125; return map[c_node-&gt;label]; &#125; //dfs void dfs(UndirectedGraphNode\* node, unorder_map&lt;int, UndirectionGraphNode*&gt;&amp; map) &#123; UndirectedGraphNode * c_node = nullptr; if(!map.find(node-&gt;label))&#123; c_node = new UndirectedGraphNode(node-&gt;label); map[c_node-&gt;label] = c_node; &#125;else&#123; c_node = map.find(node-&gt;label); &#125; UndirectedGraphNode* nei; for(int i=0; i&lt;node-&gt;neighbors.size(); i++)&#123; nei = node-&gt;neighbors[i]; if(map.find(nei-&gt;label))&#123; c_node-&gt;neighbors.push_back(nei); &#125;else&#123; dfs(nei, map); &#125; &#125; &#125; UndirectedGraphNode *cloneGraph(UndirectedGraphNode *node)&#123; unorder_map&lt;int, UndirectionGraphNode*&gt; map; dfs(node, map); return map[node-&gt;label]; &#125;]]></content>
      <tags>
        <tag>leetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[palinedrome partition with dfs & bfs]]></title>
    <url>%2F2018%2F12%2F13%2Fpalindrome-partition%2F</url>
    <content type="text"><![CDATA[parlindrome partitionleetCode 131 a sample code : 12345678910111213141516void dfs(int depth, string s)&#123; if(depth == s.size())&#123; v.push_back(v1); &#125; if(depth&lt;s.size())&#123; for(int i=depth; i&lt;s.size(); i++)&#123; //left substring if(palindrome(s.substr(depth, i-depth+1)))&#123; v1.push_back(s.substr(depth, i-depth+1)); dfs(i+1, s); v1.pop_back(); &#125; &#125; &#125; &#125; 1) each col in the table is one way to partition the string.e.g. aab. col_1 , left string a is palindrome, v1=[a]; feed right string ab to dfs(i+1, s)(depth=0, i=depth) 1 2 3 a a a a a a b b b 1.1) col1_1, left string a is palindrome, v1=[a, a];feed right string b into dfs(i+1, s) (depth=1, i=depth) 1 2 a a b b 1.1.1) b will push into v1, make v1=[a, a, b], feed into dfs(i+1, s) (depth=2, i=depth), which will return since depth==s.size(). the above dfs()(depth=2, i=depth) returned, the immediate next step is b pop out from v1; also now is the finished of dfs()(depth=1, i=depth), so the immediate next step is the second a pop out. 1.1.2) so now go to (depth=1, i=depth+1), push ab into v1, making v1=[a, ab] . but ab is not palindrome, return immediately, then pop out ab. 1.2) at this time, the very outside dfs()(depth=0, i=depth) is all done, so pop the first a in v1; and go to (depth=0, i=depth+1); then (depth=0, i=depth+2)… recursive of DFSanother example of recursive dfs. leetcode 129 sum root of leaf numbers dfs is used to find all the solutions, in a way to go through each branch in the same level first, then go into the next level. to traverse all solutions(branches), also need to mark traversing/traversed branches. so usually need: to push -&gt; current traversing element to pop -&gt; traversed element and a container to hold all solutions. if design recursive with returned value, consider the critical condition. e.g. what suppose to return when the last element, and usually design the returned value as local variable in each recursive function, rather than a global variable. one more thing is about the traversing direction, from left to right end, then when coding make sure the direction is consistent at all times. BFS to find mini cutsDFS is used to solve the kind of problem: traversal all possible solutions; leetcode 132 askes for the mini cuts, once find out the mini-cuts solution, done, no traverse all possibilities. 1234567891011121314151617181920// define done, cut as class member variable void bfs(string s, bool&amp; done)&#123; int len = s.length(); string left, right; int i =0; while(!done &amp;&amp; i&lt;len)&#123; left = s.substr(0, len-i); //starting from the longest left string right = s.substr(len-i, i); if(palindrome(left))&#123; cut++; if(palindrome(right)) &#123; done = true ; &#125;else &#123; bfs(right, done); &#125; &#125; i++; &#125;&#125;]]></content>
      <tags>
        <tag>leetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[surrounded regions]]></title>
    <url>%2F2018%2F12%2F13%2Fsurrounded-regions%2F</url>
    <content type="text"><![CDATA[leetCode130, like go game. how to design the solution, at first it is naive to traverse with two for loops, then there will be too many if statement to specify, and not clear to seperate these if statements. three ideas behind a sample solution. using addtional status variablein lower space the problem is difficult to analysis, then from a higher space, the problem is clear and easy. e.g. trakcing the O in the board, how to mark them ? keep O will not tell the marked ones and the original ones. so using M to mark these already traversed, it make the situation much more clear starting from the critial domainat first, no idea where is a good start point, but the trick point is at the boundary, only the cells at boundary with O will keep O; all the O not on the boundary either connect to the boundary Os or they will be Xs. traversing with queueif not queue, to detect these O cells need two for loops, then deal with these O cells again need another two for loops. for loop is a global handling, while queue list is a local way, no need to take all cells in consideration, but only the cells in queue. queue should be a good way to traverse whenever the operation is not deal with all the elements. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374//extending from the boundary 'O', any extened will keep same statuevoid process(int i, int j, vector&lt;vector&lt;char&gt;&gt;&amp; board)&#123; int height = board.size(); int width = board[0].size(); std::pair&lt;int, int&gt; p; queue&lt;std::pair&lt;int, int&gt;&gt; Q ; Q.push(p(i,j)); board[i][j] = 'm'; while(!Q.empty())&#123; p tmp = Q.front(); Q.pop(); i=tmp.first; j=tmp.second; if(i!=0 &amp;&amp; board[i-1][j] == 'O')&#123; board[i-1][j] = 'm'; Q.push(p[i-1][j]); &#125; if(i!= width-1 &amp;&amp; board[i+1][j]=='O') &#123; board[i+1][j] = 'm'; Q.push(p[i+1][j]); &#125; if(j!=0 &amp;&amp; board[i][j-1]=='0') &#123; board[i][j-1]='m'; Q.push(p[i][j-1]); &#125; if(j!=height-1 &amp;&amp; board[i][j+1]=='O') &#123; board[i][j+1] = 'm'; Q.push(p[i][j+1]); &#125; &#125;&#125;void sol(vector&lt;vector&lt;char&gt;&gt;&amp; board)&#123; int height = board.size(); int width = board[0].size();//tracking the boundary 'o' and mark them as 'm' for(int i=0; i&lt;width; i++)&#123; if(board[i][0] == 'O')&#123; //bottom boundary board[i][0] = 'm' ; process(i, 0, board); &#125; if(board[i][height-1] == 'O') &#123; board[i][height-1] = 'm' ; process(i, height-1, board); &#125; &#125; for(int j=0; j&lt;height; j++)&#123; if(board[0][j] == 'O')&#123; board[0][j] = 'm'; process(0, j, board); &#125; if(board[width-1][j] == 'O') &#123; board[width-1][j] = 'm' ; process(width-1, j, board); &#125; &#125; for(int i=0; i&lt;width; i++) for(int j=0; j&lt;height; j++)&#123; if(board[i][j] == 'O') board[i][j] = 'X'; if(board[i][j] == 'm') board[i][j] = 'O'; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hash table]]></title>
    <url>%2F2018%2F12%2F12%2Fhash-table%2F</url>
    <content type="text"><![CDATA[the ideas behind hashtable is: 1) input the key to hash function, output the bucket index; 2) adding the element to the bucket[idx]. usually design the bucket[idx] as a linked list, allowing mulitply keys in the same bucket, which is called “collesion chaining”. 3) hashtable has three basic operators: insert/put, search/get, and remove. the memory usage of hashtable is continuous bucket array, then each bucket is a linked list. when first thought about the “continue array”, really is a vector, then why need hash function. That’s the point, hashtable allows the benefits of both vector and linked list. a few interesting topics about hash. 1)why prime numbers? the fundamental mathematical operations with prime numbers generally result in numbers who’s bit biases are close to random. in other word, when multiply or add a set of randome numbers by a prime number, the resulting numbers when as a group, statistically analyzed at their bit levels should show no bias towards being one state or another. in comupter science, pseudo random number generator has bit bias. 2) string hash/scattering,basically the hash functions should deal with each chracter in the input string, so no information about this string will be missed, and the information entropy after hashing operator suppose increase. 3) Blizzard hash function. not all hash function deal collision conflict with linked list, here is the example. 4) hash map benchmark performance review one feeling at this moment, so many brilliant and deep-focusing guys there, stay foolish and stay humble. std::unordered_mapthere is map vs unordered_map : map | unordered_map Ordering | increasing order | no ordering | (by default) | Implementation | Self balancing BST | Hash Table search time | log(n) | O(1) -&gt;Average | | O(n) -&gt; Worst Insertion time | log(n) + Rebalance | Same as search Deletion time | log(n) + Rebalance | Same as search basically map works for traversal, pre/post element access; unordered_map is quick in once search,insertion, deletion. here is a dictionary demo exampleword ladder, hash table is used to find a special word from dictionary. the benefit of unorder_set is O(1) find. so design a unorder_set to store the dict. in the following example, each character position in the word requires 25 times search of the dict. also every marked word from the dict, should not be searched second time. another variable is used to track the list of one_character_diff from current word, it’s like BFS. so design as a queue, which only need take care the neighbor two layers. 12345678910111213141516171819202122232425262728293031323334353637int sol(string start, string end, vector&lt;string&gt;&amp; wordList)&#123; int length = 2 ; unordered_set&lt;string&gt; dict ; foreach word in wordList: dict.insert(word) queue&lt;string&gt; one_diff_list; // initial by pushing start one_diff_list.push(start); while(!one_diff_list.empty())&#123; int size = one_diff_list.size(); for(int i=0; i&lt;size; i++)&#123; string word=one_diff_list.front(); one_diff_list.pop(); for(int i=0; i&lt;start.length(); i++)&#123; char oldChar = word[i]; for(char c='a'; c&lt;='z'; c++)&#123; if(c == oldChar) continue ; word[i] = c ; if(dict.find(word) != dict.end())&#123; if(word == end) &#123; //find the match return length; &#125; one_diff_list.push(word); dict.erase(word); &#125; &#125; word[i] = oldChar; &#125; &#125; length++; &#125; return 0; &#125;]]></content>
      <tags>
        <tag>leetCode</tag>
      </tags>
  </entry>
</search>