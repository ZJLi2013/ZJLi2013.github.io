<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[how broad first search works]]></title>
    <url>%2F2018%2F08%2F29%2Fhow-broad-first-search-works%2F</url>
    <content type="text"><![CDATA[leetCode 45 (jump game): Given an array of non-negative integers, you are initially positioned at the first index of the array.Each element in the array represents your maximum jump length at that position.Your goal is to reach the last index in the minimum number of jumps. e.g. [2, 3, 1, 1, 4] –&gt; 2 solution: at current position, bfs all the next positions it can jump to, when the back element is in next position list, then reached. 1234567891011121314151617181920212223242526272829303132333435363738394041int sol(vector&lt;int&gt;&amp; nums )&#123; vector&lt;int&gt;::iterator it, cur_it ; vector&lt;int&gt; *adj = new vector&lt;int&gt;[nums.size()]; int cur_node ; for(it=nums.begin(); it &lt; nums.end(); it++) &#123; cur_node = *it; adj[cur_node].push_back(cur_node); for(int i=1; i&lt;=cur_node; i++) &#123; cur_it = std::next(it, i); adj[cur_node].push_back(*cur_it); if( *cur_it == nums.back())&#123; it = nums.end(); //reach the last element, so return &#125; &#125; &#125; vector&lt;int&gt; tmp; int last_element = nums.back(); int mini_path = 1; for(int i=0; i &lt; nums.size(); i++) &#123; tmp = adj[i]; if(!tmp.empty()) &#123; for(int j=0; j&lt;tmp.size(); j++) &#123; cout&lt;&lt; tmp[j] &lt;&lt; ' ' ; &#125; cout &lt;&lt; endl; if(tmp.back() == last_element) &#123; return mini_path; &#125; mini_path++; &#125; &#125; return -1;&#125;]]></content>
      <tags>
        <tag>leetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[how recursive works]]></title>
    <url>%2F2018%2F08%2F29%2Fhow-recursive-works%2F</url>
    <content type="text"><![CDATA[leetCode 46 Permutations: given a collection of distinct integers, return all possible permutations. Input: [1, 2, 3] Output: [ [1,2,3], [1,3,2], [2, 1, 3], [2, 3, 1], [3, 1,2], [3,2,1] ] At first glipmse, it’s middle difficult, jump to recursive implementation. But there always confused failures, e.g. passing vector as reference to the recursive calls, and even do a deep copy of that vector, the base vector is always modified by the copied vector. This problem blocks me a whole day, actually I don’t know how to make a recursive calls work. tip1) each recursive fun has its independent local variables ; tip2) recursive calls are serialized, namely only when inside recursive fun is done, then the outside recursive will continue. tip1 helps to explain when local variables changed unexpected; tip2 helps cause I trend to think all recursive calls are parallezed running, but the real running order is serialized, so no twist. simulate how human brain solve this problem: &lt;1(fix), 2(fix), 3&gt; ; &lt;1(fix), 3(fix), 2&gt; … the first position has n potentional values, then the second position has (n-1) potentional values, etc. so design the very first implementation as: traverse all potentional values in first position, and push it; then travers all potentional values in next position, and push it; till the last position, while only one element left, and push it. so a new combination is created. 123456789101112131415161718192021222324252627vector&lt;vector&lt;int&gt;&gt; sol1(vector&lt;int&gt;&amp; nums)&#123; vector&lt;vector&lt;int&gt;&gt; idx_vector ; for(int i=0; i&lt; nums.size(); i++) &#123; vector&lt;int&gt; tmp ; tmp.push_back(nums[i]); //the first element in new created vector vector&lt;int&gt; nums1 = nums ; nums1.erase(nums1.begin()+i); //the second element in new created vector for(int j=0; j&lt; nums1.size(); j++) &#123; tmp.push_back(nums1[j]); vector&lt;int&gt; nums2 = nums1; nums2.erase(nums2.begin() + j); if(nums2.size() == 1) &#123; tmp.push_back(nums2[0]); idx_vector.push_back(tmp); tmp.pop_back(); &#125; tmp.pop_back(); &#125; &#125; return idx_vector; &#125; the first implmenation only works for fixed input , so how to design a recursive fun to make this idea more general ? it’s easy to abstract the process above: for ith position in the new combination vector, the potentional values is one less from (i-1)th position, traverse all possible values in ith position and push it, don’t forget pop it out for traversing to the next possible value; and the end of recursive is when only one element left. 123456789101112131415161718192021void sol(vector&lt;int&gt;&amp; nums, vector&lt;int&gt;&amp; tmp, vector&lt;vector&lt;int&gt;&gt;&amp; idx_vector )&#123; if(nums.size() == 1) &#123; tmp.push_back(nums[0]); idx_vector.push_back(tmp); tmp.pop_back(); return ; &#125; for(int i=0; i&lt; nums.size(); i++) &#123; tmp.push_back(nums[i]); vector&lt;int&gt; nums2 = nums; nums2.erase(nums2.begin()+i); sol(nums2, tmp, idx_vector); tmp.pop_back(); &#125; return;&#125; test: 123456789101112131415161718int main()&#123; vector&lt;int&gt; nums = &#123;1, 2, 3, 4&#125;; int size = nums.size(); vector&lt;int&gt; tmp ; vector&lt;vector&lt;int&gt;&gt; outs; sol(nums, tmp, outs); vector&lt;int&gt;::iterator it ; for(int i=0; i&lt; size*(size-1); i++) &#123; for(it = outs[i].begin(); it != outs[i].end(); it++) cout&lt;&lt; *it &lt;&lt; ' ' ; cout &lt;&lt; "\n" &lt;&lt; endl ; &#125; return 0;&#125;]]></content>
      <tags>
        <tag>leetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIT 6.S094 Deep Learning for Self Driving Cars]]></title>
    <url>%2F2018%2F08%2F10%2FMIT-s094%2F</url>
    <content type="text"><![CDATA[1 Deep Reinforcement Learning linkapps: motion planning 2 Convolutional Neural Networks linkapps: End-2-end driving task(pedestrian detect) 3 Recurrent Neural Networks linkapps: steering control through time CNN Project: DeepTeslaDRL Project: DeepTrafficFramework: ConvNetJS]]></content>
  </entry>
  <entry>
    <title><![CDATA[C++ template and STL containers]]></title>
    <url>%2F2018%2F08%2F10%2FC-template-and-STL-containers%2F</url>
    <content type="text"><![CDATA[I used C++ couple years already, but never take a close look at it. when looking back to few C++ demo works at school, I had a strong feeling at that time, to make the code running is my only goal, how it work or how to improve it, is always ignored, I am afraid in the language details, as the wisdom say: the evil is in details. after working for three years, I feel the necessary and urgent to back to the language itself(e.g. Linux, C/C++) as the first step to move forward in application development. Frameworks, engineering-based APIs are more close to final products, making them easy to be attracted, compared to how the details implemented. like the mechanical undergradute students, who first-time run ABAQUS with beatiful visulized results, feels so good. anyway I have to delay the short satification or self-feeling-good. C is clean and the applications have clear structure, C++ is more metaphysics, I even don’t know where to start. even I thought I knew C++ well, but actualy there are many details behind, e.g. allocator in STL. templatetemplate is used for generic programming, e.g. both vector and vector smell same at compiler time. Template reels off or abstract the common part “vector” as a container, and whatever type is ok to store in. function template123456789101112template &lt;class T&gt; void func(T arg)template &lt;typename T&gt; void func(T arg)template &lt;class T&gt; T add(T n1, int v)&#123;&#125;;template &lt;class T1, class T2, class T3&gt; T3 func(T1 a1, T2 a2, int arg3=3)&#123;&#125;; the actual meaning of TYPE is deduced by compiler depending on the arg passed to this function. “class” or “typename” is similar. template supports default parameters, once the i-th argument is set with default value, all arguments after it must using default values. class template123456789101112template&lt;class _Tp, class _Ref, class _Ptr&gt;class _list_iterator &#123; typedef _List_iterator&lt;_Tp, _Tp&amp;, _Tp*&gt; iterator ; typedef _List_iterator&lt;_Tp, const _Tp&amp;, const _Tp*&gt; const_iterator ; typedef _List_iterator&lt;_Tp, _Ref, _Ptr&gt; _Self ; typedef _Tp value_type ; typedef _Ptr pointer; typedef _Ref reference ; typedef _List_node&lt;_Tp&gt; _Node; &#125; feel the power of template in STL source code. STL containersreferences:&lt;&lt; the annotated STL source using SGI STL &gt;&gt; by jjHoua visitor guide to C++ allocatorthe annotated STL sourceI took several days to start, cause the first section on allocator already blocked me. why allocator ?The logic of a dynamic container doesn’t depend on the specifies of how memory is obtained; namely, the design of container classes should be decoupled from the memory allocation policy. how allocator works ?memory allocation and object construction is separated, allocator has four operations: allocate(), deallocate(), construct(), destroy(). there are std interface to allocators 1234567AllocTraits = std::allocator_traits&lt;alloc&gt; // define an allocator of type "alloc" AllocTraits::pointer p = AllocTraits::allocate(a, n) //get memory space for n objects of type T, but not construct yet AllocTraits::construct(a, trueaddress(ptr), x, y, z) ; //construct a T(x, y, z)AllocTraits::destroy(a, trueaddress(ptr)); // ~T()AllocTraits::deallocate(a, ptr, 1) ; // deallocate the space for one T object WHEN we say A is an allocator for T , where T is a type e.g. AllocatTraits::value_type. we mean, A knows how to obtain and release memory to store objects of type T. git:allocator iteratoriterator is used to build algorithms in containers. while I don’t really get the traits 1234567891011121314151617template &lt;class _Iterator&gt;struct iterator_traits &#123; typedef typename _Iterator::iterator_category iterator_category ; typedef typename _Iterator::value_type value_type ; typedef typename _Iterator::difference_type difference_type ; typedef typename _Iterator::pointer pointer; typedef typename _Iterator::reference reference ;&#125;;template&lt;class _Tp&gt;struct iterator_traits&lt;_Tp*&gt; &#123; typedef typename random_access_iterator_tag iterator_category ; typedef typename _Tp value_type ; typedef typename ptrdiff_t difference_type ; typedef typename _Tp* pointer; typedef typename _Tp&amp; eference ;&#125;; vectorstd::vector is dynamic, continous. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071template &lt;class _Tp, class _Alloc = __STL_DEFAULT_ALLOCATOR(_Tp)&gt;class vector:: protected _Vector_base&lt;_Tp, _Alloc&gt;&#123; public: typedef _Tp value_type ; typedef value_type* pointer ; typedef const value_type* const_pointer ; typedef value_type* iterator ; typedef const value_type* const_iterator ; iterator begin() &#123;return _M_start;&#125;; iterator end() &#123;return _M_finish ;&#125;; protected: _Tp* _M_start ; // the starting point of current used space _Tp* _M_finish; // the ending point of current sued space _Tp* _M_end_of_storage; // the end of total avialable space(capacity) public: explicit vector(const allocator_type&amp; __a = allocator_type()) : _Base(__a)&#123;&#125; //default constructor // construt of n elements of initial-value vector(size_type __n, const _Tp&amp; __value, const allocator_type&amp; __a = allocator_type()) : _Base(__n, __a) &#123; _M_finish = uninitialized_fill_n(_M_start, __n, __value); &#125; //copy construtor vector(const vector&lt;_Tp, _Alloc&gt;&amp; __x) :_Base(__x.size(), __x.get_allocator()) &#123; _M_finish = uninitialized_copy(__x.begin(), __x.end(), _M_start); &#125; size_type size() const &#123;return size_type(end() - begin());&#125; size_type capacity() const &#123;return size_type(_M_end_of_storage - begin()); bool empty() const&#123;return begin() == end();&#125; reference operator[](size_type __n)&#123;return *(begin() + __n);&#125; void reserve(size_type __n)&#123; if (capacity() &lt; __n) &#123; const size_type __old_size = size(); iterator __tmp = _M_allocate_and_copy(__n, _M_start, _M_finish); destroy(_M_start, _M_finish); _M_deallocate(_M_start, _M_end_of_storage-_M_start); _M_start = __tmp; _M_finish = __tmp + __old_size ; _M_end_of_storage = _M_start + __n ; &#125; &#125; void push_back(const _Tp&amp; __x) &#123; if(_M_finish != _M_end_of_storage) &#123; construct(_M_finish, __x); ++_M_finish; &#125;else _M_insert_aux(end(), __x); &#125; template&lt;class _Tp, class _Alloc&gt; inline bool operator==(const vector&lt;_Tp, _Alloc&gt;&amp; __x, const vector&lt;_Tp, _Alloc&gt;&amp; __y) &#123; return __x.size() == __y.size() &amp;&amp; equal(__x.begin(), __x.end(), __y.begin()); &#125;&#125;; liststd::list is cycling double-direction link list. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140struct _List_node_base &#123; _List_node_base * _M_next ; _List_node_base * _M_prev ;&#125;;template &lt;class _Tp&gt;struct _List_node : public _List_node_base &#123; _Tp _M_data;&#125;;struct _List_iterator_base &#123; typedef size_t size_type ; typedef ptrdiff_t difference_type ; typedef bidirectional_iterator_tag iterator_category; _List_node_base* _M_node ; //constructor _List_iterator_base(_List_node_base* __x) : _M_node(__x)&#123;&#125; _List_iterator_base()&#123;&#125;; void _M_incr() &#123; _M_node = _M_node-&gt;_M_next; &#125;; void _M_decr() &#123; _M_node = _M_node-&gt;_M_prev; &#125;; bool operator==(const _List_iterator_base&amp; __x) const &#123; return _M_node == __x.M_node ; &#125;&#125;;template&lt;class _Tp, class _Ref, class _Ptr&gt;struct _List_iterator : public _List_iterator_base &#123; typedef _List_iterator&lt;_Tp, _Tp&amp;, _Tp*&gt; iterator ; typedef _List_iterator&lt;_Tp, const _Tp&amp;, const _Tp*&gt; const_iterator ; typedef _List_iterator&lt;_Tp, _Ref, _Ptr&gt; _Self ; typedef _Tp value_type ; typedef _Ptr pointer; typedef _Ref reference ; typedef _List_node&lt;_Tp&gt; _Node; //constructor _List_iterator(_Node* __x): _List_iterator_base(__x)&#123;&#125; _List_iterator()&#123;&#125; reference operator*() const &#123;return ( (_Node*) _M_node)-&gt;_M_data; &#125; _Self&amp; operator++() &#123; this-&gt;_M_incr(); return *this; &#125;&#125;;template &lt;class _Tp, class _Alloc=_STL_DEFAULT_ALLOCATOR(_Tp)&gt;class list : protected _List_base&lt;_Tp, _Alloc&gt;&#123; public: typedef _List_node&lt;_Tp&gt; _Node; protected: _List_node&lt;_Tp&gt; * _M_node; public: list(size_type __n, const _Tp&amp; __value, const allocator_type&amp; __a = allocator_type()) : Base_(__a) &#123; insert(begin(), __n, __value); &#125; explicit list(size_type __n) :_Base(allocator_type()) &#123; insert(begin(), __n, _Tp()); &#125; protected: _Node* _M_create_node(const _Tp&amp; __x) &#123; _Node* __p = _M_get_node(); __STL_TRY &#123; _Construct(&amp;_p-&gt;_M_data, __x); &#125; return __p; &#125; public: iterator begin() &#123; return (_Node*)(_M_node-&gt;_M_next);&#125; iterator end() &#123; return _M_node; &#125; bool empty() const &#123;return _M_node-&gt;_M_next == _M_node ;&#125; size_type size() const &#123; size_type __result = 0; distance( begin(), end(), __result); return __result; &#125; size_type max_size() const &#123;return size_type(-1);&#125; reference front() &#123;return *begin();&#125; reference back() &#123;return *(--end());&#125; void swap(list&lt;_Tp, _Alloc&gt;&amp; __x) &#123; __STD::swap(_M_node, __x._M_node); &#125; interator insert(iterator __position, const _Tp&amp; __x) &#123; _Node* __tmp = _M_create_node(__x); __tmp-&gt;_M_next = __position._M_node ; __tmp-&gt;_M_prev = __position._M_node-&gt;_M_prev ; __position._M_node-&gt;_M_prev-&gt;_M_next = __tmp; __position._M_node-&gt;_M_prev = __tmp; return __tmp; &#125; void push_front(const _Tp&amp; __x)&#123;insert(begin(), __x);&#125; void push_back(const _Tp&amp; __x)&#123;insert(end(), __x);&#125; iterator erase(iterator __position) &#123; _List_node_base* __next_node = __position._M_node-&gt;_M_next ; _List_node_base* __prev_node = __position._M_node-&gt;_M_prev ; _Node* __n = (_Node*) __position._M_node ; __prev_node-&gt;next = __next_node ; __next_node-&gt;prev = __prev_node; _Destroy(&amp;__n-&gt;_M_data); _M_put_data(__n); return iterator((_Node*) __next_node); &#125;&#125; dequeuestd::dequeue can operate elements at both ends, and the memory is multi-sectional, in each memory section is linear continous, with advantage of vector and list. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155template &lt;class _Tp, class _Ref, class _Ptr&gt;struct _Deque_iterator &#123; typedef _Deque_iterator&lt;_Tp, _Tp&amp;, _Tp*&gt; iterator ; typedef _Deque_iterator&lt;_Tp, const _Tp&amp;, const _Tp*&gt; const_iterator ; static size_t _S_buffer_size() &#123;return __deque_buf_size(sizeof(_Tp));&#125; typedef random_access_iterator_tag iterator_category; typedef _Tp value_type ; typedef _Ptr pointer; typedef _Ref reference; typedef size_t size_type ; typedef ptrdiff_t difference_type ; typedef _Tp** _Map_pointer ; typedef _Deque_iterator _Self; _Tp* _M_cur ; //current point in cache _Tp* _M_first; //first point in cache _Tp* _M_last; _Map_pointer _M_node ; /* deque support multi-section memory space, in each memory section is linear continous * Map index is used to point to the memory sections */ _Deque_iterator(_Tp* __x, _Map_pointer __y) :_M_cur(__x), _M_first(*__y), _M_last(*__y + _S_buffer_size()), _M_node(__y) &#123;&#125; //default //copy difference_type operator-(const _Self&amp; __x) const &#123; return difference_type(_S_buffer_size()) * (_M_node - __x.M_node - 1) + (_M_cur - _M_first) + (__x._M_last - __x._M_cur); &#125; _Self&amp; operator++() &#123; ++_M_cur ; if(_M_cur == _M_last)&#123; _M_set_node(_M_node + 1); _M_cur = _M_first; &#125; return *this; &#125; _Self&amp; operator--() &#123; if(_M_cur == _M_first)&#123; _M_set_node(_M_node - 1); _M_cur = _M_last; &#125; --_M_cur ; return *this; &#125;&#125; /* deque has two iterator: start -&gt; the first element in first cache space; finish -&gt; the last element in last cache space */template &lt;class _Tp, class _Alloc&gt;class _Deque_base &#123; protected: _Tp** _M_map ; size_t _M_map_size; iterator _M_start; iterator _M_finish;&#125;;template &lt;class _Tp, class _Alloc = __STL_DEFAULT_ALLOCATOR(_Tp)&gt;class deque : protected _Deque_base&lt;_Tp, _Alloc&gt;&#123; public: typedef typename _Base::iterator iterator ; typedef typename _Base::const_iterator const_iterator ; protected: using _Base::_M_map ; using _Base::_M_map_size ; using _Base::_M_start ; using _Base::_M_finish ; public: explicit deque(const allocator_type&amp; __a = allocator_type()) :_Base(__a, 0) &#123;&#125; deque(const deque&amp; __x): _Base(__x.get_allocator(), __x.size()) &#123; uninitialized_copy(__x.begin(), __x.end(), _M_start); &#125; ~deque()&#123; destroy(_M_start, _M_finish);&#125; void push_back(const value_type&amp; __t) &#123; if(_M_finish._M_cur != _M_finish._M_last - 1) &#123; construct(_M_finish._M_cur, __t); ++_M_finish._M_cur ; &#125;else _M_push_back_aux(__t); &#125; void push_front(const value_type&amp; __t) &#123; if(_M_start._M_cur != _M_start._M_first) &#123; construct(_M_start._M_cur - 1, __t); --_M_start._M_cur; &#125;else _M_push_front_aux(__t); &#125; void pop_back() &#123; if(_M_finish._M_cur != _M_finish._M_first) &#123; --_M_finish._M_cur; destroy(_M_finish._M_cur); &#125;else _M_pop_back_aux(); &#125; void pop_front() &#123; if(_M_start._M_cur != _M_start._M_last -1) &#123; destroy(_M_start._M_cur); ++_M_start._M_cur; &#125;else _M_pop_back_aux(); &#125; iterator insert(iterator position, const value_type&amp; __x) &#123; if(position._M_cur == _M_start._M_cur) &#123; push_front(__x); return _M_start; &#125;else if(position._M_cur == _M_finish._M_cur) &#123; push_back(__x); iterator __tmp = _M_finish; --__tmp; return __tmp; &#125; &#125;&#125;; stackstd::stack only operate on the top element (first in last out), can’t iterate the container, the default container for stack is deque. 123456789101112131415161718192021222324252627temlate &lt;class _Tp, class _Sequence&gt;class stack&#123; public: typedef typname _Sequence::value_type value_type ; typedef typname _Sequence::size_type size_type ; typedef typname _Sequence container_type ; typedef typname _Sequence::reference reference ; typedef typname _Sequence::const_reference const_reference; protected: _Sequence c; // the fundmental container: deque by default public: stack() : c() &#123;&#125; explicit stack(const _Sequence&amp; __s) : c(__s) &#123;&#125; bool empty() const &#123; return c.empty();&#125; size_type size() const &#123;return c.size();&#125; reference top() &#123;return c.back(); &#125; void push(const value_type&amp; __x)&#123; c.push_back(__x);&#125; void pop() &#123; c.pop_back();&#125; template&lt;class _Tp, class _Seq&gt; bool operator==(const stack&lt;_Tp, _Seq&gt;&amp; __x, const stack&lt;_Tp, _Seq&gt;&amp; __y) &#123; return __x.c == __y.c ; &#125;&#125;; queuestd::queue supports only pop element from front, and push element into end, the default container is dequeue 123456789101112131415161718192021222324252627template &lt;class _Tp, class _Sequence&gt;class queue&#123; public: typedef typname _Sequence::value_type value_type ; typedef typname _Sequence::size_type size_type ; typedef typname _Sequence container_type ; typedef typname _Sequence::reference reference ; typedef typname _Sequence::const_reference const_reference; protected: _Sequence c; // the fundmental container: deque by default public: queue() : c() &#123;&#125; explicit queue(const _Sequence&amp; __s) : c(__s) &#123;&#125; bool empty() const &#123; return c.empty();&#125; size_type size() const &#123;return c.size();&#125; reference top() &#123;return c.back(); &#125; void push(const value_type&amp; __x)&#123; c.push_back(__x);&#125; void pop() &#123; c.pop_front();&#125; template&lt;class _Tp, class _Seq&gt; bool operator==(const queue&lt;_Tp, _Seq&gt;&amp; __x, const queue&lt;_Tp, _Seq&gt;&amp; __y) &#123; return __x.c == __y.c ; &#125;&#125;; priority queuestd::priority_queue is queue with priority.]]></content>
      <tags>
        <tag>c++11</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[design a threadpool in pure C]]></title>
    <url>%2F2018%2F08%2F02%2Fdesign-a-threadpool-in-pure-C%2F</url>
    <content type="text"><![CDATA[actually this post should be named as “pure C multithreads”, while it’s better to write some real code. a simple threadpool in pure C exposed APIsat most simple case, what a threadpool object should do? thpool_init(num_threads); thpool_destory(threapool_obj); thpool_add_work(callback, args); // so user define event callback() can be passed in; threadpool is used to assign jobs (from job list) to a thread. so joblist should be an internal class; and it’s better to package thread, job as separate class too; to add work, what kind of funcs need for joblist ? at least, to add new job(from outside) to the joblist. internal structures:123456789101112131415161718192021222324252627struct thread &#123; int id; pthread_t thread; //pthread_t is a unsigned long int; it's system assigned threadpool *thpool; //so thread know its domain&#125;struct job &#123; (void*) func(void*); // each job/task should have a callback func (void*) func_arg ; //and func args //since consider joblist as a list, it's helpful to add pointers to neighbors job* prev; job* next;&#125;struct jobqueue &#123; int num_jobs; job* front; job* rear; //some flags to support add new node(job) to the list&#125; struct threadpool_ &#123; thread* threads_array; jobqueue job_queue; // some flags &#125; consider syncronlizationwill multi threads call add_task() simutaneously? if so, jobqueue should have a mutex object; 123456 struct jobqueue &#123; int num_jobs; job* front; job* rear; pthread_mutex_t jq_mutex;&#125; during threadpool initialization, will all threads be created simultaneously and immediately? if not, there should be thread status flags (creating, exisiting-but-idle, working, died); and to update these flags need protect by mutex object; 123456struct threadpool_ &#123; thread* threads_array; jobqueue job_queue; int num_threads; //stands for the number of all existing threads pthread_mutex_t tp_mutex; &#125; this is an interesting, when design a lib, what’s in my mind. hopefully implement by the end of week.]]></content>
      <tags>
        <tag>pure C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pure C socket]]></title>
    <url>%2F2018%2F07%2F31%2Fpure-C-socket%2F</url>
    <content type="text"><![CDATA[this is a review from GNU C lib. socket is a two-way communication channel, both read and write can be performed at either end. sys/socket.h123456789101112131415161718192021222324252627282930313233343536373839int socket(AF_INET, SOCK_STREAM, protocol); /* return fd for this new socket, or -1 as error */int close(int socket_fd); /* return 0 when close successfuly, else return -1 */int connect(int socket, struct sockaddr *addr, socketlen_t len);/* initiate a connection from (client) socket to (server) address; by default, the connection is blocking untill server respond */int listen(int socket, int n);/* n specifies the length of queue for pending connections; when the queue fills up, new clients attempting to connect will fail */int accept(int socket, struct sockaddr *addr, socketlen_t *len_ptr);/* if successfully, accept return a new socket fd, the original (server) socket remains open and unconnected; the address and len_ptr are used to return information about the name of client socket that initiated this connection */size_t send(int socket, const void *buffer, size_t size, int flags);/* param socket is the fd of current sending socket. no receiver socket explicitly defined, since connection-oriented(TCP)protocol will connect first prior to send/receive */size_t recv(int socket, const void *buffer, size_t size, int flags);/* param socket is the fd of current receiving socket */``` ## network socket I didn't realize GNU C socket I/O is so related to network socket. the missing part when reading muduo, libuv is here. The server listens the connection requests on the special server socket, then accept each incoming connection. select() blocks/sleeps the program until input is available/ready on the special server socket/fd. ```cvoid FD_ZERO(fd_set *set) /* initialized the file descriptor set to empty set */void FD_SET(int fd, fd_set *set) /* add fd to set */void FD_CLR(int fd, fd_set *set) /*remove fd from set */void FD_ISSET(int fd, const fd_set * set) /* return true(non-zero) if fd is in set; else return 0 */STDIN_FILENO ; /* file descriptor for std input “1” */ how to debug server/client code? chatRoom]]></content>
      <tags>
        <tag>pure C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pure C I/O]]></title>
    <url>%2F2018%2F07%2F31%2Fpure-C-i-o%2F</url>
    <content type="text"><![CDATA[this is a review from GNU C programming streamtwo basic mechanism for representing the connection between your program and the file: streams &amp; file descriptors. FD is represented as objects of type int; streams are represented as FILE * objects a stream is an abstract concept reprensenting a communication channel to a file, a device, or consider it as a sequence of characters with functions to take characters out of one end, and put characters into the other end, like a pipe. file positionan integer representing the number of bytes from the beginning of the file; each time a character read or written, the file position is incremented, namely, access to a file is sequential. 12345678910111213141516171819202122long int ftell(FILE \* stream); //return file position int fseek(FILE \*stream, long int offset, int whence);/\* whence is SEEK\_SET | SEEK\_CUR | SEEK\_END \*/``` “write to a file” is always appended sequentially to the end of the file, regardless of the file position; but “read from a file” is used the current file position. which means multiple reading can happens simultaneously with an independent file pointer. In fact, each opening creates an independent file position pointer, even in the same program to open a file twice. ## std streams```c FILE *stdin ; FILE *stdout; FILE *stderr; FILE *fopen(const char *filename, const char *openMode); /* create a new stream connected to the file */ int fclose(FILE *stream) /* disconnected between the stream and the file, any buffered output is written and any buffered input will discarded */ ASCII IO123456789int fputs(const char *s, FILE *stream) ;/* write a char array into stream. this call doesn't add a newline or terminal null character */char *fgets(char *s, int n, FILE *stream);/* read n number of char array from stream, default add a newline character */ssize_t getline(char **lineptr, size_t *n, FILE *stream);/* read a whole line from stream */ block IOusually block stands for either block data or text in fixed-size, instead of characters or lines 12345size_t fread(void *data, size_t size, size_t n, FILE *stream);/* read #n block, each block has size from stream, and store in data */size_t fwrite(const void *data, size_t size, size_t n, FILE *stream);/* write #n block, each block has size from buffer data to stream */ formatted IO1234567int printf(const char *template, …); //write to std outputint fprintf(FILE *stream, const char *template, ...); write to streamint sprintf(char *s, const char *template, ...); //write to a stringint scanf(const char *template, …) //formatted read from stdinint fscanf(FILE *stream, const char *template, …) // read from stream int sscanf(const char *s, const char *template, …) // read from a string EOF1234int feof(FILE* stream) ; /* return nonzero iff end of file indicator is set for stream */int ferror(FILE* stream) ; /* return nonzero iff error indicator is set for stream */ stream bufferstream and file is not communicated character-by-character, there is a buffer for I/O. 12int fflush(FILE *stream) /* take any buffered output on stream to be delivered to the file */ file descriptor123456789101112int open(const char *filename, int flags);int open64(const char *filename, int flags) ;// allow large file mode size_t read(int fd, void *buffer, size_t size) // read from fd size bytes into buffer size_t pread(int fd, void *buffer, size_t size, off_t offset) // start reading from position “offset” size_t write(int fd, const void *buffer, size_t size) off_t lseek(int fd, off_t offset, int whence) // change the file position of fd FILE *fdopen(int fd, const char *opentype) // return a new stream for this fd synchronizing I/O1234void sync(void) ;// to ensure all operations finished before they return int fsync(int fd) ;// to ensure all data associated with the fd is written to the device not yetasync I/O, event I/O, interrupt driven I/O, GPIO …]]></content>
      <tags>
        <tag>pure C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux at work]]></title>
    <url>%2F2018%2F07%2F26%2FLinux-at-work%2F</url>
    <content type="text"><![CDATA[bash environment/etc/profile -&gt; hold shell environment &amp; startup settings ~/.bashrc -&gt; system wide definitions for shell funcs &amp; alias ~/.bash_profile -&gt; user environment individually ususally during coapplication configuration on Mac or Linux, mostly use ~/.bashrc to add project related environment variables. regular expressions the$ :matches the line ending with “the” ^the :matches the line starting with “the” [^g]abc : search “abc” but not starting with “g” [^abc…z][0-9] : search numbers but not with alphabet in front ^[a-z] : search the line starting with low alphabet ^[^a-z]: search the line starting without low alphabet ^$ : match white space [xyz]: general match anyone from “xyz” [!xyz]: general match in opposite, neither from “xyz” shell expressions command &amp; : to run command in bg $! : current PID $# : number of arguments $1 : the first paramter $* : wildcard for all variables Linux commands ldd: check runtime lib unix2dos / dos2unix: change file format between Windows and Linux ps : print running processes in current terminal ipcs : check share memory portion in current system ipcrm -M ipc_key : remove the shared memory portion id by ipc_key top: print virtual,resident,shared memory percentage basename: e.g. basename /path/to/file -&gt; file wc -l .file: print total lines of the file df: check the usage of disk find . -type f -print0 | xargs -0 grep -l “xx” find D:\ | grep xml less .file : read the last few lines of file ln item link : soft link ln -s item link : hard link type command : check the type of command cat f1 f2 &gt; merged sort uniq comm f1 f2 : the common part of f1 and f2 diff f1 f2 : the difference of f1 and f2 2 &gt;&amp; 1: redirect std output(fd=2) to std error(fd=1) nm : check libs used in current executable(T defined, U undefined) gdbMPI gdb: (each MPI thread will have an independent terminal window) mpirun -np #cpus xterm -e gdb ./exe set breakpoint in different src: b sth.cxx:20 print an array: p (int[length]* a) add input file: set args -j input_file load src after triggering GDB: gdb file exe set args -j input_file books about Linuxbash guide for beginnerstao of regular expressionLinux programmer’s manualLinux system administrators guideC expert programmingwhat every programmer should know about CPU caches resources in multi-core optimizationunderstand CPU utilization &amp; optimizationIntel: optimization applications for NUMAIntel guide for developing multithreaded applicationsMPI parallel programming in Pythonconsiderations in software design for multi-core, multiprocessor architectureshow to optimize GEMMoptimize for Intel AVX using MKL with DGEMMGEMM: from pure C to SSE optimized micro kernela practical guide to SSE SIMDD with C++multi-core designUnix and pthread programming resources in applicationsintroduction to post processing finite element results with AVSCAE Linux: FEA inter-operabilityopen sourcing a Python project the right wayPPSSpyNastranhdfviewervalgrind postscriptstarting from 2016, I had went through each hot topic nowadays, even did some study in DL, AV, CV etc. every time the passion burst out and I promised to e.g. study a framework, or contribute an open project. in reality, the passion dies away soon. It’s like a new and very attracting concept bump out in the market, but no business model can handle it, then it dies out. downside of this learning pattern is that the fundmental is ignored. e.g. I can’t success in code interview, few experience in basic algorithms. kind of person always vision the big, but don’t realize how to reach there. it’s kind of wasting finally, just want to be focus at this moment.]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pure C structure]]></title>
    <url>%2F2018%2F07%2F26%2FPure-C-structure%2F</url>
    <content type="text"><![CDATA[definition12345typedef struct gid_ &#123; double x; double y; char* name ; &#125; gid ; typedef defines “gid” as an alias name to “struct gid_ “. typedef also alias function handle/pointer, which is often used in asynchronous/event callback programming. other data encapsulation are: enum : map a list of const names to integral constants union: store many data type in one memory address initialization123gid g1=&#123;2.0, 3.0, "g1"&#125;;gid g2=&#123;.x=2.0, .y=3.0, .name="g2"&#125;;gid g3=(gid)&#123;2.0, 3.0, "g3"&#125;; in C++, structure initialization can also be done in constructor. memory alignmentfor better memory access in CPU architecture, memory alignment in structure is considered. namely: chars can start on any byte address 2-bytes shorts must start on an even address 4-bytes ints/floats must start on an address divisible by 4 8-bytes doubles/longs must start on an address divisible by 8 the size of the whole structure, is aligned to intergeral times of the max size of its member variable. e.g. sizeof(gid) = 24, not 8+8+4. to put the member variables in ascending/descending order is good practice. structure pointer arithmetic“gid++” will step forward the sizeof(gid); structure also supports self-reference: 123456typedef struct gid_ &#123; char *name ; double x; double y; gid_ *next_gid ; &#125; gid ; another common utils is structure array: 12gid gid_list[MAX_SIZE]; gid **gid_list_p ; structure as function parametersin general, structure can be passing to function by value or by pointer, but not by reference in pure C. also structure as return value from function can be value or a pointer structure in C++in C++, structure supports member functions, is same as a public class. and the initialization can be done either in constructor function or direct initialization during definition. see the difference of struct between C and C++ stdlib.h123456789101112131415161718 /* convert string s1 to an int*/ int atoi(const char* s1); /* memory op */ void* malloc(size_t size); void free(void *ptr); /* system level op */ char* getenv(const char *name); int system(const char *command); /* algorithms */ void* bsearch(const void* key, const void* base, size_t nitems, size_t size, int(*compar)(const void*, const void*)) void qsort(void *base, size_t nitems, size_t size, int(*compar)(const void*, const void*))]]></content>
      <tags>
        <tag>pure C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pure C string]]></title>
    <url>%2F2018%2F07%2F24%2Fpure-C-string%2F</url>
    <content type="text"><![CDATA[string initializationin pure C, string is a char array terminating with NULL(‘\0’). To initialize an array of char(string) is with a string literal(a double-quotaed string). in general, the string literal is used as the initializer for an array of char; anywhere else, it is a static array of chars, which shouldn’t be modified by a pointer. 123456789101112131415161718192021222324252627282930313233/* initialize a string in C * the size of s1 is taken from the size of the initializer */ char s1[] = "hello world" ; // or char s1[12] = "hello world"; /* common errors */ char s2[]; s2 = "hello world"// error: storage size of 's2' isn't known char s2[12]; s2 = "hello world"// error: invalid array assignment /* there is no "=" operator defined for char array in C using strcpy()*/``` ## char pointer arithmetic ```c char s1[12]; *s1++ ; /* error: lvalue requied as increment operand * s3 is an array, can't modify the address of an array. the difference between pointer and array */ char* s2 = s1; *s2++;/* s2 is a pointer to s1, ++ is ok */ string.h1234567891011121314151617181920212223242526/* concatenate two strings */strcat(char* s1, const char* s2) ;strncat(char* s1, const char* s2, size_t n); /* locate the first/last occurance of char c in string s */strchr(const char* s1, int c);memchr(const void* s1, int c, size_t n);strrchr(const char* s1, int c); /* compare s1 and s2 alphabetically */strcmp(const char* s1, const char* s2); strncmp(const char* s1, const char* s2, size_t n);memcmp(const void* s1, const void* s2, size_t n);/* copy s2 to s1 */strcpy(char *s1, const char *s2); memcpy(void* s1, void* s2, size_t n);/* number of bytes, not including terminating NULL; sizeof() will inlcude the terminating NULL */strlen(const char* s1); /* find the first occurance of substring s2 in s1 */strstr(const char *s1, const char *s2);/* split string s1 into a sequence of tokens by s2 */strtok(char* s1, const char* s2); \ is replaced by in C++.and be aware of downs of C string.]]></content>
      <tags>
        <tag>pure C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[which function is called]]></title>
    <url>%2F2018%2F07%2F18%2Fwhich-function-is-called%2F</url>
    <content type="text"><![CDATA[which function is calledwhen a derived object calls the base class member function, inside which calls another virtual member function, which is implemented inside the derived class, so which virtual member function is actually called ? 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &lt;iostream&gt;class Weld &#123; public: Weld()&#123;&#125;; virtual ~Weld()&#123;&#125;; virtual bool getDataIndex()&#123; std::cout &lt;&lt; "Weld::getDataIndex" &lt;&lt; std::endl; return true ; &#125; bool getInternalDataList()&#123; return getDataIndex(); &#125;&#125;;class SpotWeld: public Weld&#123; public: SpotWeld()&#123;&#125;; virtual ~SpotWeld()&#123;&#125;; virtual bool getDataIndex()&#123; std::cout &lt;&lt; "SpotWeld::getDataIndex" &lt;&lt; std::endl; return true; &#125;&#125;;class ACM2 : public SpotWeld&#123; public: ACM2()&#123;&#125;; ~ACM2()&#123;&#125;; virtual bool getDataIndex()&#123; std::cout &lt;&lt; "ACM2::getDataIndex" &lt;&lt; std::endl; return true; &#125; bool getPoints()&#123; return getInternalDataList(); &#125;&#125;;int main()&#123; ACM2 acm2 ; acm2.getPoints(); return 0;&#125; output is: ACM2::getDataIndex so the derived object always calls the member function that most closest to its own class domain first, even if this member function is called from a base class member function.]]></content>
      <tags>
        <tag>c++11</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[multi-core at work]]></title>
    <url>%2F2018%2F07%2F13%2Fmulti-core-at-work%2F</url>
    <content type="text"><![CDATA[NUMAuniform memory arch, all CPUs in the same sokcet shall the memory, and the memory IO is bottleneck; non uniform memory acess(NUMA), each CPU has itw own memory, inter-CPU memory access is remote. numctl cpu_node_bind MPI binding APIs memory allocation strategy in NUMA interleave: place the memory on alternating node, the first memory portion in node0, the next portion in node1 membind: forcely to allocate memory in special node CPU affinitythe benifit of CPU affinity is to reduce context switch since one special process is binded to one speical CPU, so the data required in this process (no other process data will be switched in), can always in the CPU cache. espcially for NUMA arch to access data locally. and this is specially true when the application generate a large cache footprint, e.g. in scientific computing. for general applications, CPU afinity may reduce performance, since in this way the CPU scheduler can’t work properly. CPU info/proc/cpuinfo physical id: socket indexsiblings: number of cores in each socketcore id: current core index e.g. Ford HPC CPU architecture: 2 or 4 CPU sockets group into one computing node each socket has 10 or 12 CPU cores each socket has a on-board shared memory atomic operationCPU physical mechanism: physically there is a bus #hlock pin. if #lock is added before the assembly instruction, the corresponding machine code will be pulled down during the execution of the #hlock pin till the end, basically the bus is locked only for current instruction cache coherence: the cache unit tranfered between CPU and main memory is cache line. in one socket, as slibings share L3 cache, there is an scenario, when CPU1 modified one variable, but not yet writen to main memory, and CPU2 read from main memory and did modified again, then the variable in CPU1 and CPU2 is not coherence. volatile in C, forcely to read the variable value from main memory every time, to avoid use dirty memory in cache another scenario, to achieve and cache coherence, and the same variable is read and write repeatly by multiple processes, the performance is worse, “false sharing” lock: signal, also called &quot;sleeping lock&quot;: used when the lock need to keep for a longer time spin lock: at most hold by one thread, to block other threads into critial area, used to keep for a short time. write/read lock: system performanceresident memory(res), the portion of virtual memory space that is actually in RAM; swapped memory, when the physical memory is full and the system needs more memory, inactive pages in memory moved to the shared space, and swapped usable memory in virtual memory = res memory + swapped memory mmap, to access the remote (data block) like access local RAM. voluntary context switches(vcs): when a thread makes a system call that blocks. vcs measures the frequency of calling blocked system I/O involuntary context switches(ivcs): when a thread has being runing too long without making a system call that blocks, and there are other processes waiting for CPU, then OS will switch for other CPUs. ivcs measures the CPU competition, an unfinished processed is switched off in general, as more threads, the context switch cost increase, due to the total amount of switch increase and as well each switch is more expensive, since CPU cache is limited, and each process will hold fewer data in cache. cpu_time (clock_t): the total amount of time that a process has actually used user CPU time: the amount of time spend in user space running system CPU time: the amount of time spent during kernel space running wall-clock time: the whole time from the process start to end Linux IPCinter-process communication(IPC) share memory is used in our application to cowork with MPI. while IPC helps to balance memory distributed in multi computing nodes, and MPI threads are the working horse to eat shared data. there are other IPC libs, e.g. Boost.interprocess. ftok() -&gt; to generate a IPC key, based on a special file path shmget() -&gt; generate a shared memory portion and return a shm_id shmat() -&gt; attach to the shm_id and return a pointer to that shared memory shmctl() shmdt() in design, all threads call shmget() to get a pointer to the share memory section, but actually only master thread do create the portion, others read it. since all thread can access the share memory section, it’s important to keep the first returned pointer from master thread clean/unpolluated. MPIthere are a few basic MPI APIs. in our project, to benefit both CPU and memory performance, we actually need: 1) subgroup the MPI comm, 2) bind MPI threads to sockets. MPI_COMM_GROUP() MPI_Group_incl() MPI_COMM_create() apis and numctl during run-time: mpirun -bycore -bind-to-socket -&gt; bind process to each core on the socket mpirun -bysocket -bind-to-socket mpirun numctl -membind=0 -np 32 ./run]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vehicle network]]></title>
    <url>%2F2018%2F07%2F12%2Fvehicle-network%2F</url>
    <content type="text"><![CDATA[vehicle networ is what connects the different parts together as a vehile. the lower-level basicA few basic concepts: TCP/UDP socket, CAN Bus, Ethernet, MCU register, Ip address, MAC address, Ethernet driver, network configure, how to code a ECU/MCU etc. there are EE students working on this close to physical level implementation, on how to talk with register, MCU pins and controls, and some implementing the data traffic protocol in network. While both are isolated from top applications, since no matter where the data come from, either AV application or connected vehicle or media data, doesn’t make a difference. architecture typical vehicle network architecture beyond the basic, network architecture brings a good vision to understand what makes a vehicle electronicly as a whole. and which is serving all apps with v2x, in-vehicle entertainment, AV. I was thought ECU handling events the same as web server. while they are very different. each ECU is doing only one special task. e.g. ABS ECU only dealing with ABS events, there are no many ABS events happening simultaneously to seize the ECU computing resource; however, a web server have to deal with multiplex request simultaneously, either using thread pool or asynchronous event callbacks. each ECU actually has an easy life, but the bus seems easily choked since around 100+ ECU nodes in the vehicle network. however again, typical vehicle network architecture is very matured products, without data-heavy applications arising, current CAN bus is great. smart phone in wheels?one day in future, every vehicle running on road is requesting some data from cloud seamlessly, the scenario is like billions of browsers send requests to Google server seamlessly, then the cloud may face same issues in today’s web server. Service oriented architecture(SOA) is also rising in vehicle network architeture design, but what kind of services is better locally in vehicle, and which services in cloud ? as hardwares evolution, even the computing-heavy tasks, e.g. vision based detection, path planning in AV, suppose not be a burden for local ECUs. so these services make sense locally served. except that, I only image future vehicle as a smart phone on wheels. so remote cloud server do connect to, e.g. talk to another vehicle, play online game during driving, check weather, find out a parking lot, resturant or similar, or the car company want to steal user data privately? SOA is a good move to decouple ECU modules and make the network bus light, and it’s fun to try some vehicle network architecture open project or play with ROS simulator if hardware required. but in a business view, I don’t know a good story to attract investors, namely vehicle network sounds not that blinking amazing.]]></content>
      <tags>
        <tag>vehicle network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gnu-make]]></title>
    <url>%2F2018%2F06%2F29%2Fgnu-make%2F</url>
    <content type="text"><![CDATA[websocket projects recently reviewed: uWebSocket, which is used in Udacity self-driving car term2 projects. the other is libwebsocket, which is used in automotive message broker(amb). gcc compiler optionsgcc and make: a tutorial on how to compile, link and build c/c++ projects -wall : print all warning messages -c : compile into object, by default the object has same name as the source file -o : specify the output executable filename headers(.h), static libs(.lib, .a) &amp; shared libs(.dll, .so)gcc by default, links to the shared libraries if available. the compiler search “include-paths” for headers, which is specified via -L\ option or CPATH; the linker search “library-paths” to link the program into an executable object, which is specified via -L\ or LIBRARY_PATH, in addition, need to specify the library name via -l the system default “include-paths” can be found by “cpp -v” gcc environment variablesPATH is used to search executable and run-time shared libs. (woo, this suppose to replace LD-LIBRARY-PATH)CPATH is used to search “include-paths”, it’s searched after paths specified in -I&lt;dir> options. C_INCLUDE_PATH &amp; CPLUS_INCLUDE_PATH can be used to specify C &amp; C++ headersLIBRARY_PATH is used to search linking-time “library-paths”, it’s searched after paths specified in -L\ options. the standard directories is /usr/lib Linux utils1) readelf: read ELF 2) ldconfig : by default, read /etc/ls.so.conf, sets up the appropirate symbolic links in the dynamic link dir, and then write a cache to /etc/ld.so.cache, which then is used by other programs 3) ldd: to see recursive shared library dependencies 4) file: determine file type 5) nm: list symbol table of an object file, commonly-used to check if a particular func or variable is defined in an object file. “T” indicates it is defined; “U” means undefined, which should be resolved by the linker. make &amp; cmake[gnu make] (https://www.gnu.org/software/make/manual/html_node/index.html#SEC_Contents) there are many best practical, e.g. effective cmake build libuvit’s built based on GNU autotools sh autogen.sh &quot;libtoolize: AC_CONFIG_MACRO_DIR([m4]) conflicts with ACLOCAL_AMFLAGS=-I m4&quot; fixing-solution, then make all &amp; make install, so here is libuv.so, add the directory to \$LIBRARY_PATH in ~/.bashrc build uWebSocketsit’s built based on gnu make. the tests command has dependents on libuv. since already added libuv directory to \$LIBRARY_PATH, run through. if else, errors output: cannot find -luv collect2: error: ld returned 1 exit status Makefile:xx recipe for targe &apos;tests&apos; failed make: *** [tests] Error 1 try to run ./testsBin, get another error: error while loading shared libraries: libuWS.so: cannot open shared object file: No such file or directory that’s run-time error, since libuWS.so is not included in \$PATH or speically for building/testing purpose, define $LD_LIBRARY_PATH. build libwebsocketsit’s built based on cmake. there is some dependent, but should go through well. beyond the notewhen 2-months ago, looking at amb project, which has websocket plugin module, then find uWebSocket lib, during reading this tiny lib, where libuv APIs are highly-used, and soon found out libuv is actually the event-loop in nodejs. Woo, somehow they are related. build tools are highly used in each project, but prior experience is more done by luck.]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[career thoughts]]></title>
    <url>%2F2018%2F06%2F25%2Fcareer-thoughts%2F</url>
    <content type="text"><![CDATA[product data managmentan interesting meeting with material research group, these guys have some IT needs, but does’nt reach there. e.g. there are plenty of material fatigue data, including metadata, tables, S-N plots, so how to organize/manage them? one solution is from MSC Material Center[http://www.mscsoftware.com/news/msc-software-reinvents-materials-lifecycle-management-materialcenter] what triggered me is “data management” in general. In manufacturing industry, like automotive, many old style data used and storied in different departments, e.g. the material property data, load time history data for CAE department, the vehicle diagnostic logging data in vehcile control department, the vehicle dynamics records in vehicle test department. but neither are well-structured nor easy to track. two fields so far: data dashboard, requires data visulization and data mining; product lifecycle management, it’s a product driven, and may also have data dashboard needs. currently as I see, most data analysis is in bussiness driven view, not in product itself. how to accelerate product iteration through better using history products data suppose to be a big thing. on another side, industry data management is a little different than bussiness/market data. e.g. every year Ford releases a new F-150 truck, does it start out of new? no, the 2018 mois stly iterate by the 2017. so there is product data management, just managed through all component departments. maybe the questions should ask is: 1 do we get the most value from these whole product level data? 2 how to make special data manageble at subsystem level? it needs more experience in the whole process, but bring some thoughts in next career: PLM software and data management in special domain, e.g. material datacenter product. cloud CAEsince dashboard is so popluar to migrate the traditional software GUI to web/mobile app; and CAE solver can deployed in cloud, which is a better stronger reason to do user-side dashboard: with job submission, job status, and result plot/visulization sections. but why is it necessary to migrate CAE solvers in cloud? why the manufacturing product companines would like to share their prodcuts data with cloud providers? turn one step back, the most obvious reason is whenever internet is needed, e.g. communication among different end-users, cloud is good chonice; and for startup companies, who can’t afford to run jobs locally, have to migrate their calculation in cloud. I am kind of curious who is using AWS? Netflix, BMW, Autodesk! I was so suprised at first, how BMW and Autodesk would like to use AWS? anyway Netflix is data-flow based company, the core business is client-server communication, which make sense to use AWS. while BMW is using AWS for new business: connected service, all a sudden it makes sense. the car product business won’t be shared with AWS, but cloud is required as infrastructure for v2x connected service. that’s amazing. AutoDesk say a different story in cloud, since they sell CAD softwares, few people want to buy and own an expensive software but choose to pay for the service, by all meaning, this is not a new business, but AWS offer a mature channel. even though, cloud providers say they are cost-reducing, high-scalability, but I don’t think manufacturing companies will buy it due to security, instead they maintain their own clusters and share limited business in cloud. so standing in manufacturing industry, it’s better to figure out new services, which require communications through cloud, than migrate CAE solvers to cloud. that’s my second point.]]></content>
      <tags>
        <tag>automotive software</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[muduo 3]]></title>
    <url>%2F2018%2F06%2F20%2Fmuduo3%2F</url>
    <content type="text"><![CDATA[one concern yesterday: does each worker thread call loop() and run the whole active channels callback ? suppose no. let’s track the process : 123456tcpServer.start() -&gt; threadPool.start() -&gt; new EventLoopThread(eventLoopThread::threadFunc, threadName) eventLoopThread-&gt;startLoop() -&gt; thread.start() -&gt; detail::startThread() -&gt; runInThread()::func_() the real func_ during creating new eventLoopThread, is threadFunc(), in which call loop.loop(). first, this loop object is created inside this new eventLoopThread object, no issue that the loop.threadId_ is current threadID. when multiple worker threads exist, what happens ? threads A B C conns T1 T2 T3 suppose T1 is assigend to worker thread A, etc. when calling eventLoopThread A.loop(), epoll_wait() is first called and return number of active events, epoll_wait() is thread-safe, no worry. then activeChannels:HandleEvents() is called. the real executor of this handleEvents is in the TcpConnection objects, namely: T1 or T2 or T3. each TcpConnection object has its own loop-, and only the worker thread with the same loop- can execute the events. so even though threadA get the list of all active events, but only events in T1 conenction will be executed by threadA. so we can see, in per (worker) thread per epoll, each (worker) epoll looks like only return the active channels/events in current worker thread’s eventLoop. There is no conflict among multiple worker threads, since each worker threas has its own eventLoop. how about eventLoop in server/clientthe eventLoopThread is kind of triggered inside Muduo, where is Tcp server/client eventLoop triggered ? from outsiders.]]></content>
      <tags>
        <tag>muduo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[moduo 2]]></title>
    <url>%2F2018%2F06%2F19%2Fmuduo2%2F</url>
    <content type="text"><![CDATA[The several classes defined in Moduo: TcpServer, Acceptor, TcpConnection, EventLoop, epoller, channel, eventLoopThread, eventLoopThreadPool. 1) TcpServer constructioneach TcpServer suppose have multi- tcp connections, the acceptor works as the main I/O thread, which listen the server-side I/O socket, and handle all client input connections at first, (later will assign the connection task to each worker thread from pool). so the input argument “loop” during TcpServer construction is the main I/O eventLoop. 12345678910111213141516171819202122232425TcpServer::TcpServer(EventLoop* loop, const InetAddress&amp; listenAddr, const string&amp; nameArg, Option option) : loop_(CHECK_NOTNULL(loop)), ipPort_(listenAddr.toIpPort()), name_(nameArg), acceptor_(new Acceptor(loop, listenAddr, option == kReusePort)), threadPool_(new EventLoopThreadPool(loop, name_)), connectionCallback_(defaultConnectionCallback), messageCallback_(defaultMessageCallback), nextConnId_(1)&#123; acceptor_-&gt;setNewConnectionCallback( boost::bind(&amp;TcpServer::newConnection, this, _1, _2));&#125;``` one advantage of bind/callback is to import functors to different class domain. and server has always one I/O socket, but client-socket-fd suppose be a lot. ## 2) TcpServer newConnectionEach new tcp connection, will assign a worker thread to execute the speical callback functor for this new connection, How to schedule thread in threadpool is implemented by round-robin, which are implemented as:```cioLoop-&gt;runInLoop(boost::bind(&amp;TcpConnection::connectEstablished, conn)); EventLoopThreadPool::getNextLoop() does each worker thread run EventLoop.loop(), which will call epoll_wait(), and execute all active channels? suppose no.3) TcpConnection constructioneach new client connection will reponse to a new socket fd, and a new channel. (channel is actually the container of socket fd); and the functors on this connection is also imported to channel callbacks. basically from outside, we only see channel objects, TcpConnection object is the inner class. 123456789101112131415161718192021222324252627TcpConnection::TcpConnection(EventLoop* loop, const string&amp; nameArg, int sockfd, const InetAddress&amp; localAddr, const InetAddress&amp; peerAddr) : loop_(CHECK_NOTNULL(loop)), name_(nameArg), state_(kConnecting), reading_(true), socket_(new Socket(sockfd)), channel_(new Channel(loop, sockfd)), localAddr_(localAddr), peerAddr_(peerAddr), highWaterMark_(64*1024*1024)&#123; channel_-&gt;setReadCallback( boost::bind(&amp;TcpConnection::handleRead, this, _1)); channel_-&gt;setWriteCallback( boost::bind(&amp;TcpConnection::handleWrite, this)); channel_-&gt;setCloseCallback( boost::bind(&amp;TcpConnection::handleClose, this)); channel_-&gt;setErrorCallback( boost::bind(&amp;TcpConnection::handleError, this)); LOG_DEBUG &lt;&lt; "TcpConnection::ctor[" &lt;&lt; name_ &lt;&lt; "] at " &lt;&lt; this &lt;&lt; " fd=" &lt;&lt; sockfd; socket_-&gt;setKeepAlive(true);&#125; so where are channel (read/write/error/close) handleEvent callbacks triggered? it is during the eventloop.loop, after epoller return the active events, based on the status of each revent, special handleEvent is called. 4) eventLoop constructionduring constrution of eventLoop, a new poller is created based on this eventLoop itself, also a new wakeupfd and a new wakeupChannel. the purpose of wakeup fd/channel is to immediately wake up the work thread, rather than waiting till PollTime. and the wakeupfd bind handleRead, in which read one byte to make this wakeupfd I/O readable, which then is ready for I/O. 123456789101112131415161718192021222324252627EventLoop::EventLoop() : looping_(false), quit_(false), eventHandling_(false), callingPendingFunctors_(false), iteration_(0), threadId_(CurrentThread::tid()), poller_(Poller::newDefaultPoller(this)), timerQueue_(new TimerQueue(this)), wakeupFd_(createEventfd()), wakeupChannel_(new Channel(this, wakeupFd_)), currentActiveChannel_(NULL)&#123; LOG_DEBUG &lt;&lt; "EventLoop created " &lt;&lt; this &lt;&lt; " in thread " &lt;&lt; threadId_; if (t_loopInThisThread) &#123; LOG_FATAL &lt;&lt; "Another EventLoop " &lt;&lt; t_loopInThisThread &lt;&lt; " exists in this thread " &lt;&lt; threadId_; &#125; else &#123; t_loopInThisThread = this; &#125; wakeupChannel_-&gt;setReadCallback( boost::bind(&amp;EventLoop::handleRead, this)); // we are always reading the wakeupfd wakeupChannel_-&gt;enableReading();&#125; “activeChannels” suppose to be a class variable, which is shared by eventLoop objects, but it’s ok to keep a copy for each eventLoop object to avoid multi-thread competing. and suppose epoll_wait() is thread-safe, so later during construction of eventLoopThreadPool, multi eventLoopThreads won’t conflict with “active channels” 5) epoller constructionepoller object is created during eventloop construction. since the three interface of epoll instance are thread-safe, they look like global funcs. epoll_create(), return an epfd referring to the new epoll instance, this epfd is used by all subsequent calsl to the epoll interface. epoll_wait(), return all ready events on the epoll instance referred by epfd. epoll_ctl(), traverse the red-black tree strucutre to return the existing fd, or add new fd to the tree. 6) channel constructionchannel is the container of one fd, and is related to one eventLoop. channel is not responsible to create/delete fd, the real owner of each fd is TcpConnection or acceptor. Channel object works like a pipe to send fd from inner object TcpConnection to the eventLoop. the advantage here is eventLoop is independent from connections. 7) channel:update()1234channel::update() --&gt; loop-&gt;updateChannel() --&gt; poller-&gt;updateChannel() --&gt; //poller maintain a channel list, call epoll_ctl to add/return/delete the requiring channel 8) one epoll + threadpool vs per thread per epollthe first method, one global epoll listens all new connections, and send each connection callback to a new thread to execute. method 2, to listen the server I/O socket need to bind a unique epoll, in Moduo which is the acceptor epoll. then all client connection socket will be dealt with their own worker epoll.]]></content>
      <tags>
        <tag>muduo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[moduo 1]]></title>
    <url>%2F2018%2F06%2F14%2Fmuduo1%2F</url>
    <content type="text"><![CDATA[Moduo: A C++ non-blocking network library for multi-threaded server in Linux. C++ 11 featuresa. functional, bindin C++11, function declartion can be: 123456789 return-type func(args)//or auto func(args) -&gt; return_type// the good part of &quot;-&gt;&quot; is to use &quot;decltype&quot; to decide the return type from future. std::function&lt;return_type(arg1_type, arg2_type..)&gt; ; [](type1, type2..)&#123;&#125; ; //lambda func, anonymous func std::forward std::bind() is used to assign existed variables to func parameters during compile-time, and unassigned parameters stand as placeholders, which then replace by real parameters during running-time, and return a new func. bind() can be used to bind static func, global func, class member func. 12345678910 std::bind(global_func, 1.0, _2) ; std::bind(&amp;class:memberfunc, classPointer[this], _1); ``` ### b. multi-threads synchronized primitives Q: what is condition variable ? A: used to block one or more threads, till been notified by another thread or overtime been wakeup. But if all threads are waiting, that&apos;s a problem, so at least for one thread, the condition variable should be true. threadA.wait(condition_variable); while(;;) { threadB.do(); } threadB.notify(); threadA.do(); 1234567891011121314151617181920212223242526 Q: what is mutex? A: it works like a lock, to block any other thread to access special part of memory, at any time, only one thread can own the mutex. in reality, to define mutexLock class will manage lock/unlock automatically due to the constructor/destructor of the object. ### c. rvalue reference, universal reference, std::forward Q: why need right-value reference ? A: to use rvalue like lvalue, save object/variable copying, the detail is about move constructor. universal reference, means either rvalue reference or lvalue reference, is to declare a variable /parameter with type T&amp;&amp; for some deduced type T### d. functional template, variadic arguments basically, it&apos;s to support any type, any number of parameters in function template. ``` template&lt;typename… Args&gt; class tuple&#123;&#125; //a template class template&lt;typename T, typename… Args&gt; void func()&#123;&#125; /* Args :: a template type parameter pack, a list of parameters T :: a normal template type parameter */ func(T, args...) /* (...) at right of func parameters is meta operator used to unpack &quot;args&quot; into separate arguments */ a simple thread / thread pool libtake a look at: thread-pool basically, a task queue to store all todo tasks; a thread pool, to store the worker threads, each of which takes one task from the task queue continuously till the queue empty. racing condition happens when two threads try to take the same task simultaneously, so mutex. and all operators requrieing thread-safe should use mutex, e.g. enqueue/dequeue taskthen callback functors, how each worker thread deal with the task at hand? to design function template with variadic arguments. a simple Tcp network libtake a look at: simpleNetwork in server: socket() -&gt; bind() -&gt; listen() -&gt; accept() –&gt; send()/write() in client: socket() -&gt; connect() -&gt; recv()/send() a basic idea is that each connection, the server will create a new thread to handle it. But it consume server quickly. I/O nonblocking/event-driven &amp; multiplex1234567891011#include &lt;poll.h&gt; struct pollfd &#123;int fd; //file descriptor, non-negativeshort events; //events to watch, set by usershort revents; //returned events, return from system kernel &#125;;POLLIN | POLLPRI ; //event readPOLLOUT | POLLWRBAND ; //event writePOLLER ; //event errorPOLLHUP ; //event hang up the basic idea of multplex(Linux APIs: poll(), epoll()) is to use one single thread to listen many connections/socket/fd, once any socket is ready for I/O, the thread will execute the write/read callback. epoll() on success, return the number of revents or 0; on error return -1 123456789struct epoll_event &#123; __u32 events ; union&#123; void* ptr, //if need to store a pointer int fd, // if need to store socket fd __u32 u32, // to store general 32 bit number __u64 u64 // to store general 64 bit number &#125; data;&#125;;]]></content>
      <tags>
        <tag>c++11</tag>
        <tag>muduo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[coding is the system engineering]]></title>
    <url>%2F2018%2F06%2F05%2Fcoding-is-the-system-engineering%2F</url>
    <content type="text"><![CDATA[during this week studying: automotive message broker. It’s a framework to support in-vehicle network communication, based on which many vehicle applications can build up. After jumped into the source code, soonly I found the dependent third-party libs, e.t. Qt, Glib, libwebsockets. None of these basis libs looks familiar, that’s really headache. So had to jump to learn a little about these libs first. Qt is heavy, give up on reading source code, basically to know what’s used for; Glib has a core on eventloop, after review the concept: nonblocking event I/O; and few lighter projects are found, e.g. event.js in RocketEngine. so I was happy to read js source code, but after reading nothing really keep in my head due to don’t know where to use them, neither don’t understand why these implement is necessary. maybe a bigger project with more details, so picked moduo, written by a Chinese. there is a book on this project, which started with multi-thread programming issues, no help. same problem, I am not familar with the application scenario, so can’t really catch the essential. back to the question: how to effciently read source code?1) I suppose, first to understand the application scenario2) compile and run in debug to track the data flow for projects, which you can’t see the flow by once3) if it’s a framework/foundmental libs, write demo The company IT system is not developer-friendly, git, cmake, npm, python lib are missing. that’s maybe the reason I am becoming lazy, if tools at-hand, may give a try; if not, nothing really do. The company has itself gmake system, but the bottom code never been touched. Then I compiled the demo libwebsocket, many failures bump out, during school time, I used a lot cmake, g++ with local headers, link to LD/_LIBRARY/_PATH, all out of practice for couple years. Anyway, at this moment, I realize that coding is a system enginnering. first to be really familar with OS, then the basic libs, then think about to write applications. For me, the first occupy with software is from (industry) application layer, lack of foundmental. now when to do sth, I feel hands are bounded, that push me to the root. Rome is not built in one day, OMG.]]></content>
      <tags>
        <tag>automotive software</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[open source project checked]]></title>
    <url>%2F2018%2F05%2F31%2Fopen-source-project-checked%2F</url>
    <content type="text"><![CDATA[pen source projects I scatched To review how many open source projects I tried to study has been in my mind for a long time. I am always interested in new areas, new trends, and want to understand a little more, few contribution by now. One college friend, who worked at Intel China, first brought me to GTC 2008 at Beijing, where my eyes is opened first time, to know there is so many amazing works around the world. 1) HPC/FEA/CFDafter 2008, I was driven by computing mechanics more HPCG: a benchmark work to test FLOPs, which I used as a class project at Buffalo PETSC: a large sparse matrix solver, developed from Texas Austin, also met at Buffalo. the math is attracted me more and it is complex and a good learning material for C++ LibMesh: based on PETSC to do FEM. ran some demo and interested in math more GPU-SPH: GPU implementd of SPH(CFD algorithm), during 2011, GPU is popular in applied mechanics. PetIBM: immersed boundary method (CFD algorithm), used as a class project OpenFEM, OpenFOAM etc. during that period, I collected many small/big open source CFD/FEA projects, but few study. 2) Game Engineat 2011, Virtual Reality is attracting CHAI3D: my friend show me a demo, but I didn’t go through LiquidFun: google product Box2D: for fun, like LiquidFun, where cmake, configure skills played That time I also collected many other game engines from web, e.g. Unreal, skynet.. it’s so cool to be a game developer, but no time to deep in 3) Robotat 2014, robot is hot ROS: that’s my way to study the platform first, with a few study and no more SLAM: the cool concept when to uderstand the core algorithm OpenCV: used everywhere, read some docs, but not used independtely Arduino: platform, when I joined the Detroit Hacking Night(DHN) meetup RasiBerry: similar experience during DHN, later I tried to implement deep learning detection algorithm on a race car, not finished till now RTOS: go through like 2-month ago, when I feel I need more understand at embedded system. simple to understand, but not sure how can I master in practice 4) Deep Learningafter 2016, DL is so popular, many frameworks, online courses, papers bursting, no way to keep calm. TensorFlow: I thought it’s easy at first, but really didn’t go to the code, but implemented some examples Caffe: the DL framework jumped in, first knew about Google protocol buffer, very confused at that time ChatAI: I tried to add some fun in WeChat public platform 5) VehicleBuffalo Car simulator: at Buffalo lab, there was a physical car simulator, and code in OSG, cool project to learn C++ SimCar: later I had chance to know many open source/ commercial car simulators AGL: at 2017, started to view AGL updates, try to keep in mind the new trends in vehicle software fields GEVIVI: same time with AGL, interested in IVI as a sub-field openXC, SmartDeviceLink: OEM level apps 6) Web &amp; Mobileat 2017, luckly transfered to a web project, have the chance to know AngularJS, Node.js and event driven, REST, AJAX, async, many new and exciting ideas here. Hexo.io: a static blog engine web crawler: looks like a hack skill async event I/O lib: moduo, event.js top github Rankers have js projects, looks very intersting 7) OS relatedmany little tools when playing with Linux, system level, apps. One big stuff is Linux Process Communication(IPC). I used share memory in one product but never had a chance to know the big picture of IPC, later want to know about Linux network, and see socket, signal… feel connected. CMAKE: QT: Glib: MKL: intel math kernal lib, cool in first impression plugin-pattern in embedded software the good is know diversity, the short I don’t have done any contribution to combine these experince GBM]]></content>
  </entry>
  <entry>
    <title><![CDATA[W3C automotive open web platform]]></title>
    <url>%2F2018%2F05%2F22%2FW3C-automotive-open-web-platform%2F</url>
    <content type="text"><![CDATA[W3C automotive open web platformMain page: Link A clear web framework: 1) define the network communication APIs(based WebSocket protocol); 2) standarlize vehicle data; 3) define APIs to access vehicle data. Vehicle Information Service Specification(VISS)Object: to define WebSocket based APIs for a vehicle information service to enable client apps to get, set, subscribe, unsubscribe to vehicle signals, data attributesLink Vehicle Datadoc Link Object: to define a standard Vehicle Data with might be available in a vehicle 1) VehicleInterface2) VehicleCommonDataType3) VehicleConfigurationInterface (identification, sizeConfiguration, fuelConfiguration, transmissionConfiguration, wheelConfiguration, steeringWheelConfiguration)4) Running Status Interface(vehicleSpeed, wheelSpeed, engineSpeed, vehiclePowerMode, powertrain, acceleratorPedalPosition, throttlePosition, tripMeters, transmission, cruiseControlStatus, lightStatus, interiorLightStatus, horn, chime, fuel, engineOil, acceleration, engineeCoolant, steeringWheel, wheelTick, ignitionTime, gyro, brakeOperation, buttonEvent, drivingStatus, nightMode, startStopMode)5) Maintenance Interfaces(odometer, transmissionOil, transmissionClutch, brakeMaintenance, washerFluid, malfuncitonIndicator, battery Status, Tire, trouble Code, diagnostic)6) Personalization Interfaces ( languageConfiguration, unitsOfMeasures, mirror, driveMode, seatAdjustment, dashboardIllumination, vehicleSound )7) DrivingSafety Interfaces ( antilockBrakingSystem, tractionControlSystem, electroniceStabilitySystem, topSpeedLimit, airbagStatus, door, childSafetyLock, seat)8) Climate Interfaces (temperature, rainSensor, wiperStatus, defrost, sunroof, convertibleRoof, sideWindow, climateControl, atmosphericPressure)9) Vision &amp; Parking Interfaces( laneDepartureDetection, alarm, parkingBrake, parkingSensors) Vehicle Information API SpecificationObject: define a high level API for accessing vehicle signals, data attributes and communicate with in-vehicle data servers doc Link VISClient Interface used to define any on-board, off-board clients VISClientOptions Interface used to define a connection to a vehicle signal server(speicifiable by protocal, host, port) VISSubscription Interface used as return value from subscribed() methods VSS Interface used as return value from getVSS(), which should be sufficent to fully traverse the VSS tree Vehicle Information Access APIdoc Link Object: enable connectivity through in-vehicle infotainment systems and vehicle data access protocols 1) Navigator Interface: exposes the interface to vehicle information services2) Vehicle Interface: the initial entry point to get access to the vehicle information3) Zone interface: physical and logical zones4) VehicleIneterfaceCallback5) AvailableCallback6) VehicleInterfaceError (permission denied, invalid operation, timeout, invalid zone, unknown, error, message)7) VehicleInterface : the base interface to get all vehicle properties8) VehicleCOnfigurationInterface : access to static vehicle information9) VehicleSignalInterface: access to variables vehicle info10) Data Availability : available, not supported, not supported security policy, supported …]]></content>
      <tags>
        <tag>automotive software</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[where are you in next 5 years(2)]]></title>
    <url>%2F2018%2F05%2F18%2Fwhere-are-you-in-next-5-years-2%2F</url>
    <content type="text"><![CDATA[the billions level productThink about these guys in the world, who defined products/services used by billions people, they are more than model innovation, but really make a diffrence in most people’s life. e.g. iPhone, facebook, Google, they are not created from nowhere, but went through product iterations by iterations, and they only imerge when the tech, the market, the time all are in perfect. How many people in history achieved here? in any sense, it’s the history choose that person, not that person made history. so take it easy even you are not the 0.01%. in the promising marketmost people in their life time neither can be the next Jobs, nor Bill Gates. that’s the reality, no sad but clean expectation in the end before really in the last day of life. the second goal is easier, find a promising market and lead a small product, maybe lucky invovled in a domain market finally. You are not significant, but one of other 5000 competitors. Even this career path, however, you can’t expect the normal lifestyle: start at 25 and retire at 55 with enough 401K to death. If so, mostly you will be fired out someday in 30s or 40s. Chinese say, “if you dont plan far, you will be in trouble soon” so what may be the promising market subset in automotive software in next 5 years? generally say: 1) service, consulting; 2) self-business; 3)tech expert. in specificly saying: 1) electricity, energy infrastructure; 2) connected, cloud infrastructure; 3) autonomous vehicle.4) mobile apps, which is the carrier for the first 3. I mean the trend is so clear these days. No one can say he dont’ know where should stand in future. but where are you now? I like the model Elon Musk mentioned when do future plan: what’s your reality, what’s your goal, how to meet the gap, than focus on acting. will automotive software like mobile smartphones ?It’s interesting even now there are few third-party or independent “automotive software” companines, on opposite, in mobile market, there are lots of small or big third-party developing companines around Andriod, iOS, the mobile ecosystem is plenty and diversity. the reality in automotive field is lack of abstraction and separation between bottom hardware and top applications. in a mature mobile ecosystem, developer -&gt; end-usersin automotive software ecosystem, devleoper –&gt; OEM –&gt; end-users some auto allience is working on(AutoSAR). as developers, either gain some knowledge in hardware/control to solve the gap, or waiting for the day. what I expect on the way, may be like mobile time:1) automotive OS platform2) vehicle apps3) infrastures extending once the platform is unified, suppose many new auto branchs will emergy. like HTC, Xiaomi, Vivo, Huawei, Lenvo branches after Andriod OS. then new market stratgey is highly required, also traditional OEMs will be heavily impacted. OEM don’t like mobile ecosystemthis may happen soon or never, since OEM don’t like this trend. most possiblely, automotive software developers will highly rely on OEMs. so where will I stand in next 5-years ? OEM is good at strategy study and product integration. at a tech supplier, the benefit is more product driven experience. OEM suppose to have the advantage at automotive platform built-up, but reality is big Internet/software companies with product-level solutions, and OEM doesn’t. it is clear for me in next 5-years to jump into electric, connected, autonomous related industries, either at OEM level or supplier level. Also it is the time to do some study at automotive platform vendors.]]></content>
      <tags>
        <tag>automotive software</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[where are you in next 5-years]]></title>
    <url>%2F2018%2F05%2F14%2Fwhere-are-you-in-next-5-years%2F</url>
    <content type="text"><![CDATA[what industry will you be in? in every industry, the product should be the core to define all other supporting teams, the market team, the product design team, IT team, manufacture team etc. so my answer suppose to be vehicle, or maybe I even should not limit here. as an mechanical/mechanics based engieer for three years, I realized there are three aspects: 1 solid knowledge in this special field, e.g. Mechanical has concept of stress, material property; market has concept of supply chain, product strategy, customer expectation; 2 fluent with the industry tools, which give the feeling you can fix the world by it, e.g. CAD/CAE tools to mechanical, programming tools to developers; 3 passion in the product, which is the driven force to explore and creativity. Do I have passion in vehicle? Or Do I feel the pain/hurt due to any unsolved issues in vehicle? maybe no, that’s why I stay in surface even after 3-years, don’t feel the pain neither the fire. knowledge is almost the easist part, which answer “what”, the true barrier is about tools(how) and passion(why). From knowledge level, reading couple professional books will be enough to absolve the concepts in a new acknowledge. career passionBut why you want to involve in this job? most people even don’t go to why, since reality is here: after getting married, pay the house fee, kids taken after. Job is a job, to support family, period. I love family, by the way. but don’t take family as an excuse for muddy career. Is there a field that knowledge is the tool? maybe AI, so far AI products is limited, e.g. voice, image. so tools are highly overlapped with knowledge. It sounds like an easy-in field. for a while as AI maturing and becoming an common service, then AI related products will burst out in every field, then product/market/capital-driven occupy. It’s a bad strategy to chase after market hot topics to develop personal career path. Current market trend is helpful, but there is no clear next-10-year market goal; on personal level, however, you definitely need a clear next 5/10-year goal, then every small adjust should point to that goal. so basically there are two categories: knowledge-driven, which is new coming e.g. AI; market-driven, which is always matured, e.g. automotive, smart phones. I suppose the matured market, if lacking creative input(vey much based on customer needs, market trends), is going to sunset sooner or later; the knowldege-driven market is bursting on the way, and absorving/requiring more and more labors. so where I am now, and where will I be in next 5-year? How do you survive in a matured market, or are you born to be the new-comers? and is the new-comers achievable? e.g. will AI related knowledge/tools be easy to catch? if in a matured market (automotive CAE)if I dont figure out a clear 5-year goal, most possiblely I will stay very unhappy in CAE the next 5 years; also very possible I can’t tolerant the work environemnt any more, and jump to any field at that time accept me; few chance to refresh my opinions and define a CAE sub-market. there are a few concerns. First, it’s not good to change direction immidetialy anytime the market is down, that’s too risk if you don’t setup the long-term goal. Second, in a sunset field, it’s not only about salary not high, the hidden aspects includes scattered office environoment, lay-off dangerous, not been valued, which will change the person into sunset. I mean, even a sunset period still need labers to maintain, but depends on personality, not for me somehow. what may be the CAE sub-market to rise? I can feel the tools is still highly used in automotive fields, but the maturity in CAE software products requires few new development, some new try into autonomous vehicle safety will not be a burst. Another try is in cloud based environment, is more like a sub-applications for the cloud vendors, it’s kind of a pattern innovation, not a product innovation. Due to the fewer chances in mature products, the level of candicates is actually high, most openings requries PhD. the mature market is a employer market, not employees friendly. And the education system may take decades to make a shift, during the period the employee somehow have to be devalued, because of the market decide their values. if in a new market (AI)by all menas, a new market does have more chances and promising futures and all the benifits where the mature market doesn’t have. The point is how to transfer from a mature market employee to a new market employee. I mentioned before, three aspects, knowledge, tools, and passion. Love it first, and the knowledge and familar with tools will achieved. need more discussion in next time.]]></content>
      <tags>
        <tag>CAE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[what happens in automotive softwares]]></title>
    <url>%2F2018%2F05%2F07%2Fwhat-happens-in-automotive-softwares%2F</url>
    <content type="text"><![CDATA[Backgroud:Due to 6-year school experince in mechanics, I was native to jump into CAE position. But I have few willing with CAD, which is very fundemental to be a qualified Product-Design side engineer. so I am not. So I am in a CAE tools support team, where I don’t directly design, mainly in enhancement, deployment etc, where understandthe old code, and write new function code, track bugs, system configure is enough. even in the big CAE vendors companies(Altair, Dassult System), the new methodlogy invest is very possibly less than product integration and customer consultings. what’s changing:the trend of CAE tools in OEMs demenstrates less investment, not because CAE tools are less used, but the CAE methodlodgy/process is really mature; new trends are arising, e.g. connected vehicles(CV), smart city, autonomous vehicles(AV), all of which require different knowledge and mindset(from mechanical to EE/CS). In 2017 Sep, I enrolled Udacity self-driving car, which gave some knowledge(openCV, deep learning, data fusion, object detect, plan algorithm), and the problems in AV. This field is so hot that openings around everywhere, basically California companies are high-demand, looking for a strong guy, AI expert, or senior automotiveengineer; while Michigan companies are old fasion, more on system integration level, and the required skills are random. if it’s a turning pointwhile Udacity didn’t work for me and AV is too young to all-in invest.For a while I am actually back to CAE, tried to enjoy it. But bad news keep coming, less projects funded, and frustrated office environment, so I move on. this time, IoT(connected vehicles) come to mind. it is really an old topic, since 2008 at college, IBM throw the big vision: smart earth to connect everything. Decades passed, finally the infrastructure, the application levels(transport, home, offices) are prepared-well. Good for me, I track AGL, GENIVI open source projects for around a year already and had 8-months experience in mobile developement(RESTful, server-client model), all bring me some fresh idea in connected vehicles. To study the automotive embedded software system, I tracked freeRTOS first, cause it’s easy since knowldege in Linux; about applications based on RTOS, however, I have no idea, e.g. vehicle dynamics, body/engine/powertrain control components, sensors, algorithms. While they are new but not difficult, hopefully can be familar in short period. softwares in connected vehiclesThere are two sections: the vehicle development, including traditional control components, which requires professional knowledge in vehicle dynamics, ECU; Vehicle Infotainment(IVI)components, which is like mobile developement. this section already has standard architecture, e.g. AutoSar secondly the vehicle to surroundings, either cloud device, peronsal mobile devices, or other vehicles. which requires: cloud infrastructures, communication protocol, security vendors, Android/IOS mobile apps, and IoT hardware vendors. it’s a clear big market, also it presents more valuable to do business than to be an engineer in each small field. the reasons come to mind, 1)architecture is done, no big mind/theory updated ; 2) so all should be about products, the integration components to market products is virtual. on-goingCAE softwares is heavy-math/numerical algorithms demanding; on the other side, embedded softwares is like enterprise Java, more on logic flow. As the population of mobile frameworks, and standarlization in automotive embedded system, the threshold suppose become lower. On the other hand, the dependence on suppliers’ libs and the test/release on hardwares may draw the life cycle of embedded software development longer. To explore embedded softwares may not the right career path, but no doubt it’s good to know what’s happening there. Finally, Either embrace the changing or wait it come to you, I mean, both are good stragies, maybe!]]></content>
      <tags>
        <tag>automotive software</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[open source car control]]></title>
    <url>%2F2017%2F09%2F05%2Fopen-source-car-control%2F</url>
    <content type="text"><![CDATA[OSCC Introgithub it’s a modular using software to interface with a vehicle’s communication network and control systems. functions: to send control commands to the vehicle, read control messages from the vehicle’s OBD-II CAN network,f and forward reports for current vehicle control state (e.g. steering angle, wheel speed) sensors: steerng wheel torque sensor, throttle position sensor, brake position sensor issues: not safe for spoofing CAN message, or hacking firmware &amp; hardwareapplication layer (API)]]></content>
      <tags>
        <tag>automotive software</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231 -- CNN in computer vision]]></title>
    <url>%2F2017%2F07%2F24%2FCNN-demo%2F</url>
    <content type="text"><![CDATA[what happend in high dimensional space?Pixel-based distance on high-dimensional data can be very unintuitive. Linear Classification1) define a score function from image pixels to class scores. benefits, no need to store all data 2) SVM and Softmax 3) a loss function, measure the quality of a paricular set of parameters based on how well the induced scores agreed with the ground truth labels optimization (SGD)the loss function as a hihg-dimeonsional optimization landscape, in which trying to reach the bottom BPRectified linear unit (ReLU)Neural Networkstrain a small network, the final loss are relatively few local minima, and easy to converge, but they are high loss; if train large network, there may many different solutions, but the variance in final loss is much smaller. –&gt; all solutions are equally as good, rely less on the random initialization in practice, use regularization tech to control overfit on large train network Data Preprocessing1) mean subtraction 2) normalization 3) PCA &amp; whitening 4) weight initialization 5) regularization 5.1) L-norm regularization 5.2) Dropout Hyperparamter optimization1) initial learning rate 2) learning rate decay schedule 3) regularization strength (L2 penalty) tips: decay learning rate over the period of training; search for good hyperparameters with random search CNNlayers used to build ConvNet architectures: 1) Convolutional layer 2) ReLU layer 3) Pooling layer 4) Fully-connected layer case study: LeNet AlexNet ZF Net GoogleNet VGGNet ResNet Visulization CNNTransfer learning]]></content>
  </entry>
</search>