<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="what I don&apos;t know">
<meta property="og:type" content="website">
<meta property="og:title" content="Serious Autonomous Vehicles">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Serious Autonomous Vehicles">
<meta property="og:description" content="what I don&apos;t know">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Serious Autonomous Vehicles">
<meta name="twitter:description" content="what I don&apos;t know">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Serious Autonomous Vehicles</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Serious Autonomous Vehicles</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/26/boost-python-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="David Z.J. Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Serious Autonomous Vehicles">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/26/boost-python-1/" itemprop="url">boost.python 1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-26T07:19:15-04:00">
                2020-06-26
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/06/26/boost-python-1/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2020/06/26/boost-python-1/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><p>in OEM’s ADS team, there are bunch of model-based design engineers, who build the ADAS features based on Matlab/Simulink tools, which is good to build quick demos, when comes to massive data verification, we can’t really depends on Matlab solver, which is so slow and licensed. </p>
<p>so a common idea is to recompile the Matlab/Simulink model to C/C++ code, which can further embedded to more open envs, e.g. python or C++.</p>
<p>as previously mentioned, we had designed a rq based massive data driven test framework, so the gap from C++ ADAS code to this python test framework is fixed in this blog.</p>
<p>there are a few wasy to integrate C++ code to Python, one is <code>Boost.Python</code>: </p>
<h2 id="setup"><a href="#setup" class="headerlink" title="setup"></a>setup</h2><p><a href="https://www.boost.org/doc/libs/1_73_0/more/getting_started/unix-variants.html" target="_blank" rel="external">install boost</a></p>
<p><a href="https://www.boost.org/doc/libs/1_72_0/libs/python/doc/html/building/configuring_boost_build.html" target="_blank" rel="external">configuring Boost.BUild</a></p>
<p>python env :  3.6 (from conda env aeb)<br>gcc:  4.5.0 </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> BOOST_BUILD_PATH=`<span class="built_in">pwd</span>`   <span class="comment">#where we keep `user-config.jam`</span></div></pre></td></tr></table></figure>
<h4 id="user-config-jam"><a href="#user-config-jam" class="headerlink" title="user-config.jam"></a>user-config.jam</h4><figure class="highlight yml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="string">using</span> <span class="string">gcc</span> <span class="string">:</span> <span class="number">5.4</span><span class="number">.0</span> <span class="string">:</span> <span class="string">/usr/bin/g++</span> <span class="string">;</span></div><div class="line"></div><div class="line"><span class="string">using</span> <span class="string">python</span> <span class="string">:</span> <span class="number">3.6</span></div><div class="line">        <span class="string">:</span> <span class="string">"/home/anaconda3/envs/aeb/bin/python"</span></div><div class="line">        <span class="string">:</span> <span class="string">"/home/anaconda3/envs/aeb/include"</span></div><div class="line">        <span class="string">:</span> <span class="string">"/home/anaconda3/envs/aeb/include/python3.6m"</span> <span class="string">;</span> </div><div class="line"><span class="string">```</span> </div><div class="line"></div><div class="line"><span class="string">bootstrap</span> <span class="string">will</span> <span class="string">find</span> <span class="string">`user-config.jam`</span> <span class="string">from</span> <span class="string">$BOOST_BUILD_PATH.</span> </div><div class="line"></div><div class="line"><span class="string">```sh</span> </div><div class="line"><span class="string">cd</span>  <span class="string">/path/to/boost_1_73_0</span></div><div class="line"><span class="string">./bootstrap.sh</span> <span class="bullet">--help</span></div><div class="line"><span class="string">./bootstrap.sh</span> <span class="bullet">--prefix=/usr/local/</span> <span class="bullet">--show-libraries</span></div><div class="line"><span class="string">b2</span> <span class="bullet">--with-python</span> <span class="bullet">--prefix="/usr/local/"</span> <span class="string">install</span> <span class="string">variant=release</span> <span class="string">link=static</span> <span class="string">address-model=64</span> </div><div class="line"><span class="string">b2</span> <span class="bullet">--clean</span></div></pre></td></tr></table></figure>
<p><a href="https://cloud.tencent.com/developer/article/1011767" target="_blank" rel="external">a sample of user-config.jam</a></p>
<ul>
<li>error fixing </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">fatal error: pyconfig.h: No such file or directory</div><div class="line">compilation terminated.</div></pre></td></tr></table></figure>
<p>need <code>export CPATH=~/anaconda/envs/aeb/include/python3.6m/</code>,  where located <code>pyconfig.h</code> and other headers</p>
<p>finally report: boost.python build successfully !</p>
<h2 id="demo-run"><a href="#demo-run" class="headerlink" title="demo run"></a>demo run</h2><p>the following is simple sample of how to use boost_python wrapper to wrapping an AEB model(in c++) to python</p>
<h4 id="aeb-h"><a href="#aeb-h" class="headerlink" title="aeb.h"></a>aeb.h</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></div><div class="line"></div><div class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></div><div class="line">	<span class="keyword">float</span> time; </div><div class="line">	<span class="keyword">float</span> dx ;</div><div class="line">	<span class="keyword">float</span> dy ;</div><div class="line">&#125; AEB;</div><div class="line"></div><div class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></div><div class="line">	AEB out1 ;</div><div class="line">&#125; OP;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">aeb</span> &#123;</span></div><div class="line">    <span class="keyword">public</span>:</div><div class="line">        Student() &#123;&#125;</div><div class="line">	OP op_out ;</div><div class="line">        <span class="function"><span class="keyword">void</span> <span class="title">test_op</span><span class="params">()</span></span>&#123;</div><div class="line">            (<span class="keyword">void</span>) <span class="built_in">memset</span>((<span class="keyword">void</span> *)&amp;op_out, <span class="number">0</span>, <span class="keyword">sizeof</span>(OP)); </div><div class="line">            op_out.out1.time = <span class="number">1.0</span> ; </div><div class="line">            op_out.out1.dx = <span class="number">2.0</span> ;</div><div class="line">            op_out.out1.dy = <span class="number">3.0</span> ; </div><div class="line">            AEB o1 = op_out.out1 ;</div><div class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; o1.time &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span> ;</div><div class="line">        &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<h4 id="wrap-aeb-cpp"><a href="#wrap-aeb-cpp" class="headerlink" title="wrap_aeb.cpp"></a>wrap_aeb.cpp</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;Python.h&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;boost/python.hpp&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;boost/python/suite/indexing/vector_indexing_suite.hpp&gt;</span></span></div><div class="line"></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"aeb.h"</span></span></div><div class="line"></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> boost::python;</div><div class="line"></div><div class="line">BOOST_PYTHON_MODULE(aeb) &#123;</div><div class="line">    scope().attr(<span class="string">"__version__"</span>) = <span class="string">"1.0.0"</span>;</div><div class="line">    scope().attr(<span class="string">"__doc__"</span>) = <span class="string">"a demo module to use boost_python."</span>;</div><div class="line">    class_&lt;aeb&gt;(<span class="string">"aeb"</span>, <span class="string">"a class of aeb"</span>)</div><div class="line">        .def(init&lt;&gt;())</div><div class="line">        .def(<span class="string">"test_op"</span>, &amp;aeb::test_op, <span class="string">"test op"</span>)</div><div class="line">        .def_readonly(<span class="string">"op_out"</span>, &amp;aeb::op_out)</div><div class="line">&#125;</div><div class="line"></div><div class="line">``` </div><div class="line"></div><div class="line">tips, <span class="keyword">if</span> aeb.h <span class="keyword">and</span> aeb.cpp are separated files, it's better to merge them first;  <span class="keyword">for</span> nested structure in wrapper is another topic later.</div><div class="line"></div><div class="line">###<span class="meta"># build and python import </span></div><div class="line"></div><div class="line">* check header location</div><div class="line"></div><div class="line">      Python.h @ `/home/anaconda3/envs/aeb/include/python3<span class="number">.6</span>m`</div><div class="line"></div><div class="line">      boost/python @ `/usr/local/include/boost`</div><div class="line"></div><div class="line">* check .so lib location</div><div class="line">     </div><div class="line">       /usr/local/lib/</div><div class="line"></div><div class="line"></div><div class="line">tips, <span class="keyword">if</span> there is duplicated `boost lib` in system, e.g. `/usr/lib/x86_64-linxu-gnu/libboost_python.so` which maybe conflict with `boost_python` install location at `/usr/local/lib/libboost_python36.so`</div><div class="line"></div><div class="line">* build </div><div class="line"></div><div class="line">```sh</div><div class="line">g++ -I/home/anaconda3/envs/aeb/include/python3<span class="number">.6</span>m -I/usr/local/include/boost -fPIC wrap_aeb.cpp -L/usr/local/lib/ -lboost_python36 -shared -o aeb.so</div></pre></td></tr></table></figure>
<ul>
<li>test </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> aeb</div><div class="line">t = aeb.aeb</div><div class="line">t.test_op()</div></pre></td></tr></table></figure>
<h2 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h2><p>this blog gives the basic idea how to use <code>boost.python</code> to integrate c++ to python test framework. there are plenty details need fixed, e.g. nested structures, share_pointers. maybe share in next blog.</p>
<h4 id="refere"><a href="#refere" class="headerlink" title="refere"></a>refere</h4><p><a href="https://www.boost.org/doc/libs/1_66_0/libs/python/doc/html/tutorial/index.html" target="_blank" rel="external">boost.python tutorial</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/22/play-with-ros-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="David Z.J. Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Serious Autonomous Vehicles">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/22/play-with-ros-2/" itemprop="url">play with ros 2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-22T08:32:13-04:00">
                2020-06-22
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/06/22/play-with-ros-2/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2020/06/22/play-with-ros-2/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="ros-msgs"><a href="#ros-msgs" class="headerlink" title="ros msgs"></a>ros msgs</h4><p>ros msgs usually used in C++, as our ADS data tool is implemented in Python, I’d try to build <code>*.msg</code> to python module. find two blogs from ROS doc: </p>
<p><a href="http://wiki.ros.org/rospy_tutorials/Tutorials/Makefile" target="_blank" rel="external">writing a ROS python Makefile</a></p>
<p><a href="http://wiki.ros.org/ROS/Tutorials/CreatingMsgAndSrv#Common_step_for_msg_and_srv" target="_blank" rel="external">create a ros msg</a></p>
<p>to build msg into python module, the <code>package.yml</code> should at least include the following lines:</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="string">&lt;buildtool_depend&gt;catkin&lt;/buildtool_depend&gt;</span></div><div class="line"><span class="string">&lt;build_depend&gt;message_generation&lt;/build_depend&gt;</span></div><div class="line"><span class="string">&lt;build_export_depend&gt;rospy&lt;/build_export_depend&gt;</span></div><div class="line"><span class="string">&lt;build_export_depend&gt;std_msgs&lt;/build_export_depend&gt;</span></div><div class="line"><span class="string">&lt;exec_depend&gt;rospy&lt;/exec_depend&gt;</span></div><div class="line"><span class="string">&lt;exec_depend&gt;std_msgs&lt;/exec_depend&gt;</span></div></pre></td></tr></table></figure>
<p>the default <code>CMakeList.txt</code> looks like: </p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="string">find_package(catkin</span> <span class="string">REQUIED</span> <span class="string">COMPONENTS</span></div><div class="line">	<span class="string">roscpp</span></div><div class="line">	<span class="string">rospy</span></div><div class="line">	<span class="string">std_msgs</span></div><div class="line">	<span class="string">message_generation</span></div><div class="line"><span class="string">)</span> </div><div class="line"></div><div class="line"><span class="string">add_message_files(</span></div><div class="line">	<span class="string">FILES</span></div><div class="line">	<span class="string">custom.msg</span></div><div class="line"><span class="string">)</span></div><div class="line"></div><div class="line"><span class="string">generate_messages(</span></div><div class="line">	<span class="string">DEPENDENCIES</span></div><div class="line">	<span class="string">std_msgs</span></div><div class="line"><span class="string">)</span></div></pre></td></tr></table></figure>
<p>the generated msg python module is located at <code>~/catkin_ws/devel/lib/python2.7/dist-packages/my_msg_py/msg</code>,which can add to <code>$PYTHONPATH</code> for later usage</p>
<h4 id="create-ros-node-with-catkin"><a href="#create-ros-node-with-catkin" class="headerlink" title="create ros node with catkin"></a>create ros node with catkin</h4><p>first check your <code>$ROS_PAKCAGE_PATH</code>, the default pkg path is <code>/opt/ros/kinetic/share</code>, append custom pkgs path from <code>~/cakin_ws/devel/share</code>. </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> ~/my_catkin_ws/src</div><div class="line">catkin_create_pkg my_pkg [dependencies, e.g. sd_msgs rospy]</div><div class="line">rospack find</div></pre></td></tr></table></figure>
<p><code>catkin_create_pkg</code> will create a <code>CMakeList.txt</code> at pkg level, and a <code>src</code> folder, where can hold custom nodes definition.</p>
<h4 id="sensor-serial-data-to-ros-node"><a href="#sensor-serial-data-to-ros-node" class="headerlink" title="sensor serial data to ros node"></a>sensor serial data to ros node</h4><p>sensors(e.g. rtk, imu) to ros is communication from external world to ros sys. Things need to take care:  mostly sensor hardware device doesn’t support ROS driver directly, so first need device serial or CAN or Ethernet to get the raw sensor data, and package it as <code>sensor/raw_msg</code> to publish out; the real ros-defined sensor node, will subscribe <code>sensor/raw_msg</code> and publish the repackaged <code>sensor/data</code> to the ros system, (which usually happened in ros callback).</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rtk_cb</span><span class="params">(std_msgs::ByteMultiArray raw_msg)</span>:</span></div><div class="line">    rtk_msg = func(raw_msg)</div><div class="line">    pub.publish(rtk_msg)</div><div class="line"></div><div class="line">pub = nodeHandler.advertise&lt;sensor_msgs:NavSatFix&gt;(<span class="string">"/rtk_gps/data"</span>, <span class="number">1</span>)</div><div class="line">sub = nodeHandler.subscribe(<span class="string">"/rtk_gps/raw_data"</span>, <span class="number">1</span>, rtk_cb)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">raw_data_generator</span><span class="params">()</span>:</span></div><div class="line">	<span class="keyword">try</span>:	</div><div class="line">		<span class="keyword">with</span> open(<span class="string">"/dev/ttyS0"</span>, <span class="string">"r|w"</span>) <span class="keyword">as</span> fd:</div><div class="line">			header = read(fd, buf, header_line)</div><div class="line">			<span class="keyword">while</span> ros::ok():</div><div class="line">				content = read(fd, buf, content_lines)</div><div class="line">				std::msgs::ByteMultiArray raw_msg </div><div class="line">				raw_msg.data.push_back(header)</div><div class="line">				raw_msg.data.push_back(content)	</div><div class="line">				pub.publish(raw_msg)</div><div class="line">		close(fd)</div><div class="line">	<span class="keyword">except</span>:</div><div class="line">		print(<span class="string">"failed to read raw data\n"</span>)</div></pre></td></tr></table></figure>
<h4 id="sensor-CAN-data-to-ros-node"><a href="#sensor-CAN-data-to-ros-node" class="headerlink" title="sensor CAN data to ros node"></a>sensor CAN data to ros node</h4><p>sensors(such as camera, radar, lidar e.t.c) go to ros sys through Veh CAN Bus. the difference between <strong>CAN</strong> msg and <strong>serial</strong> msg is data atomicity. as serial msg is only one variable, which gurantee atomicity in application level;  while each CAN frame usually include a few variables, which need custom implement atomicity in application level. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">thread0 = pthread_create(recv_thread, raw_can_data)</div><div class="line">thread1 = pthread_create(send_thread)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">thread0</span><span class="params">()</span>:</span></div><div class="line"> 	 recv_data = func(raw_can_data)</div><div class="line">	 pthread_mutex_lock(mutex_lock)</div><div class="line">	 sensor_raw = recv_data </div><div class="line">	 sem_post(sem_0)</div><div class="line">	 pthread_mutex_unlock(mutex_lock)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">thread1</span><span class="params">()</span>:</span></div><div class="line">	pthread_mutex_lock(mutex_lock)</div><div class="line">	sensor_ros_msg =  sensor_raw </div><div class="line">	pthread_mutex_unlock(mutex_lock)</div><div class="line">	node_.publish(sensor_ros_msg)</div></pre></td></tr></table></figure>
<h4 id="ros-node-to-external-device"><a href="#ros-node-to-external-device" class="headerlink" title="ros node to external device"></a>ros node to external device</h4><p>another kind of communication, is from ros system to external device/env, such as dSPACE. the external device, if not communication through serial, then usually support Ethernet(udp/tcp), which then need to implement a custom udp/tcp data recv/send func. the ros node subscribe the necessary data, then send it out through udp/tcp.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cb</span><span class="params">(sensor_msg)</span>:</span></div><div class="line">	data = repack(sensor_msg)</div><div class="line">	udp.send(data)</div><div class="line"></div><div class="line">sub = nodeHandler.subscribe(<span class="string">"/useful/data"</span>, <span class="number">1</span>, cb)</div></pre></td></tr></table></figure>
<h4 id="xml-rpc-amp-amp-tcpros"><a href="#xml-rpc-amp-amp-tcpros" class="headerlink" title="xml-rpc &amp;&amp; tcpros"></a>xml-rpc &amp;&amp; tcpros</h4><p>the ros sytem has two communication, to register/update ros node, publish/subscribe topics to ros master. this kind of message go through <code>xml-rpc</code>. after worker nodes registered in master node, the P2P communication can generated, and the data is transfered through <code>tcpros</code>. </p>
<p>each ros node has a <code>xml-rpc</code> server, in code, <code>nodeHandler.advertise()</code> called in publisher/subscriber node, to register their topices to ros master.</p>
<p>once a subscribe node register to master, which topics it subscribed, master returns a URI as response, then the subscriber and publisher can build connection through this URI. when a publish node register to master, master call <code>publisherUpdate()</code> to notify all subscriber, who subscribe topices from this publisher.</p>
<h4 id="ros-visual-rviz"><a href="#ros-visual-rviz" class="headerlink" title="ros visual(rviz)"></a>ros visual(rviz)</h4><p><a href="https://github.com/jstnhuang/ros-rviz" target="_blank" rel="external">ros-rviz</a></p>
<ul>
<li>how rviz works ?</li>
</ul>
<p>If you want to create a node providing a set of interactive markers, you need to instantiate an InteractiveMarkerServer object. This will handle the connection to the client (usually RViz) and make sure that all changes you make are being transmitted and that your application is being notified of all the actions the user performs on the interactive markers. </p>
<p><img src="http://wiki.ros.org/rviz/Tutorials/Interactive%20Markers:%20Getting%20Started?action=AttachFile&amp;do=get&amp;target=interactive_marker_architecture.png" alt="image"></p>
<ul>
<li>rviz rosbag </li>
</ul>
<p><strong>rviz config</strong> can customize the rviz display, the default located at <code>~/.rviz/default.rviz</code>. </p>
<p>the idea to play rosbag and render in rviz is to define a custom node, to receive the custom <code>pkg_msg</code> from replayed rosbag, then repckage <code>pkg_msg</code> as corresponded <code>marker/markerArray</code>, then publish these msg out, which will be received by <code>rviz</code></p>
<p>usually we define a custom node to receive replayed topics from <code>rosbag.play()</code>, and define a callback func to publish its marker objects out. </p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">ros::Publisher markerArray </div><div class="line">def pkg_cb(sensor_pkg):</div><div class="line">	for objIdx in sensor_pkg.ObjNum:</div><div class="line">		prepare_marker(marker, sensor_pkg.objects[objIdx]</div><div class="line">		SensorDisplay.markers.append(marker)	</div><div class="line">	markerArray.publish(SensorDisplay)</div><div class="line">	SensorDisplay-&gt;markers.clear()</div><div class="line"></div><div class="line">subPkg = nodeHandler.subscribe("sensor_pkg", 1, pkg_cb);</div><div class="line">markerArray = nodeHandler.advertise&lt;visulization_msgs::MarkerArray&gt;("sensor_pkg", 1)</div></pre></td></tr></table></figure>
<h4 id="ros-summary"><a href="#ros-summary" class="headerlink" title="ros summary"></a>ros summary</h4><p>ros is a very common communciation way and message type in ADS dev, many demo are implemented based on ros, which gives a bunch of ros related tools. in this blog, we review three of them:</p>
<ul>
<li>ros based sensor device data collection </li>
<li>ros rviz </li>
<li>rosbag.play </li>
</ul>
<p>which can support a few kinds of applications, e.g. debuging, replay, data collection.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/04/redis-queue-rq-in-data-processing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="David Z.J. Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Serious Autonomous Vehicles">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/04/redis-queue-rq-in-data-processing/" itemprop="url">redis queue(rq) in data processing</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-04T08:49:49-04:00">
                2020-06-04
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/06/04/redis-queue-rq-in-data-processing/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2020/06/04/redis-queue-rq-in-data-processing/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><p>recently thinking about how to design middleware and frameworks in self-driving team, just like the back-end service in most web servicees, which gives the way to go uppper and go abstract. currently many start-up ADS team, espcially Internet-based start-ups, have built-in ADS frameworks and middlewares, to support their daily dev and easily to implement new features. here is an old topic, <a href="https://zjli2013.github.io/2020/04/28/redis-task-queue-2/" target="_blank" rel="external">redis task queue</a>, how to design a robost <code>data pipeline</code> to support ADAS/ADS functions virtual test with physical collected road data.</p>
<p>the experience to put lgsvl into swarm/k8s brings the idea about <code>micro-services</code>, which is a great way to decouple a large work to a few small pieces of independent but communicatable services. so when coming to handle large data set, which is very common in ADS dev. </p>
<p>so the first idea is to decouple the <code>data pipeline</code> as a few micro-services:  reader service, process service, post-analysis service e.t.c</p>
<p>then two questions immediately up:  which data/message type fits, which network communication protocol fits.and beyond these two basic questions, also need a lot work about message adapter among/in services. previously, I designed <code>websocket and json</code> solution. but it’s too tedious to plug in a <code>ws-server/client</code> at the front/end of each service, especially as the number of serivces grows. </p>
<p>take it back, <code>data pipeline</code> is a heavy data IO work, is it really smart to split the work into a few pieces, then find the network communication among them ? we increase the system complex by introducing additional network communcation moduels, and the only benefit is decouple a heavy data IO work. and more, the network modules need consider cache, job scheduler, load balance issues, as the data process service may take much longer than reader services. </p>
<p>traditionally, heavy data IO work is common run in <code>batch processing</code>, disregarding network issues, and it’s better to run directly in memory/cache. so I go to <a href="https://github.com/rq/rq" target="_blank" rel="external">rq</a></p>
<h2 id="interprocess-communication-in-distributed-system"><a href="#interprocess-communication-in-distributed-system" class="headerlink" title="interprocess communication in distributed system"></a>interprocess communication in distributed system</h2><p><code>MPI</code> is the standard for IPC in HPC apps, of course there are <code>Linux IPC</code> libs, which brings more low-level ipc APIs. <code>MPI</code> apps mostly run on high performance computing cluster, which has the samilar API e.g. <code>Allreduce</code> as <code>Hadoop/MapReduce</code>, while the difference <code>MPI/allReduce</code> doesn’t tolerate failure, which means any node failed, the <code>MPI</code> apps failed. Which is the foundmental difference from HPC to distributed system nowadays, really popular as the new infrastructure for cloud and AI. </p>
<p>in the distributed system, there are a few ways to do interprocess communication:</p>
<ul>
<li><p><strong>RESTful protocol</strong>, such as TCP, UDP, websocket. </p>
</li>
<li><p><strong>async communication</strong>, there are different ways to implement async interprocess communication, one way is <code>message queue</code>, of course many language, e.g. js, go have some light-weight libs/framework to support ansyc communication interprocessly.</p>
</li>
<li><p><strong>rpc</strong>, <a href="">thrift</a> is an Apache project, <a href="">grpc</a> is high efficient with protobuf, but it doesn’t support well service discovery/load balance mechanism inside, which is a limitation in cloud-native applications.  <a href="">dubbo</a> has a better design for service discovery and load balance, the message type by default is <code>json</code>. so all of these can be the corner-stone service in modern micro service envs. also the common <code>micro-service framework</code>, e.g. Spring Cloud has interprocess communication component as well.</p>
</li>
</ul>
<p>for data hungry services, <code>batch processing</code> frameworks, e.g. Spring Batch, Linux Parallel should also consider.</p>
<h2 id="rq"><a href="#rq" class="headerlink" title="rq"></a>rq</h2><p>the following is from <a href="http://python-rq.org/docs/" target="_blank" rel="external">rq doc</a></p>
<h4 id="Queues"><a href="#Queues" class="headerlink" title="Queues"></a>Queues</h4><p>a <code>job</code> is a Python object, namely a function that is invoked async in a worker process. <code>enqueueing</code> is simply pushing a reference to the func and its ars onto a queue.</p>
<p>we can add as many Queue instance as we need in one Redis instance, the Queue instance can’t tell each other, but they are hosted in the same redis instance, which gives the way to find jobs binding to Queue1 in worker2 from Queue2</p>
<h4 id="jobs"><a href="#jobs" class="headerlink" title="jobs"></a>jobs</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">job1 = q.enqueue(my_func, func_args)</div><div class="line">job2 = Job.create(my_func, ttl=<span class="number">100</span>, failure_ttl=<span class="number">10</span>, depends_on=, description=, func_args)</div><div class="line">q.enqueue_job(job2)</div></pre></td></tr></table></figure>
<ul>
<li>timeout: specifies the max runtime of job before it’s interrupted and marked as failed. </li>
<li>ttl: specifies the maximum queued time(in sec) of the job before it’s dscarded. default is None(infinite TTL)</li>
<li>failure_ttl: specifies how long(in sec) failed jobs are kept(default to 1 years)</li>
</ul>
<p>the following sample is a way to find all <code>rq:job:</code>s, but the return is a bytes object, which need encode as <code>utf-8</code> for any further usage.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> redis</div><div class="line">r = redis.StrictRedis()</div><div class="line">r.keys()</div><div class="line"><span class="keyword">for</span> key <span class="keyword">in</span> r.scan_iter(<span class="string">"rq:job:*"</span>):</div><div class="line">	print(key.encode(<span class="string">'utf-8'</span>)</div></pre></td></tr></table></figure>
<h4 id="workers"><a href="#workers" class="headerlink" title="workers"></a>workers</h4><p>workers will read jobs from the given queues(the order is important) in an endless loop, waiting for new work to arrive when all jobs done. each worker will process a single job at a time. by default, workers will start working immediately and wait until new jobs. another mode is <code>burst</code>, where to finish all currently avaiable work and quit asa all given queues are emptied.</p>
<p><code>rq worker</code> shell script is a simple <code>fetch-fork-execute</code> loop</p>
<h2 id="connections"><a href="#connections" class="headerlink" title="connections"></a>connections</h2><p>when you want to use multiple connections, you should use <code>Connection</code> contexts or pass connections around explicitly. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">conn1 = Redis(<span class="string">'localhost'</span>, <span class="number">6379</span>)</div><div class="line">conn2 = Redis(<span class="string">'remote.host.org'</span>, <span class="number">9836</span>)</div><div class="line"></div><div class="line">q1 = Queue(<span class="string">'foo'</span>, connection=conn1)</div><div class="line">q2 = Queue(<span class="string">'bar'</span>, connection=conn2)</div></pre></td></tr></table></figure>
<p>Every job that is enqueued on a queue will know what connection it belongs to. The same goes for the workers.<br>within the Connection context, every newly created RQ object instance will have the connection argument set implicitly.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">setUp</span><span class="params">(self)</span>:</span></div><div class="line">    push_connection(Redis())</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tearDown</span><span class="params">(self)</span>:</span></div><div class="line">    pop_connection()</div></pre></td></tr></table></figure>
<p>this should be the way to handle distributed queues. </p>
<h4 id="results"><a href="#results" class="headerlink" title="results"></a>results</h4><p>if a job returns a <code>non-None</code> value, the worker will write that return value back to the job’s Redis hash under <code>result</code> key. the job’s Redis hash itself expire in 500sec by default after the job is finished.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">q.enqueue(foo, result_ttl=<span class="number">86400</span>)  <span class="comment"># result expires after 1 day</span></div><div class="line">q.enqueue(func_without_rv, result_ttl=<span class="number">500</span>)  <span class="comment"># job kept explicitly</span></div></pre></td></tr></table></figure>
<p>when an exception is thrown inside a job, it’s caught by the worker, serialized and stored under <code>exc_info</code> key. By default, jobs should execute within 180 seconds. After that, the worker kills the work horse and puts the job onto the failed queue, indicating the job timed out.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">q.enqueue(mytask, args=(foo,), kwargs=&#123;<span class="string">'bar'</span>: qux&#125;, job_timeout=<span class="number">600</span>)  <span class="comment"># 10 mins</span></div></pre></td></tr></table></figure>
<h4 id="job-registries"><a href="#job-registries" class="headerlink" title="job registries"></a>job registries</h4><p>each queue maintains a set of Job Registries. e.g.  <code>StartedJobRegistry</code>, <code>FinishedJobRegistry</code> e.t.c. we can find these after log in <code>redis-cli</code></p>
<h4 id="version-bug"><a href="#version-bug" class="headerlink" title="version bug"></a>version bug</h4><p>when run <code>rq</code> demo, it reports: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">raise</span> RedisError(<span class="string">"ZADD requires an equal number of "</span></div><div class="line">redis.exceptions.RedisError: ZADD requires an equal number of values <span class="keyword">and</span> scores</div></pre></td></tr></table></figure>
<p>manually change <code>/rq/registry.py</code>:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"> <span class="comment"># return pipeline.zadd(self.key, &#123;job.id: score&#125;)</span></div><div class="line"><span class="keyword">return</span> pipeline.zadd(self.key, job.id, score)</div></pre></td></tr></table></figure>
<h2 id="data-pipeline-for-ADS-function-verification"><a href="#data-pipeline-for-ADS-function-verification" class="headerlink" title="data pipeline for ADS function verification"></a>data pipeline for ADS function verification</h2><ul>
<li>queues </li>
</ul>
<p>each queue instance can be taken as a separate namespace in the Redis instance, so the workers only process the jobs in the same queue. but if multi-queues are hosted in the same Redis instance, then Redis api can find Queue A’s jobs in Queue B’s workers. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">conn = Redis() </div><div class="line">mf4q = Queue(<span class="string">'mf4Q'</span>, connection=conn)</div><div class="line">aebq = Queue(<span class="string">'aebQ'</span>, connection=conn)</div><div class="line">dbq = Queue(<span class="string">'dbQ'</span>, connection=conn)</div></pre></td></tr></table></figure>
<ul>
<li>jobs</li>
</ul>
<p>if the handler_fun has return values, namely status. <code>rq</code> store its status at <code>job.result</code>, thle lifecycle of which can be controlled by <code>result_ttl</code>, e.t.c.</p>
<p>to control the order of running the jobs, jobs can have <code>depend</code>. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mf4_jober</span><span class="params">(url_path)</span>:</span></div><div class="line">    mf4_job = mf4q.enqueue(mf4_reader, args=(url_path,), timeout=<span class="number">60</span>, ttl=<span class="number">60</span>, failure_ttl=<span class="number">1</span>,  job_timeout=<span class="number">60</span>, result_ttl=<span class="number">60</span>, job_id=mf4_job_id)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">aeb_jober</span><span class="params">(mf4_frames)</span>:</span></div><div class="line">    aeb_job = aebq.enqueue(aeb_oneStep, args=(i_, ), timeout=<span class="number">60</span>, ttl=<span class="number">20</span>, failure_ttl=<span class="number">1</span>, result_ttl=<span class="number">10</span>, job_id=aeb_job_id)</div><div class="line">	</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">db_jober</span><span class="params">(aeb_frame, idx)</span>:</span></div><div class="line">    db_job= dbq.enqueue(db_oneStep, args=(aeb_frame,), timeout=<span class="number">60</span>, ttl=<span class="number">20</span>, failure_ttl=<span class="number">1</span>, job_id=db_job_id)</div></pre></td></tr></table></figure>
<ul>
<li>workers </li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">def mf4_workers(conn, num_workers=1):</div><div class="line">	for i in range(num_workers):</div><div class="line">		worker_ = Worker([mf4q], connection=conn, name=worker_name)</div><div class="line">		workers_.append(worker_)</div><div class="line">	for w in workers_:</div><div class="line">		w.work(burst=True)</div><div class="line">def aeb_workers()</div><div class="line">def db_workers()</div></pre></td></tr></table></figure>
<ul>
<li>runners</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">runner</span><span class="params">(conn)</span>:</span></div><div class="line">	mf4_workers(conn)</div><div class="line">	<span class="keyword">for</span> k <span class="keyword">in</span> conn.scan_iter(<span class="string">"rq:job:mf4_job_*"</span>):</div><div class="line">		t_ = k.decode(<span class="string">'utf-8'</span>)</div><div class="line">		j_ = Job.fetch(t_[<span class="number">7</span>:], connection=conn)</div><div class="line">		aeb_jober(j_.result)</div><div class="line">	input(<span class="string">"hold on ..."</span>)</div><div class="line">	aeb_workers(conn)</div></pre></td></tr></table></figure>
<p>since the output of <code>mf4</code> jobs is the input of <code>aeb</code>, so we need <code>runners</code>, similar for <code>db</code>.</p>
<h2 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h2><p> <code>rq</code> is a good framework for this level data pipleine. for even bigger and complex system, <code>rq</code> maybe just a middleware, and there should be an even larger framework. the software engineering idea about <code>framework</code> and <code>middleware</code> in a large system gradually become the foundataion of ADS team</p>
<p>in distributed system, there are a few basic concepts <code>distributed consensus</code>, the popular choice e.g. zookeeper, etcd; <code>interprocess communication</code>, <code>distributed cache</code> e.t.c. really cool to know.</p>
<h2 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h2><p><a href="http://xiaorui.cc/" target="_blank" rel="external">xiaorui blog: rq</a></p>
<p><a href="https://www.cnblogs.com/jpwahaha/p/10601096.html" target="_blank" rel="external">微服架构中的进程间通信</a></p>
<p><a href="https://www.cnblogs.com/imyalost/p/6792724.html" target="_blank" rel="external">微服务架构</a></p>
<p><a href="https://www.tuicool.com/articles/QreqUnU" target="_blank" rel="external">使用gRPC构建微服务</a></p>
<p><a href="https://blog.csdn.net/fly910905/article/details/100016003" target="_blank" rel="external">微服务, 通信协议对比</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1081697" target="_blank" rel="external">理解批处理的关键设计</a></p>
<p><a href="https://www.cnblogs.com/rookiemzl/p/9788002.html" target="_blank" rel="external">spring batch 批处理</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/05/23/Linux-iptables/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="David Z.J. Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Serious Autonomous Vehicles">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/05/23/Linux-iptables/" itemprop="url">Linux iptables</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-05-23T08:21:38-04:00">
                2020-05-23
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/05/23/Linux-iptables/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2020/05/23/Linux-iptables/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><p>从<a href="http://www.zsythink.net/archives/1199" target="_blank" rel="external">iptable详解</a>整理的。 理解iptables，是为了更好的理解k8s中网络。</p>
<p>从逻辑上讲。防火墙可以大体分为主机防火墙和网络防火墙。</p>
<p>主机防火墙：针对于单个主机进行防护。</p>
<p>网络防火墙：往往处于网络入口或边缘，针对于网络入口进行防护，服务于防火墙背后的本地局域网。</p>
<p><strong>iptables</strong>其实不是真正的防火墙，我们可以把它理解成一个客户端代理，用户通过iptables这个代理，将用户的安全设定执行到对应的”安全框架”中，这个”安全框架”才是真正的防火墙，这个框架的名字叫<strong>netfilter</strong></p>
<p>Netfilter是Linux操作系统核心层内部的一个数据包处理模块，它具有如下功能：</p>
<ul>
<li><p>网络地址转换(Network Address Translate)</p>
</li>
<li><p>数据包内容修改</p>
</li>
<li><p>以及数据包过滤的防火墙功能</p>
</li>
</ul>
<p>规则一般的定义为”如果数据包头符合这样的条件，就这样处理这个数据包”. 规则分别指定了源地址、目的地址、传输协议（如TCP、UDP、ICMP）和服务类型（如HTTP、FTP和SMTP）等.  当数据包与规则匹配时，iptables就根据规则所定义的方法来处理这些数据包，如放行（accept）、拒绝（reject）和丢弃（drop）等。配置防火墙的主要工作就是添加、修改和删除这些规则。</p>
<p>如果我们想要防火墙能够达到”防火”的目的，则需要在内核中设置关卡，所有进出的报文都要通过这些关卡，经过检查后，符合放行条件的才能放行，符合阻拦条件的则需要被阻止，于是，就出现了input关卡和output关卡，而这些关卡在iptables中不被称为”关卡”,而被称为”链”。当客户端发来的报文访问的目标地址并不是本机，而是其他服务器，当本机的内核支持IP_FORWARD时，我们可以将报文转发给其他服务器。这个时候，我们就会提到iptables中的其他”关卡”，也就是其他”链”，他们就是  “路由前”、”转发”、”路由后”，他们的英文名是</p>
<p>PREROUTING、FORWARD、POSTROUTING</p>
<p><img src="http://www.zsythink.net/wp-content/uploads/2017/02/021217_0051_2.png" alt="image"></p>
<p>当我们定义iptables规则时，所做的操作其实类似于”增删改查”。</p>
<p>“关卡“/”链“ 包括：  prerouting, INPUT, OUTPUT, FORWARD, POSTROUTING</p>
<p>表： filter 负责过滤(iptables_filter); nat(network address translation), 网络地址转发， mangle, raw</p>
<p>表名：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">iptables -t filter -L (INPUT/OUTPUT/FORWARD/DOCKER-USER/DOCKER-ISOLATION-STAGE/KUBE-EXTERNAL-SERVICES/KUBE-FIREWALL/UBE-FORWARD/KUBE-KUBELET-CANARY/KUBE-SERVICES)</div></pre></td></tr></table></figure>
<p>-t选项，查看表名(filter/mangle/nat)<br>-L 选项，规则/链名<br>-n 选项，表示不对IP地址进行名称反解，直接显示IP地址。<br>–line-number 选项， 显示规则的编号</p>
<p>规则大致由两个逻辑单元组成，匹配条件与动作。 </p>
<ul>
<li>测试1：拒绝某个远程主机(10.20.180.12)访问当前主机(10.20.181.132)</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">iptables -t filter -I INPUT -s 10.20.180.12  -j DROP</div></pre></td></tr></table></figure>
<ul>
<li>-I 选项, 插入规则到哪个链</li>
<li>-s 选项，匹配条件中的源地址</li>
<li>-j 选项，当匹配条件满足，采取的动作</li>
</ul>
<ul>
<li>测试1.1：拒绝某个远程主机(10.20.180.12)访问当前主机(10.20.181.132)，再追加一条接受(10.20.180.12)的访问</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">iptables -A INPUT -s 10.20.180.12 -j ACCEPT</div></pre></td></tr></table></figure>
<ul>
<li>-A选项，追加规则到某个链。 </li>
</ul>
<p>不通 即规则的顺序很重要。如果报文已经被前面的规则匹配到，iptables则会对报文执行对应的动作，即使后面的规则也能匹配到当前报文，很有可能也没有机会再对报文执行相应的动作了。</p>
<ul>
<li>测试2: 根据规则编号去删除规则</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">iptables --line -vnL INPUT</div><div class="line">iptables -t filter -D INPUT N</div></pre></td></tr></table></figure>
<ul>
<li>-D选项，删除某条链上的第N条规则</li>
</ul>
<ul>
<li>测试2.2: 根据具体的条件去执行删除规则</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">iptables -vnL INPUT</div><div class="line">iptables -t filter -D INPUT -s 10.20.180.12 -j DROP</div></pre></td></tr></table></figure>
<p>修改规则，一般就是删除旧规则，再添加新规则。</p>
<ul>
<li>保存规则</li>
</ul>
<p>当重启iptables服务或者重启服务器以后，我们平常添加的规则或者对规则所做出的修改都将消失，为了防止这种情况的发生，我们需要将规则”保存”</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">iptables-save &gt; /etc/network/iptables.up.rules</div><div class="line">iptables-apply <span class="comment">#restart iptables</span></div></pre></td></tr></table></figure>
<ul>
<li><p>更多关于匹配条件</p>
<p>-s 选项， 指定源地址作为匹配条件，还可以指定一个网段，或用 逗号分割多个IP;  支持取反。</p>
<p>-d 选项，指定目标地址作为匹配条件。 </p>
</li>
</ul>
<p>不指定，默认就是（0.0.0.0/0），即所有IP</p>
<p> -p 选项，指定匹配的报文协议类型。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">iptables -t filter -I INPUT -s 10.20.180.12 -d 10.20.181.132 -p tcp -j REJECT</div><div class="line">ssh 10.20.180.12 <span class="comment">#from 10.20.181.132 ssh t0 10.20.180.12 suppose to be rejected. but not ?</span></div></pre></td></tr></table></figure>
<p> -i选项，匹配报文通过哪块网卡流入本机。</p>
<h4 id="iptables之网络防火墙"><a href="#iptables之网络防火墙" class="headerlink" title="iptables之网络防火墙"></a>iptables之网络防火墙</h4><p>当外部网络中的主机与网络内部主机通讯时，不管是由外部主机发往内部主机的报文，还是由内部主机发往外部主机的报文，都需要经过iptables所在的主机，由iptables所在的主机进行”过滤并转发”，所以，防火墙主机的主要工作就是”过滤并转发”</p>
<p><img src="http://www.zsythink.net/wp-content/uploads/2017/05/051017_0955_1.png" alt="image"></p>
<p>主机B也属于内部网络，同时主机B也能与外部网络进行通讯，如上图所示，主机B有两块网卡，网卡1与网卡2，网卡1的IP地址为10.1.0.3，网卡2的IP地址为192.168.1.146: </p>
<p><img src="http://www.zsythink.net/wp-content/uploads/2017/05/051017_0955_2.png" alt="image"></p>
<p>c主机网关指向B主机网卡1的IP地址；A主机网关指向B主机网卡2的IP地址。</p>
<p>on hostmachine A:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">route add -net 10.1.0.0/16 gw 192.168.1.146 </div><div class="line">ping 10.1.0.1 <span class="comment"># ping machine C, not available</span></div><div class="line">ping 10.1.0.3 <span class="comment"># ping machine B(NIC 2), avaiailable</span></div></pre></td></tr></table></figure>
<ul>
<li>为什么10.1.0.1没有回应。</li>
</ul>
<p>A主机通过路由表得知，发往10.1.0.0/16网段的报文的网关为B主机，当报文达到B主机时，B主机发现A的目标为10.1.0.1，而自己的IP是10.1.0.3，这时，B主机则需要将这个报文转发给10.1.0.1（也就是C主机），但是，Linux主机在默认情况下，并不会转发报文，如果想要让Linux主机能够转发报文，需要额外的设置，这就是为什么10.1.0.1没有回应的原因，因为B主机压根就没有将A主机的ping请求转发给C主机，C主机压根就没有收到A的ping请求，所以A自然得不到回应</p>
<ul>
<li>为什么10.1.0.3会回应。</li>
</ul>
<p>这是因为10.1.0.3这个IP与192.168.1.146这个IP都属于B主机，当A主机通过路由表将ping报文发送到B主机上时，B主机发现自己既是192.168.1.146又是10.1.0.3，所以，B主机就直接回应了A主机，并没有将报文转发给谁，所以A主机得到了10.1.0.3的回应</p>
<ul>
<li>如何让LINUX主机转发报文</li>
</ul>
<p>check <code>/proc/sys/net/ipv4/ip_forward</code>. if content is <code>0</code>, meaning the Linux hostmachine doesn’t forward; if content is <code>1</code>, meaning the Linux hostmachine does forward. </p>
<p>or </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sysctl -w net.ipv4.ip_forward=1</div></pre></td></tr></table></figure>
<p>for permenentally allow Linux host forward, update file <code>/etc/sysctl.conf</code>. </p>
<p>设置linux主机转发报文后， A ping C || C ping A should works.</p>
<p>如果我们想要使内部的主机能够访问外部主机的web服务，我们应该怎样做呢？ 我们需要在FORWARD链中放行内部主机对外部主机的web请求.</p>
<ul>
<li>在B主机上：</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">iptables -I FORWARD -j REJECT</div><div class="line">iptables -I FORWARD -s 10.1.0.0/16 -p tcp --dport 80 -j ACCEPT</div></pre></td></tr></table></figure>
<p>B主机上所有转发(forward)命令，都REJECT。只ACCEPT 内网10.1.0.0/16 网段， 端口80的转发FORWARD. （即内网网段可访问外网)。 C ping A (ok)</p>
<p>destnation port 目标端口。</p>
<p>想让A ping C, 需在B主机iptables中再添加如下规则：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">iptables -I FORWARD -d 10.1.0.0/16 -p tcp --sport 80 -j ACCEPT</div></pre></td></tr></table></figure>
<p>source port 源端口。 </p>
<p>配置规则时，往往需要考虑“双向性”。因为一条规则(forward)，只会匹配最新的规则定义。上述修改完了，A 可以PING C，但C又PING不通A了。 a better way, no matter request from in to out or the othersize, both should FORWARD.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">iptables -I FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT</div></pre></td></tr></table></figure>
<ul>
<li>更多动作</li>
</ul>
<p>NAT, network address translation. 网络地址转换。NAT说白了就是修改报文的IP地址，NAT功能通常会被集成到路由器、防火墙、或独立的NAT设备中。那为什么要修改报文的IP地址呢？</p>
<p>scenario1 :网络内部有10台主机，它们有各自的IP地址，当网络内部的主机与其他网络中的主机通讯时，则会暴露自己的IP地址，如果我们想要隐藏这些主机的IP地址，该怎么办呢？</p>
<p>当网络内部的主机向网络外部主机发送报文时，报文会经过防火墙或路由器，当报文经过防火墙或路由器时，将报文的源IP修改为防火墙或者路由器的IP地址，当其他网络中的主机收到这些报文时，显示的源IP地址则是路由器或者防火墙的，而不是那10台主机的IP地址，这样，就起到隐藏网络内部主机IP的作用。同时路由器会维护一张NAT表，记录这个内部主机的IP和端口。当外部网络中的主机进行回应时，外部主机将响应报文发送给路由器，路由器根据刚才NAT表中的映射记录，将响应报文中的目标IP与目标端口再改为内部主机的IP与端口号，然后再将响应报文发送给内部网络中的主机。</p>
<p>刚才描述的过程中，”IP地址的转换”一共发生了两次。</p>
<p>内部网络的报文发送出去时，报文的源IP会被修改，也就是源地址转换：Source Network Address Translation，缩写为SNAT。</p>
<p>外部网络的报文响应时，响应报文的目标IP会再次被修改，也就是目标地址转换：Destinationnetwork address translation，缩写为DNAT。</p>
<p>不论内网访问外网，Or the otherwise。都会有上述两次IP转换。一般将内网请求外网服务称为snat，外网请求内网服务称为dnat.</p>
<p>上述场景不仅仅能够隐藏网络内部主机的IP地址，还能够让局域网内的主机共享公网IP，让使用私网IP的主机能够访问互联网。比如，整个公司只有一个公网IP，但是整个公司有10台电脑，我们怎样能让这10台电脑都访问互联网呢？ 只要在路由器上配置公网IP，在私网主机访问公网服务时，报文经过路由器，路由器将报文中的私网IP与端口号进行修改和映射，将其映射为公网IP与端口号，这时，内网主机即可共享公网IP访问互联网上的服务了</p>
<p>场景2：公司有自己的局域网，网络中有两台主机作为服务器，主机1提供web服务，主机2提供数据库服务，但是这两台服务器在局域网中使用私有IP地址，只能被局域网内的主机访问，互联网无法访问到这两台服务器，整个公司只有一个可用的公网IP。如何让公网访问到公司的内网服务呢？</p>
<p>将这个公网IP配置到公司的某台主机或路由器上，然后对外宣称，这个IP地址对外提供web服务与数据库服务，于是互联网主机将请求报文发送给这公网 IP地址，也就是说，此时报文中的目标IP为公网IP，当路由器收到报文后，将报文的目标地址改为对应的私网地址，比如，如果报文的目标IP与端口号为：公网IP+3306，我们就将报文的目标地址与端口改为：主机2的私网IP+3306，同理，公网IP+80端口映射为主机1的私网IP+80端口，当私网中的主机回应对应请求报文时，再将回应报文的源地址从私网IP+端口号映射为公网IP+端口号，再由路由器或公网主机发送给互联网中的主机。</p>
<h6 id="测试环境，同ABC主机。"><a href="#测试环境，同ABC主机。" class="headerlink" title="测试环境，同ABC主机。"></a>测试环境，同ABC主机。</h6><ul>
<li>SNAT 测试</li>
</ul>
<p>on B hostmachine:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">iptables -t nat -A POSTROUTING -s 10.1.0.0/16 -j SNAT --to-source 192.168.1.146</div></pre></td></tr></table></figure>
<p>SNAT规则只能存在于POSTROUTING链与INPUT链中。 “–to-source”就是SNAT动作的常用选项，用于指定SNAT需要将报文的源IP修改为哪个IP地址。此处，即B的公网IP.</p>
<ul>
<li>DNAT测试</li>
</ul>
<p>on B machine:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">iptables -t nat -F  <span class="comment">#flash nat table </span></div><div class="line">iptables -t nat -I PREROUTING -d 192.168.1.146 -p tcp --dport 3389 -j DNAT --to-destination 10.1.0.6:3389</div></pre></td></tr></table></figure>
<p>-j DNAT –to-destination 10.1.0.6:3389”表示将符合条件的报文进行DNAT，也就是目标地址转换，将符合条件的报文的目标地址与目标端口修改为10.1.0.6:3389。 理论上只要完成上述DNAT配置规则即可，但是在测试时，只配置DNAT规则后，并不能正常DNAT，经过测试发现，将相应的SNAT规则同时配置后，即可正常DNAT，于是我们又配置了SNAT:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">iptables -t nat -A POSTROUTING -s 10.1.0.0/16 -j SNAT --to-source 192.168.1.146</div></pre></td></tr></table></figure>
<h4 id="MASQUERADE"><a href="#MASQUERADE" class="headerlink" title="MASQUERADE"></a>MASQUERADE</h4><p>当我们拨号网上时，每次分配的IP地址往往不同，不会长期分给我们一个固定的IP地址，如果这时，我们想要让内网主机共享公网IP上网，就会很麻烦，因为每次IP地址发生变化以后，我们都要重新配置SNAT规则，这样显示不是很人性化，我们通过MASQUERADE即可解决这个问题。</p>
<p>MASQUERADE会动态的将源地址转换为可用的IP地址，其实与SNAT实现的功能完全一致，都是修改源地址，只不过SNAT需要指明将报文的源地址改为哪个IP，而MASQUERADE则不用指定明确的IP，会动态的将报文的源地址修改为指定网卡上可用的IP地址。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">iptables -t nat -I POSTROUTING -s 10.1.0.0/16 -o en1 -j MASQUERADE</div></pre></td></tr></table></figure>
<p>通过B外网网卡出去的报文在经过POSTROUTING链时，会自动将报文的源地址修改为外网网卡上可用的IP地址，这时，即使外网网卡中的公网IP地址发生了改变，也能够正常的、动态的将内部主机的报文的源IP映射为对应的公网IP。</p>
<p>可以把MASQUERADE理解为动态的、自动化的SNAT</p>
<h4 id="REDIRECT"><a href="#REDIRECT" class="headerlink" title="REDIRECT"></a>REDIRECT</h4><p>使用REDIRECT动作可以在本机上进行端口映射</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-ports 8080</div></pre></td></tr></table></figure>
<p>经过上述规则映射后，当别的机器访问本机的80端口时，报文会被重定向到本机的8080端口上。<br>REDIRECT规则只能定义在PREROUTING链或者OUTPUT链中。</p>
<h2 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h2><p><a href="https://www.digitalocean.com/community/tutorials/iptables-essentials-common-firewall-rules-and-commands" target="_blank" rel="external">iptables essentials</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1521589" target="_blank" rel="external">ip route, ip rule &amp; iptables 知多少</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/05/23/k8s-flannel/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="David Z.J. Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Serious Autonomous Vehicles">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/05/23/k8s-flannel/" itemprop="url">k8s: flannel</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-05-23T08:19:53-04:00">
                2020-05-23
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/05/23/k8s-flannel/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2020/05/23/k8s-flannel/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><p>k8s networks include a few topics:</p>
<ul>
<li><p>pod to pod communication in k8s</p>
</li>
<li><p>pod to the host node communication</p>
</li>
<li><p>pod to external service URL</p>
</li>
<li><p>external request to pod in k8s</p>
</li>
</ul>
<p>the networks have two components: DNS and iptables. DNS used to resovle URL name to IP; iptables used to control network message transfer in/out. </p>
<h4 id="preparation-image-for-test"><a href="#preparation-image-for-test" class="headerlink" title="preparation image for test"></a>preparation image for test</h4><p>to test network inside pod/docker, we need add the following network tools: </p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="string">iputils-ping</span> <span class="string">\</span> </div><div class="line"><span class="string">net-tools</span> <span class="string">\</span> </div><div class="line"><span class="string">iptables</span> <span class="string">\</span></div><div class="line"><span class="string">iproute</span></div></pre></td></tr></table></figure>
<h4 id="docker-pod-runtime-privileges"><a href="#docker-pod-runtime-privileges" class="headerlink" title="docker/pod runtime privileges"></a>docker/pod runtime privileges</h4><p>by default, docker doesn’t allow run <code>iptables</code> inside container. and it give errors:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">root@redisjq:/<span class="comment"># iptables -t nat -L | grep INPUT </span></div><div class="line">iptables v1.6.0: can<span class="string">'t initialize iptables table `nat'</span>: Permission denied (you must be root)</div><div class="line">Perhaps iptables or your kernel needs to be upgraded.</div></pre></td></tr></table></figure>
<p>which need to add <a href="https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities" target="_blank" rel="external">docker runtime privilege and Linux capabilities</a></p>
<p><a href="https://www.weave.works/blog/container-capabilities-kubernetes/" target="_blank" rel="external">container capabilities in k8s</a></p>
<p>In a Kubernetes pod, the names are the same, but everything has to be defined in the pod specification. When implementing this in Kubernetes, you add an array of capabilities under the securityContext tag.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">securityContext:</div><div class="line">  capabilities:</div><div class="line">    add:</div><div class="line">       - NET_ADMIN</div></pre></td></tr></table></figure>
<h2 id="k8s-pod-DNS"><a href="#k8s-pod-DNS" class="headerlink" title="k8s pod DNS"></a>k8s pod DNS</h2><p><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/" target="_blank" rel="external">DNS for services and pods</a> introduced four types: </p>
<ul>
<li><p>None </p>
</li>
<li><p>Default， where POD derived DNS config from the host node where to run pod.</p>
</li>
<li><p>ClusterFirst， where POD use DNS info from kube-dns or coreDNS</p>
</li>
<li><p>ClusterFirstWithHostNet, as the name explained.</p>
</li>
</ul>
<p>tips, <strong>Default</strong> is not the default DNS policy. If dnsPolicy is not explicitly specified, then <strong>ClusterFirst</strong> is used as default.</p>
<p>the purpose of pod/service DNS is used to transfer URL to IP, which is the second step after iptabels is understand successfully.</p>
<p><code>coreDNS</code> is setted during <code>kube-adm init</code> with <code>serverDNS</code>. To do SNAT, the pod/service in K8S need access </p>
<h5 id="resolv-conf-DNS-inside-pod"><a href="#resolv-conf-DNS-inside-pod" class="headerlink" title="resolv.conf/DNS inside pod :"></a>resolv.conf/DNS inside pod :</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">root@redisjq:/redis<span class="comment"># cat /etc/resolv.conf </span></div><div class="line">nameserver 10.96.0.10</div><div class="line">search lg.svc.cluster.local svc.cluster.local cluster.local</div><div class="line">options ndots:5</div></pre></td></tr></table></figure>
<p>clearly, the pod DNS is coming from cluster, which can be check by: </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">kubectl describe configmap kubeadm-config -n kube-system </div><div class="line"></div><div class="line">kind: ClusterConfiguration</div><div class="line">kubernetesVersion: v1.18.2</div><div class="line">networking:</div><div class="line">  dnsDomain: cluster.local</div><div class="line">  podSubnet: 10.4.0.0/16</div><div class="line">  serviceSubnet: 10.96.0.0/12</div></pre></td></tr></table></figure>
<p>and that’s the reason why resolve URL failed, as clusterDNS is kind of random defined. </p>
<p>the DNS failure gives errors as following inside pod: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">socket.gaierror: [Errno -3] Temporary failure in name resolution</div></pre></td></tr></table></figure>
<h4 id="docker0-DNS"><a href="#docker0-DNS" class="headerlink" title="docker0 DNS"></a>docker0 DNS</h4><p>docker engine can configure its DNS at <code>/etc/daemon.json</code>, with “dns” section. when running docker in single mode, docker0 looks has host machine’s DNS, but when runnig in swarm mode, need to define additional DNS for external access.</p>
<h2 id="iptables-in-K8S"><a href="#iptables-in-K8S" class="headerlink" title="iptables in K8S"></a>iptables in K8S</h2><p><a href="https://docs.docker.com/network/iptables/" target="_blank" rel="external">docker and iptables</a></p>
<p>you should not modify the rules Docker inserts into your iptables policies. Docker installs two custom iptables chains named <code>DOCKER-USER</code> and <code>DOCKER</code>, and it ensures that incoming packets are always checked by these two chains first</p>
<p>a simple test can found, <code>docker0</code> NIC is bridge is well to bind host network namespace, either export or response request externally. while with <code>flannel.d</code> NIC, pod can’t access external resources.</p>
<p><a href="https://www.bookstack.cn/read/source-code-reading-notes/kubernetes-kube_proxy_iptables.md" target="_blank" rel="external">code review: kube-proxy iptables</a>: </p>
<p>iptables has 5 tables and 5 chains.</p>
<p>the 5 chaines: </p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="attr">PREROUTING:</span> <span class="string">before</span> <span class="string">message</span> <span class="string">into</span> <span class="string">route,</span> <span class="string">the</span> <span class="string">external</span> <span class="string">request</span> <span class="string">DNAT.</span></div><div class="line"><span class="attr">INPUT:</span> <span class="string">message</span> <span class="string">to</span> <span class="string">local</span> <span class="string">host</span> <span class="string">or</span> <span class="string">current</span> <span class="string">network</span> <span class="string">namespace.</span> </div><div class="line"><span class="attr">FORWARD:</span> <span class="string">message</span> <span class="string">forward</span> <span class="string">to</span> <span class="string">other</span> <span class="string">host</span> <span class="string">or</span> <span class="string">other</span> <span class="string">network</span> <span class="string">namespace.</span></div><div class="line"><span class="attr">OUTPUT:</span> <span class="string">message</span> <span class="string">export</span> <span class="string">from</span> <span class="string">current</span> <span class="string">host</span></div><div class="line"><span class="attr">POSTROUTING:</span> <span class="string">after</span> <span class="string">route</span> <span class="string">before</span> <span class="string">NIC,</span> <span class="string">message</span> <span class="string">SNAT</span></div></pre></td></tr></table></figure>
<p>the 5 tables: </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">filter table, used to control the network package need to ACCEPT, or DROP, or REJECT when it comes to a chain</div><div class="line">nat(network address translation) table, used to modify the src/target address of network package</div><div class="line">mangle table, used to modify IP header info of network package</div><div class="line">raw table</div><div class="line">security table</div></pre></td></tr></table></figure>
<p>for k8s pods/services, mostly consider <code>filter</code> and <code>nat</code> tables. and k8s expand another 7 chains: KUBE-SERVICES、KUBE-EXTERNAL-SERVICES、KUBE-NODEPORTS、KUBE-POSTROUTING、KUBE-MARK-MASQ、KUBE-MARK-DROP、KUBE-FORWARD. </p>
<p><img src="https://static.bookstack.cn/projects/source-code-reading-notes/729e704bd9fc39c9223da5185e9ef084.png" alt="image"></p>
<h2 id="virtual-network-flannel"><a href="#virtual-network-flannel" class="headerlink" title="virtual network flannel"></a>virtual network flannel</h2><h4 id="check-running-flanneld"><a href="#check-running-flanneld" class="headerlink" title="check running flanneld"></a>check running flanneld</h4><ul>
<li><p><code>/etc/cni/net.d/10-flannel.conflist</code> on host machine is the same as <code>/etc/kube-flannel/cni-conf.json</code> in flannel container on master node.</p>
</li>
<li><p><code>/run/flannel/subnet.env</code> exist in both flannel container (on master node) and in master host machine. it looks like network configure(subnet.env) is copied from container to host machine. so if there is no <code>flannel container</code> running on some nodes, these nodes won’t have the right network configure. </p>
</li>
</ul>
<p>at master node, <code>HostPath</code> points to: /run/flannel, /etc/cni/net.d, kube-flannel-cfg (ConfigMap); while at working node(due to missing /gcr.io/flannel image), <code>/run/flannel/subnet.env</code> is missed. previously, I thought to cp this file from master node to woker node is the solution, then this file is missed every time to restart worker node. </p>
<p>once copied both <code>kube-proxy</code> and <code>flannel</code> images to worker node, and restart <code>kubelet</code> at worker node, the cluster should give <code>Running status</code> of all these components. including 2 copies of <code>flannel</code>, one running on master node, and the other running on working node. </p>
<p>as we are using <code>kubectl</code> to start the cluster, the actual flanneld is <code>/opt/bin/flanneld</code> from the running flannel container, and it maps NIC to the host machine. </p>
<p>another thing is, <code>flannel</code> is the core of the default <code>kube-proxy</code>, so <code>kube-proxy</code> image is also required on both nodes. <code>coreDNS</code> run two copies on master node.</p>
<h4 id="Flannel-mechanism"><a href="#Flannel-mechanism" class="headerlink" title="Flannel mechanism"></a><a href="https://www.jianshu.com/p/165a256fb1da" target="_blank" rel="external">Flannel mechanism</a></h4><p>the data flow: <strong>the sending message</strong> go to VNC(virtual network card) <code>docker0</code> on host machine, which transfers to VNC <code>flannel0</code>. this process is P2P. the global <code>etcd</code> service maintain a iptables among nodes, which store the subnet range of each node. 2) the <code>flanneld</code> service on the special node package <strong>the sending message</strong> as UDP package, and delivery to target node, based on the iptables. 3) when the target node received the UDP package, it unpackage the message, and send to its <code>flannel0</code>, then transfer to its <code>docker0</code>. </p>
<p>1) after flanneld started，will create <code>flannel.1</code> virtual network card. the purpose of <code>flannel.1</code> is for across-host network, including package/unpackage UDP, and maintain iptables among the nodes. </p>
<p>2) each node also create <code>cni0</code> virtual network card, at the first time to run flannel CNI. the purpose of <code>cni0</code> is same as <code>docker0</code>, and it’s a bridge network, used for communication in the same node. </p>
<p><img src="https://www.centos.bz/wp-content/uploads/2017/06/flannel-01.png" alt="image"></p>
<h2 id="test-with-redis-services"><a href="#test-with-redis-services" class="headerlink" title="test with redis services"></a>test with redis services</h2><p>we had define a <code>redisjq</code> pod, the following testes are all in this pod:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">kubectl <span class="built_in">exec</span> -it redisjq -n lg  /bin/bash</div><div class="line">ping localhost <span class="comment">#ok</span></div><div class="line">ping 10.255.18.3 <span class="comment">#not </span></div><div class="line">ping 10.3.101.101 <span class="comment">#not</span></div><div class="line">ping 10.20.180.12 </div><div class="line"></div><div class="line">ifconfig </div><div class="line">&gt;&gt;eth0,  10.4.1.46</div><div class="line">&gt;&gt;lo, 127.0.0.1</div></pre></td></tr></table></figure>
<p>the output above is initial output before we had any network setttings. basically the pod can only ping localhost, neither the host DNS, or the host IP. the vip(10.4.1.46) is not in the same network namespace as host network space. </p>
<h4 id="flannel-d-pod-on-both-nodes"><a href="#flannel-d-pod-on-both-nodes" class="headerlink" title="flannel.d pod on both  nodes:"></a>flannel.d pod on both  nodes:</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">david@meng:~/k8s/lgsvl$ kubectl get pods kube-flannel-ds-amd64-85d6m  -n kube-system --output=wide</div><div class="line">NAME                          READY   STATUS    RESTARTS   AGE   IP             NODE   NOMINATED NODE   READINESS GATES</div><div class="line">kube-flannel-ds-amd64-85d6m   1/1     Running   5          15d   10.20.180.12   meng   &lt;none&gt;           &lt;none&gt;</div><div class="line"></div><div class="line">david@meng:~/k8s/lgsvl$ kubectl get pods kube-flannel-ds-amd64-fflsl  -n kube-system --output=wide</div><div class="line">NAME                          READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES</div><div class="line">kube-flannel-ds-amd64-fflsl   1/1     Running   154        15d   10.20.181.132   ubuntu   &lt;none&gt;           &lt;none&gt;</div></pre></td></tr></table></figure>
<p><code>flannel.d</code> is runing on each node, should triggered by <code>kubelet</code>. <code>flannel.d</code> is used as virtual network interface, to manage across-node pod communication inside k8s. </p>
<h4 id="coredns-pod-in-meng-node"><a href="#coredns-pod-in-meng-node" class="headerlink" title="coredns pod in meng node"></a>coredns pod in meng node</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">david@meng:~/k8s/lgsvl$ kubectl get pods coredns-66bff467f8-59g97  -n kube-system --output=wide</div><div class="line">NAME                       READY   STATUS    RESTARTS   AGE   IP          NODE   NOMINATED NODE   READINESS GATES</div><div class="line">coredns-66bff467f8-59g97   1/1     Running   4          14d   10.4.0.27   meng   &lt;none&gt;           &lt;none&gt;</div></pre></td></tr></table></figure>
<p><code>coredns</code> has two replicas, both running on master node(meng), and we can see it only has virtual ip/cluster ip ( 10.4.0.x).</p>
<h4 id="redisjs-pod-in-ubuntu’s-node"><a href="#redisjs-pod-in-ubuntu’s-node" class="headerlink" title="redisjs pod in ubuntu’s node"></a>redisjs pod in ubuntu’s node</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">david@meng:~/k8s/lgsvl$ kubectl get pods redisjq -n lg --output=wide </div><div class="line">NAME      READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES</div><div class="line">redisjq   1/1     Running   0          20m   10.4.1.47   ubuntu   &lt;none&gt;           &lt;none&gt;</div></pre></td></tr></table></figure>
<p>by default, working pod is only running at woker node(ubuntu), which has clusterIP(10.4.1.47).</p>
<h4 id="pod1-ping-pod2-in-the-same-node"><a href="#pod1-ping-pod2-in-the-same-node" class="headerlink" title="pod1 ping pod2 in the same node"></a>pod1 ping pod2 in the same node</h4><p><strong>ping successfully</strong> there is no doubt in the same node, pods can ping each other.</p>
<h4 id="redisjq-pod1-10-4-1-47-in-ubuntu-ping-corends-pod2-10-4-0-27-in-meng"><a href="#redisjq-pod1-10-4-1-47-in-ubuntu-ping-corends-pod2-10-4-0-27-in-meng" class="headerlink" title="redisjq pod1(10.4.1.47) in ubuntu ping corends pod2(10.4.0.27) in meng"></a>redisjq pod1(10.4.1.47) in ubuntu ping corends pod2(10.4.0.27) in meng</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">root@redisjq:/redis<span class="comment"># ping 10.4.0.27 </span></div><div class="line">PING 10.4.0.27 (10.4.0.27) 56(84) bytes of data.</div><div class="line">64 bytes from 10.4.0.27: icmp_seq=1 ttl=62 time=0.757 ms</div></pre></td></tr></table></figure>
<p><strong>ping successfully</strong>, which is the working of flanneld. </p>
<h4 id="redisjq-pod1-10-4-1-7-in-ubuntu-ping-hostIP-10-20-181-132-of-ubuntu"><a href="#redisjq-pod1-10-4-1-7-in-ubuntu-ping-hostIP-10-20-181-132-of-ubuntu" class="headerlink" title="redisjq pod1(10.4.1.7) in ubuntu  ping hostIP(10.20.181.132) of ubuntu"></a>redisjq pod1(10.4.1.7) in ubuntu  ping hostIP(10.20.181.132) of ubuntu</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">root@redisjq:/redis<span class="comment"># ping 10.20.181.132</span></div><div class="line">PING 10.20.181.132 (10.20.181.132) 56(84) bytes of data.</div><div class="line">64 bytes from 10.20.181.132: icmp_seq=1 ttl=64 time=0.127 ms</div></pre></td></tr></table></figure>
<p><strong>ping successfuly</strong>, pod in cluster can ping its host node, sounds no problem. </p>
<h4 id="redisjq-pod1-10-4-1-7-in-ubuntu-ping-hostIP-10-20-180-12-of-meng"><a href="#redisjq-pod1-10-4-1-7-in-ubuntu-ping-hostIP-10-20-180-12-of-meng" class="headerlink" title="redisjq pod1(10.4.1.7) in ubuntu ping hostIP(10.20.180.12) of meng"></a>redisjq pod1(10.4.1.7) in ubuntu ping hostIP(10.20.180.12) of meng</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">root@redisjq:/redis<span class="comment"># ping 10.20.180.12 </span></div><div class="line">PING 10.20.180.12 (10.20.180.12) 56(84) bytes of data.</div></pre></td></tr></table></figure>
<p><strong>ping failed</strong>, interesting, so pod in cluster can’t ping any non-hosting node’s IP.</p>
<p>so far, pod with vip can ping any other pod with vip in the cluster, no matter in the same node or not.  pod with vip can only ping its host machine’s physical IP, but pod can’t ping other hostIP.</p>
<p>namely, the network of pod VIP inside k8s and the bridge network from pod vip to its host is set well. but the network from pod to external IP is not well.</p>
<p>these 4 tests give a very good understanding about flannel’s function inside k8s: pod to pod in the same node or not. but usually we need SNAT or DNAT. to make SNAT/DNAT avaiable, we need understand <strong>DNS &amp; iptables</strong> of k8s. </p>
<h2 id="update-iptables-to-allow-pod-access-public-IP"><a href="#update-iptables-to-allow-pod-access-public-IP" class="headerlink" title="update iptables to allow pod access public IP"></a>update iptables to allow pod access public IP</h2><h4 id="cni0-docker0-eno1-flannel-1-in-host-machine-vs-eth0-in-pod"><a href="#cni0-docker0-eno1-flannel-1-in-host-machine-vs-eth0-in-pod" class="headerlink" title="cni0, docker0, eno1, flannel.1 in host machine vs eth0 in pod"></a>cni0, docker0, eno1, flannel.1 in host machine vs eth0 in pod</h4><p>these virtual NIC are common in k8s env. </p>
<ul>
<li>on node1 </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cni0: 10.4.1.1</div><div class="line">docker0: 172.17.0.1</div><div class="line">eno1: 10.20.181.132</div><div class="line">flannel.1: 10.4.1.0</div></pre></td></tr></table></figure>
<ul>
<li>on pod1, which is running on node1</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">eth0:  10.4.1.48</div></pre></td></tr></table></figure>
<p><a href="https://www.centos.bz/2017/06/k8s-flannel-network/" target="_blank" rel="external">pod1 -&gt; pod2 network message flow</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pod1(10.4.1.48) on node1(10.20.181.132) -&gt; cni0(10.4.1.1) -&gt; flannel.1(10.4.1.0) -&gt; kube-flannel on node1(10.20.181.132) -&gt; kube-flannel on node2(10.20.180.12) -&gt; flannel.1 on node2 -&gt; cni0 on node2 -&gt; pod2(10.4.1.46) on node2</div></pre></td></tr></table></figure>
<p>to allow network message SNAT, namely to handle <strong>FORWARD</strong> internal clusterIP data to external services, we can add the following newe iptable rule:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">iptables -t nat -I POSTROUTING -s 10.4.1.0/24 -j MASQUERADE</div></pre></td></tr></table></figure>
<p>after add the new rule, check inside pod:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">root@redisjq:/redis<span class="comment"># ping 10.20.180.12 </span></div><div class="line">PING 10.20.180.12 (10.20.180.12) 56(84) bytes of data.</div><div class="line">64 bytes from 10.20.180.12: icmp_seq=1 ttl=62 time=0.690 ms</div><div class="line">root@redisjq:/redis<span class="comment"># ping 10.20.181.132</span></div><div class="line">PING 10.20.181.132 (10.20.181.132) 56(84) bytes of data.</div><div class="line">64 bytes from 10.20.181.132: icmp_seq=1 ttl=64 time=0.108 ms</div><div class="line">root@redisjq:/redis<span class="comment"># ping 10.20.180.61 </span></div><div class="line">PING 10.20.180.61 (10.20.180.61) 56(84) bytes of data.</div><div class="line">64 bytes from 10.20.180.61: icmp_seq=1 ttl=126 time=0.366 ms</div><div class="line">root@redisjq:/redis<span class="comment"># ping www.baidu.com</span></div><div class="line">ping: unknown host www.baidu.com</div><div class="line">root@redisjq:/redis<span class="comment"># ping 61.135.169.121   #baidu IP</span></div><div class="line">PING 61.135.169.121 (61.135.169.121) 56(84) bytes of data.</div><div class="line">64 bytes from 61.135.169.121: icmp_seq=1 ttl=51 time=8.16 ms</div></pre></td></tr></table></figure>
<p>the DNS is not fixed, so we can’t ping <code>www.baidu.com</code>, but we can ping its IP.</p>
<p>on the other hand, to hanle <strong>FORWARD</strong> external request to internal clusterIP, we can add the following new iptable rule:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">iptables -t nat -I PREROUTING -d 10.4.1.0/24 -j MASQUERADE</div></pre></td></tr></table></figure>
<p>that’s beauty of iptables. </p>
<p>as mentioned previously, to handle pod DNS error, we need add <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/" target="_blank" rel="external">pod/service DNS strategy</a> inside the <code>pod.yaml</code>:</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">spec:</span></div><div class="line"><span class="attr">  dnsPolicy:</span> <span class="string">Default</span></div></pre></td></tr></table></figure>
<p>our k8s cluster has none DNS server itself, so to do SNAT/DNAT, we have to keep <code>Default</code> dns strategy, which make pod/service to use its host machine’s DNS, which is defined at <code>/etc/resovl.conf</code>.</p>
<p>one thing to take care, some host machine has only <code>nameserver 127.0.0.1</code> in <code>resolv.conf</code>, then we need add the real DNS server.</p>
<h2 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h2><p>with knowledge about iptables and dns, we can make an useful K8S cluster. the left work is make useful pods.</p>
<h2 id="refere"><a href="#refere" class="headerlink" title="refere"></a>refere</h2><p><a href="https://jimmysong.io/blog/configuring-kubernetes-kube-dns/" target="_blank" rel="external">jimmy song: config K8S DNS: kube-dns</a></p>
<p><a href="https://askubuntu.com/questions/346838/how-do-i-configure-my-dns-settings-in-ubuntu-server" target="_blank" rel="external">configure DNS settings in Ubuntu</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/90992878" target="_blank" rel="external">zhihu: k8s network</a></p>
<p><a href="https://www.kubernetes.org.cn/4317.html" target="_blank" rel="external">k8s expose service</a></p>
<p><a href="https://www.kubernetes.org.cn/6838.html" target="_blank" rel="external">k8s network advance</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1140088" target="_blank" rel="external">docker iptables from tencent cloud</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/05/17/k8s-setup-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="David Z.J. Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Serious Autonomous Vehicles">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/05/17/k8s-setup-2/" itemprop="url">k8s setup 2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-05-17T08:50:57-04:00">
                2020-05-17
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/05/17/k8s-setup-2/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2020/05/17/k8s-setup-2/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><p>this blog try to deploy two service in k8s: redis and dashboard. the other is engineering. </p>
<h2 id="manual-deploy-via-kubectl"><a href="#manual-deploy-via-kubectl" class="headerlink" title="manual deploy via kubectl"></a>manual deploy via kubectl</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">kubectl create ns <span class="built_in">test</span>-busybox</div><div class="line">kubectl run busybox --namespace=<span class="built_in">test</span>-busybox \</div><div class="line">                      --port=8280 \</div><div class="line">                      --image=busybox \</div><div class="line">                      -- sh -c <span class="string">"echo 'Hello' &gt; /var/www/index.html &amp;&amp; httpd -f -p 8280 -h /var/www/"</span></div><div class="line"></div><div class="line">kubectl get pods -n <span class="built_in">test</span>-busybox  <span class="comment">#should display `Running`, but `Pending`</span></div></pre></td></tr></table></figure>
<ul>
<li>error1: pending pod</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl describe pods/busybox -n <span class="built_in">test</span>-busybox</div></pre></td></tr></table></figure>
<p>gives:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  0/2 nodes are available: 1 node(s) had taint &#123;node-role.kubernetes.io/master: &#125;, that the pod didn<span class="string">'t tolerate, 1 node(s) had taint &#123;node.kubernetes.io/unreachable: &#125;, that the pod didn'</span>t tolerate.</div></pre></td></tr></table></figure>
<p>a few things to check:</p>
<ul>
<li><p><code>swapoff -a</code> to close firewall on working node </p>
</li>
<li><p><code>kubectl uncordon</code> to make node schedulable <a href="https://github.com/Azure/AKS/issues/856" target="_blank" rel="external">kubectl uncordon</a></p>
</li>
</ul>
<ul>
<li>error 2: failed create pod sandbox</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Warning  FailedCreatePodSandBox  25s (x4 over 2m2s)  kubelet, ubuntu    Failed to create pod sandbox: rpc error: code = Unknown desc = failed pulling image <span class="string">"k8s.gcr.io/pause:3.2"</span>: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled <span class="keyword">while</span> waiting <span class="keyword">for</span> connection (Client.Timeout exceeded <span class="keyword">while</span> awaiting headers)</div></pre></td></tr></table></figure>
<p>solution is to copy <code>k8s.grc.io/pause:3.2</code> image to <code>ubuntu node</code>, and restart kubelet on working node.</p>
<ul>
<li>error 3: no network plugin CNI</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">networkPlugin cni failed to <span class="built_in">set</span> up pod <span class="string">"busybox_test-busybox"</span> network: open /run/flannel/subnet.env: no such file or directory</div></pre></td></tr></table></figure>
<p><a href="https://github.com/kubernetes/kubernetes/issues/36575" target="_blank" rel="external">a temp solution</a> is to cp <code>/run/flannel/subnet.env</code> from master node to worker node, then restart kubelet at the worker node. as further study, the <code>cp subnet.env</code> to worker node is not the right solution, as every time the worker node shutdown, this <code>subnet.env</code> file will delete, and won’t restart when reboot the worker node the next day.</p>
<p>so the final solution here is to pull <code>quay.io/coreos/flannel</code> image to worker node, as well as <code>k8s.gcr.io/kube-proxy</code>. in later k8s version, <code>kube-proxy</code> is like a proxy, what’s really inside is the flannel daemon. so we need both <code>kube-proxy</code> and <code>flannel</code> at worker node, to guarantee the network working.</p>
<p>we can see the <code>busybox</code> service is running well: </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">kubectl expose pod busybox --<span class="built_in">type</span>=NodePort   --namespace=<span class="built_in">test</span>-busybox</div><div class="line">kubectl get pods --output=wide -n <span class="built_in">test</span>-busybox</div><div class="line">NAME      READY   STATUS    RESTARTS   AGE     IP         NODE     NOMINATED NODE   READINESS GATES</div><div class="line">busybox   1/1     Running   0          7m57s   10.4.0.3   ubuntu   &lt;none&gt;           &lt;none&gt;</div><div class="line">kubectl get service busybox -n <span class="built_in">test</span>-busybox</div><div class="line">NAME      TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</div><div class="line">busybox   NodePort   10.107.117.219   &lt;none&gt;        8280:32431/TCP   33s</div></pre></td></tr></table></figure>
<p>but the problem here is, we can’t access this service from host machine. </p>
<h4 id="exposing-an-external-IP-to-access-an-app-in-cluster"><a href="#exposing-an-external-IP-to-access-an-app-in-cluster" class="headerlink" title="exposing an external IP to access an app in cluster"></a><a href="https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/" target="_blank" rel="external">exposing an external IP to access an app in cluster</a></h4><p>to expose service externally, define the service as either<code>LoadBalancer</code> or <code>NodePort</code> type. but <code>LoaderBalancer</code> <a href="https://github.com/kubernetes/kubernetes/issues/23562" target="_blank" rel="external">requires external third-party: 23562</a> implement of load balancer, e.g. AWS.<br><a href="https://stackoverflow.com/questions/44110876/kubernetes-service-external-ip-pending" target="_blank" rel="external">why loadBalancer service doesn’t work</a>: if you are using a custom Kubernetes Cluster (using minikube, kubeadm or the like). In this case, there is no LoadBalancer integrated (unlike AWS or Google Cloud). With this default setup, you can only use NodePort or an Ingress Controller.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">kubectl apply -f /home/gwm/k8s/busybox.yaml</div><div class="line">kubectl get deployments hello-world  	<span class="comment">#display info of Deployment</span></div><div class="line">kubectl describe deployments hello-world</div><div class="line">kubectl get replicasets		<span class="comment">#display info of ReplicaSet</span></div><div class="line">kubectl describe replicasets</div><div class="line">kubectl expose deployment hello-world --<span class="built_in">type</span>=NodePort --name=my-service  <span class="comment"># create a service object that exposes the deployment </span></div><div class="line">kubectl get services my-service </div><div class="line">kubectl describe services my-service</div><div class="line"><span class="comment">#cleanup when test done</span></div><div class="line">kubectl delete services my-service</div><div class="line">kubectl delete deployment hello-world</div></pre></td></tr></table></figure>
<p>looks the <code>NodePort</code> service doesn’t work as expected: </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">curl http://10.20.180.12:8280 </div><div class="line">curl: (7) Failed to connect to 10.20.180.12 port 8280: Connection refused</div></pre></td></tr></table></figure>
<p>if pods can’t be cleaned by <code>kubectl delete pods xx</code>, try <code>kubectl delete pod &lt;PODNAME&gt; --grace-period=0 --force --namespace &lt;NAMESPACE&gt;</code>. </p>
<p><strong>how to access k8s service outside the cluster</strong></p>
<h2 id="kubectl-config"><a href="#kubectl-config" class="headerlink" title="kubectl config"></a>kubectl config</h2><h4 id="reconfigure-a-node’s-kubelet-in-a-live-cluster"><a href="#reconfigure-a-node’s-kubelet-in-a-live-cluster" class="headerlink" title="reconfigure a node’s kubelet in a live cluster"></a><a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/#automatic-rbac-rules-for-node-authorizer" target="_blank" rel="external">reconfigure a node’s kubelet in a live cluster</a></h4><p>Basic workflow overview</p>
<p>The basic workflow for configuring a kubelet in a live cluster is as follows:</p>
<pre><code>Write a YAML or JSON configuration file containing the kubelet’s configuration.
Wrap this file in a ConfigMap and save it to the Kubernetes control plane.
Update the kubelet’s corresponding Node object to use this ConfigMap.
</code></pre><ul>
<li>dump configure file of each node </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">NODE_NAME=<span class="string">"the-name-of-the-node-you-are-reconfiguring"</span>; curl -sSL <span class="string">"http://localhost:8001/api/v1/nodes/<span class="variable">$&#123;NODE_NAME&#125;</span>/proxy/configz"</span> | jq <span class="string">'.kubeletconfig|.kind="KubeletConfiguration"|.apiVersion="kubelet.config.k8s.io/v1beta1"'</span> &gt; kubelet_configz_<span class="variable">$&#123;NODE_NAME&#125;</span></div></pre></td></tr></table></figure>
<p>our cluster have <code>ubuntu</code> and <code>meng</code>(as leader) two nodes. with these two config files, we found two existing issues: </p>
<p>1) network config on two nodes doesn’ match each other</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;   <span class="string">"clusterDomain"</span>: <span class="string">"xyz.abc"</span>,</div><div class="line">---</div><div class="line">&gt;   <span class="string">"clusterDomain"</span>: <span class="string">"cluster.local"</span>,</div><div class="line">&lt;     <span class="string">"10.3.0.10"</span></div><div class="line">---</div><div class="line">&gt;     <span class="string">"10.96.0.10"</span></div></pre></td></tr></table></figure>
<p>after generrating the NODE config files above, we can edit these files, and then push the edited config file to the control plane:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">NODE_NAME=meng; kubectl -n kube-system create configmap meng-node-config --from-file=kubelet=kubelet_configz_<span class="variable">$&#123;NODE_NAME&#125;</span> --append-hash -o yaml</div><div class="line">NODE_NAME=ubuntu; kubectl -n kube-system create configmap ubuntu-node-config --from-file=kubelet=kubelet_configz_<span class="variable">$&#123;NODE_NAME&#125;</span> --append-hash -o yaml</div></pre></td></tr></table></figure>
<p>after this setting up, we can check the new generated configmaps:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl get configmaps -n kube-system</div><div class="line">kubectl edit configmap meng-node-config-t442m526c5 -n kube-system</div></pre></td></tr></table></figure>
<p>tips: configMaps is also an Object in k8s, just like namespace, pods, svc. but which is only in /tmp, need manually dump. </p>
<p>namely:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">meng-node-config-t442m526c5          1      35m</div><div class="line">ubuntu-node-config-ghkg27446c        1      18s</div></pre></td></tr></table></figure>
<ul>
<li>set node to use new configMap, by <code>kubectl edit node ${NODE_NAME}</code>, and add the following YAML under <code>spec</code>:</li>
</ul>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="attr">configSource:</span></div><div class="line"><span class="attr">    configMap:</span></div><div class="line"><span class="attr">        name:</span> <span class="string">CONFIG_MAP_NAME</span> <span class="comment"># replace CONFIG_MAP_NAME with the name of the ConfigMap</span></div><div class="line"><span class="attr">        namespace:</span> <span class="string">kube-system</span></div><div class="line"><span class="attr">        kubeletConfigKey:</span> <span class="string">kubelet</span></div></pre></td></tr></table></figure>
<ul>
<li>observe the node begin with the new configuration</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl get node <span class="variable">$&#123;NODE_NAME&#125;</span> -o yaml</div></pre></td></tr></table></figure>
<p>2) kubectl command doesn’t work at worker node</p>
<p>basically, worker node always report <code>error: Missing or incomplete configuration info.  Please point to an existing, complete config file</code> when running <code>kubectl</code> command. </p>
<p>which needs to copy <code>/etc/kubernetes/admin.conf</code> from master to worker, then append <code>cat &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; /etc/profile</code> at worker node.</p>
<p><a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/" target="_blank" rel="external">organizing cluster accesss using kubecnfig files</a></p>
<h4 id="docker0-iptables-transfer"><a href="#docker0-iptables-transfer" class="headerlink" title="docker0 iptables transfer"></a>docker0 iptables transfer</h4><p>when starting docker engine, <code>docker0</code> VNC is created, and this vnc add its routing rules to the host’s iptables. From docker 1.13.1, the routing rules of <code>docker0</code> vnc is only transfer to localhost of the host machine, namely <code>docker0</code> to any other non-localhost is forbidden, which leads to the service can only be access on the host machine, where this pod/container is running. in multi-nodes k8s, we need enable iptable FORWARD. </p>
<p>append the following line to <code>ExecStart</code> line in file <code>/lib/systemd/system/docker.service</code>: </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPT</div></pre></td></tr></table></figure>
<p>then restart docker engine: </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">systemctl daemon-reload</div><div class="line">systemctl restart docker.service</div></pre></td></tr></table></figure>
<p>after enable docker0 iptable rules, the following test service can be accessed on both nodes. </p>
<h2 id="deploy-redis-service"><a href="#deploy-redis-service" class="headerlink" title="deploy redis service"></a>deploy redis service</h2><h4 id="create-a-k8s-redis-image"><a href="#create-a-k8s-redis-image" class="headerlink" title="create a k8s-redis image"></a>create a k8s-redis image</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># use existing docker image as a base</div><div class="line">FROM ubuntu:16.04</div><div class="line"></div><div class="line"># Download and install dependency</div><div class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends redis-server</div><div class="line"></div><div class="line"># EXPOSE the port to the Host OS</div><div class="line">EXPOSE 6379</div><div class="line"></div><div class="line"># Tell the image what command it has to execute as it starts as a container</div><div class="line">CMD ["redis-server"]</div></pre></td></tr></table></figure>
<p>build the image and push to both nodes. </p>
<h4 id="deploy-a-redis-deployment"><a href="#deploy-a-redis-deployment" class="headerlink" title="deploy a redis-deployment"></a>deploy a redis-deployment</h4><ul>
<li>create <code>redis-deployment.yaml</code>: </li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">apiVersion: apps/v1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  labels:</div><div class="line">    app.kubernetes.io/name: load-balancer-example</div><div class="line">  name: kredis-deployment</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app.kubernetes.io/name: load-balancer-example</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app.kubernetes.io/name: load-balancer-example</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - image: 10.20.181.119:5000/k8s_redis</div><div class="line">        name: kredis</div><div class="line">        ports:</div><div class="line">        - containerPort: 6379</div></pre></td></tr></table></figure>
<ul>
<li>expose deployment as service</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">kubectl create ns <span class="built_in">test</span>-busybox</div><div class="line">kubectl apply -f redis-deployment.yaml</div><div class="line">kubectl get deployments redis-deployment 	<span class="comment">#display info of Deployment</span></div><div class="line">kubectl describe deployments redis-deployment</div><div class="line">kubectl get replicasets		<span class="comment">#display info of ReplicaSet</span></div><div class="line">kubectl describe replicasets</div><div class="line">kubectl expose deployment redis-deployment --<span class="built_in">type</span>=NodePort --name=my-redis  <span class="comment"># create a service object that exposes the deployment </span></div><div class="line">kubectl get services my-redis </div><div class="line">kubectl describe services my-redis </div><div class="line">kubectl get pods --output=wide</div><div class="line"><span class="comment">#clean up later (afer step 3)</span></div><div class="line">kubectl delete services my-redis</div><div class="line">kubectl delete deployment redis-deployment</div></pre></td></tr></table></figure>
<h4 id="access-as-pod"><a href="#access-as-pod" class="headerlink" title="access as pod"></a>access as pod</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">gwm@meng:~/k8s/alpine$ kubectl get pods --output=wide </div><div class="line">NAME                                 READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES</div><div class="line">kredis-deployment-7567b7f4b7-wmqgd   1/1     Running   0          16h   10.4.1.18   ubuntu   &lt;none&gt;           &lt;none&gt;</div><div class="line">gwm@meng:~/k8s/alpine$ redis-cli -p 6379</div><div class="line">Could not connect to Redis at 127.0.0.1:6379: Connection refused</div><div class="line">not connected&gt; </div><div class="line">gwm@ubuntu:~$ redis-cli -p 6379</div><div class="line">Could not connect to Redis at 127.0.0.1:6379: Connection refused</div><div class="line">not connected&gt;</div></pre></td></tr></table></figure>
<p>as we can see here, as <code>redis-server</code> as pod, won’t expose any port. and pod-IP(10.4.1.18) is only accessible inside cluster</p>
<h4 id="access-as-service"><a href="#access-as-service" class="headerlink" title="access as service"></a>access as service</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">kubectl get services --output=wide </div><div class="line">NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE   SELECTOR</div><div class="line">kredis-deploy   NodePort    10.104.43.224   &lt;none&gt;        6379:31962/TCP   23h   app.kubernetes.io/name=load-balancer-example</div><div class="line">kubernetes      ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          8d    &lt;none&gt;</div><div class="line">root@ubuntu:~<span class="comment"># docker container inspect 60bfd6c5ccac  | grep 31962 </span></div><div class="line">root@ubuntu:~<span class="comment"># redis-cli -p 31962 </span></div><div class="line">127.0.0.1:31962&gt; </div><div class="line">root@ubuntu:~<span class="comment"># redis-cli -p 31962 -h 10.20.181.132 </span></div><div class="line">10.20.181.132:31962&gt; </div><div class="line">gwm@meng:~$ redis-cli -h 10.20.181.132 -p 31962 </div><div class="line">10.20.181.132:31962&gt;</div></pre></td></tr></table></figure>
<p>so basically, we can access <code>redis</code> as service with the exposed port <code>31962</code>, and the host node’s IP(10.20.181.132), (rather than the serivce cluster IP(10.104.43.224). </p>
<p>tips, only check service, won’t tell on which node, the pod is running. so need check the pod, and get its node’s IP. </p>
<p>with <code>docker StartExec</code> with <code>iptable FORWARD</code>, <code>redis-cli</code> on on both ubuntu node and meng node can access the service. </p>
<p>in summary:  if we deploy service as NodePort,  we suppose to access the service with its host node’s IP and the exposed port, from external/outside of k8s.</p>
<h4 id="endpoints"><a href="#endpoints" class="headerlink" title="endpoints"></a>endpoints</h4><p><a href="https://theithollow.com/2019/02/04/kubernetes-endpoints/" target="_blank" rel="external">k8s endpoints</a>. what’s the difference from endpoints to externalIP ? </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">kubectl get endpoints </div><div class="line">NAME         ENDPOINTS           AGE</div><div class="line">kubernetes   10.20.180.12:6443   8d</div></pre></td></tr></table></figure>
<p>it gives us the kubernetes endpoints, which is avaiable on both meng and ubuntu nodes.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">gwm@meng:~$ curl http://10.20.180.12:6443 </div><div class="line">Client sent an HTTP request to an HTTPS server.</div><div class="line">gwm@ubuntu:~$ curl http://10.20.180.12:6443 </div><div class="line">Client sent an HTTP request to an HTTPS server.</div></pre></td></tr></table></figure>
<p>not every service has <code>ENDPOINTS</code>, which gives the way to access outside of the cluster. but <code>NodePort</code> type service can bind to the running pod’s host IP with the exported port.</p>
<p>whenever <a href="https://yq.aliyun.com/articles/679802" target="_blank" rel="external">expose k8s service</a> to either internally or externally, it goes through <code>kube-proxy</code>. when <code>kube-proxy</code> do network transfer, it has two ways: Userspace or Iptables.</p>
<p>clusterIP, is basically expose internnaly, with the service’s cluster IP; while nodePort type, is basically bind the service’s port to each node, so we can access the service from each node with the node’s IP and this fixed port. </p>
<h4 id="apiserver"><a href="#apiserver" class="headerlink" title="apiserver"></a>apiserver</h4><p><a href="https://www.jianshu.com/p/a25e9e613f2c" target="_blank" rel="external">core of k8s: API Server</a>, is the RESTful API for resource POST/GET/DELETE/UPDATE. we can access through: </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">curl apiserver_ip:apiserver_port/api</div><div class="line">curl apiserver_ip:apiserver_port/api/v1/pods</div><div class="line">curl apiserver_ip:apiserver_port/api/v1/services</div><div class="line">CURL apiserver_ip:apiserver_port/api/v1/proxy/nodes/&#123;name&#125;/pods/</div></pre></td></tr></table></figure>
<ul>
<li>check apiServer IP</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl get pods -n kube-system --output=wide</div><div class="line">kube-apiserver-meng            1/1     Running   2          8d     10.20.180.12    meng     &lt;none&gt;           &lt;none&gt;</div></pre></td></tr></table></figure>
<p>if check the <code>LISTEN</code> ports on both worker and master nodes, there are many k8s related ports, some are accessible, while some are not. </p>
<h2 id="k8s-dashboard"><a href="#k8s-dashboard" class="headerlink" title="k8s dashboard"></a>k8s dashboard</h2><p>the following is from <a href="https://kuboard.cn/install/install-k8s-dashboard.html#%E5%AE%89%E8%A3%85" target="_blank" rel="external">dashboard doc in cn</a></p>
<ul>
<li>download src</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">docker search kubernetes-dashboard-amd64</div><div class="line">docker pull k8scn/kubernetes-dashboard-amd64</div><div class="line">docker tag k8scn/kubernetes-dashboard-amd64:latest k8s.gcr.io/kubernetes-dashboard-amd64:latest</div></pre></td></tr></table></figure>
<ul>
<li>clear old dashboard resources </li>
</ul>
<p>if there are old running dashboard, can clear first. </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">kubectl get clusterroles kubernetes-dashboard --output=wide </div><div class="line">kubectl get clusterrolebindings kubernetes-dashboard  --output=wide </div><div class="line">kubectl delete clusterroles kubernetes-dashboard </div><div class="line">kubectl delete clusterrolebindings kubernetes-dashboard </div><div class="line">kubectl delete ns kubernetes-dashboard</div></pre></td></tr></table></figure>
<ul>
<li>start a fresh dashboard</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl apply -f https://kuboard.cn/install-script/k8s-dashboard/v2.0.0-beta5.yaml </div><div class="line">kubectl apply -f https://kuboard.cn/install-script/k8s-dashboard/auth.yaml</div></pre></td></tr></table></figure>
<p>or src from <a href="https://github.com/kubernetes/dashboard/blob/master/aio/deploy/recommended.yaml" target="_blank" rel="external">github/dashboard/recommended.yaml</a>, and run:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl create -f admin-user.yaml</div><div class="line">kubectl create -f recommended.yaml</div></pre></td></tr></table></figure>
<p><code>admin-user.yaml</code> is defined wih <code>admin authorization</code>. if not define or applied, when login to dashboard web UI, it gives some errors like:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">namespaces is forbidden: User <span class="string">"system:serviceaccount:kubernetes-dashboard:kubernetes-dashboard"</span> cannot list resource <span class="string">"namespaces"</span> <span class="keyword">in</span> API group <span class="string">""</span> at the cluster scope</div></pre></td></tr></table></figure>
<p>so there are two tips during creating dashboard.</p>
<ul>
<li><p>auth/admin-user.yaml is required</p>
</li>
<li><p>add NodePort type service to expose dashboard. if not, can’ access dashboard on host machine. </p>
</li>
</ul>
<p>refer from <a href="https://tomoyadeng.github.io/blog/2019/08/11/k8s-dashboard-openssl/index.html" target="_blank" rel="external">deploy dashboard &amp;&amp; metrics-server</a></p>
<ul>
<li><p>create <code>external-http.yaml</code> to expose NodePort service </p>
</li>
<li><p>create <code>admin-user.yaml</code> for admin manage</p>
</li>
</ul>
<ul>
<li>get the ServiceAccount token</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk <span class="string">'&#123;print $1&#125;'</span>)</div></pre></td></tr></table></figure>
<ul>
<li>go to <code>https://nodeIP.6443</code>, tips, dashboard service is using <code>https</code></li>
</ul>
<ul>
<li>login dashboard</li>
</ul>
<p>there are two ways to auth to login dashboard: </p>
<p>– kubeconfig, the configure to access the cluster</p>
<p>– token, every service account has a secret with valid Bearer Token, that can used to login to Dashboard. </p>
<ul>
<li>system checks</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">kubectl get secrets -n kubernetes-dashboard</div><div class="line">kubectl get serviceaccount -n kubernetes-dashboard</div><div class="line">kubectl describe serviceaccount kubernetes-dashboard  -n kubernetes-dashboard</div></pre></td></tr></table></figure>
<h4 id="metrics-server"><a href="#metrics-server" class="headerlink" title="metrics-server"></a>metrics-server</h4><p>metrics-server is a replace of Heapster. </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl edit deploy -n kubernetes-dashboard dashboard-metrics-scraper</div></pre></td></tr></table></figure>
<h4 id="roles"><a href="#roles" class="headerlink" title="roles"></a>roles</h4><p>the right way to create a role:</p>
<ul>
<li>create a ServiceAccount </li>
<li>bind a role for the ServiceAccount(cluster-admin role is needed)</li>
<li>make a ClusterRoleBinding for ServiceAccount </li>
</ul>
<h4 id="list-all-container-images-in-all-ns"><a href="#list-all-container-images-in-all-ns" class="headerlink" title="list all container images in all ns"></a><a href="https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/" target="_blank" rel="external">list all container images in all ns</a></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">kubectl get pods --all-namespaces -o jsonpath=<span class="string">"&#123;..image&#125;"</span> |\</div><div class="line">tr -s <span class="string">'[[:space:]]'</span> <span class="string">'\n'</span> |\</div><div class="line">sort |\</div><div class="line">uniq -c</div></pre></td></tr></table></figure>
<h2 id="refere"><a href="#refere" class="headerlink" title="refere"></a>refere</h2><p><a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/" target="_blank" rel="external">kubectl cheatsheet</a></p>
<p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank" rel="external">deployments from k8s doc</a></p>
<p><a href="https://jekhokie.github.io/k8s/busybox/helm/2020/04/23/small-web-server-to-k8s.html" target="_blank" rel="external">deploy tiny web server to k8s</a></p>
<p><a href="https://learnk8s.io/production-best-practices" target="_blank" rel="external">k8s production best practices</a></p>
<p><a href="https://github.com/containernetworking/cni/blob/master/SPEC.md" target="_blank" rel="external">cni readme</a></p>
<p><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/" target="_blank" rel="external">configure network plugins</a>:</p>
<p><a href="https://www.centos.bz/2017/06/k8s-flannel-network/" target="_blank" rel="external">k8s与flannel网络原理</a></p>
<p><a href="https://network.51cto.com/art/201908/601109.htm" target="_blank" rel="external">清晰脱俗的直解K8S网络</a></p>
<p> <a href="https://shogokobayashi.com/2018/09/27/k8s-ex01-iptables-and-docker/" target="_blank" rel="external">k8s: iptables and docker0</a></p>
<p> <a href="https://blog.csdn.net/whatday/article/details/105197120" target="_blank" rel="external">linux docker and iptables</a></p>
<p><a href="https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/" target="_blank" rel="external">controlling access to k8s APIserver</a></p>
<p><a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" target="_blank" rel="external">understand RBAC auth</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/05/17/k8s-setup-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="David Z.J. Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Serious Autonomous Vehicles">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/05/17/k8s-setup-1/" itemprop="url">k8s setup 1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-05-17T08:50:13-04:00">
                2020-05-17
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/05/17/k8s-setup-1/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2020/05/17/k8s-setup-1/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><p>transfer from docker swarm to K8S finally. this is a lot engineering work, once have some knowledge about docker/swarm. there are two things: </p>
<ul>
<li><p>a more general abstract object. e.g. pod, service/svc, deployment, secret, namespace/ns, role e.t.c.</p>
</li>
<li><p>more DevOps engineering  </p>
</li>
</ul>
<p>previously, I <a href="https://zjli2013.github.io/2020/04/09/play-with-k8s/" target="_blank" rel="external">palyed  with k8s</a> in theory. this time is more about build a k8s cluster in reality. </p>
<h2 id="install-kubeadm"><a href="#install-kubeadm" class="headerlink" title="install kubeadm"></a>install kubeadm</h2><ul>
<li><p>kubeadm,  used to initial cluster</p>
</li>
<li><p>kubectl, the CLI tool for k8s </p>
</li>
<li><p>kubelet, run on all nodes in the cluster</p>
</li>
</ul>
<p>all three commands are required on all nodes. check <a href="https://v1-16.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" target="_blank" rel="external">install kube officially</a></p>
<ul>
<li>swapoff </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo swapoff -a</div></pre></td></tr></table></figure>
<ul>
<li>create a file <code>/etc/apt/sources.list.d/kubernetes.list</code> with the following content:</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">deb https://mirrors.aliyun.com/kubernetes/apt/  kubernetes-xenial main</div></pre></td></tr></table></figure>
<ul>
<li>add gpg key </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">gpg --keyserver keyserver.ubuntu.com --recv-keys BA07F4FB </div><div class="line">gpg --<span class="built_in">export</span> --armor BA07F4FB | sudo apt-key add -</div></pre></td></tr></table></figure>
<ul>
<li>apt install</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update </div><div class="line">sudo apt-get install kubelet kubectl kubeadm</div></pre></td></tr></table></figure>
<p>tips, <code>apt-get install</code> will install v1.18.2. </p>
<ul>
<li>restart kubelet </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">systemctl daemon-reload</div><div class="line">systemctl restart kubelet</div></pre></td></tr></table></figure>
<p>if need degrade to v17.3, do the following: </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install -qy --allow-downgrades kubelet=1.17.3-00</div><div class="line">sudo apt-get install -qy --allow-downgrades kubeadm=1.17.3-00</div><div class="line">sudo apt-get install -qy --allow-downgrades kubectl=1.17.3-00</div></pre></td></tr></table></figure>
<h4 id="kubeadm-setup"><a href="#kubeadm-setup" class="headerlink" title="kubeadm setup"></a>kubeadm setup</h4><p>we setup k8s with kubeadm tool, which requires a list of images:</p>
<ul>
<li>check the required images to start kubeadm</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubeadm config images list</div></pre></td></tr></table></figure>
<p>which returns:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">k8s.gcr.io/kube-apiserver:v1.18.2</div><div class="line">k8s.gcr.io/kube-controller-manager:v1.18.2</div><div class="line">k8s.gcr.io/kube-scheduler:v1.18.2</div><div class="line">k8s.gcr.io/kube-proxy:v1.18.2</div><div class="line">k8s.gcr.io/pause:3.2</div><div class="line">k8s.gcr.io/etcd:3.4.3-0</div><div class="line">k8s.gcr.io/coredns:1.6.7</div></pre></td></tr></table></figure>
<p>the image source above is not aviable, which can be solved by:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubeadm config images pull --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers/</div></pre></td></tr></table></figure>
<p>if the command above doesn’t work well, try to <code>docker pull</code> directly and tag the name back to <code>k8s.gcr.io</code>:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/<span class="variable">$imageName</span></div><div class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/<span class="variable">$imageName</span> k8s.gcr.io/<span class="variable">$imageName</span></div></pre></td></tr></table></figure>
<h4 id="start-a-k8s-cluster"><a href="#start-a-k8s-cluster" class="headerlink" title="start a k8s cluster"></a>start a k8s cluster</h4><p>after the preparation above, finally start a k8s cluster as:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubeadm init --pod-network-cidr=10.4.0.0/16 --cluster_dns=10.3.0.10</div></pre></td></tr></table></figure>
<p>futher, check <a href="https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/" target="_blank" rel="external">kubeadm init options</a>:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">--pod-network-cidr  <span class="comment"># pod network IP range</span></div><div class="line"></div><div class="line">--service-cidr <span class="comment"># default 10.96.0.0/12</span></div><div class="line"></div><div class="line">--service-dns-domain <span class="comment">#cluster.local</span></div></pre></td></tr></table></figure>
<p><code>cluster_dns</code> option is used as the cluster DNS/namespace, which will be used in the configureMap for <code>coreDNS</code> forwarding. </p>
<p>if start successfully, </p>
<ul>
<li>then run the following as a regular user to config safety-verficiation:</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">mkdir -p <span class="variable">$HOME</span>/.kube</div><div class="line">sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</div><div class="line">sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</div></pre></td></tr></table></figure>
<ul>
<li>check</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo kubectl get nodes   <span class="comment">#both nodes are READY</span></div><div class="line">sudo kubectl get pods -n kube-system  <span class="comment">#check system</span></div><div class="line">sudo kubectl describe pod coredns-xxxx  -n kube-system</div></pre></td></tr></table></figure>
<h4 id="add-pod-network"><a href="#add-pod-network" class="headerlink" title="add pod network"></a>add pod network</h4><p>pod network, is the network through which the cluster nodes can communicate with each other.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</div></pre></td></tr></table></figure>
<ul>
<li>worker node to join the new k8s cluster : </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">kubeadm reset</div><div class="line">sudo swapoff -a </div><div class="line">kubeadm join 10.20.180.12:6443 --token o0pcpc.v3v8bafmbu6e4bcs \</div><div class="line">    --discovery-token-ca-cert-hash sha256:2a15d392821f8c51416e49e6ccd5393df6f93d738b24b2132e9a9a19276f4f54</div></pre></td></tr></table></figure>
<p>then cp <code>flannel-cni.conflist</code> into worker node. <code>/etc/cni/net.d/10-flannel.conflist</code> to the same path in worker node.</p>
<p>if check <code>sudo kubectl get pods -n kube-system</code> :  there may come <strong>an error: here found: coredns CrashLoopBackOff or Error</strong></p>
<p>this is due to <code>DNS/nameserver resolving issue in Ubuntu, where</code>coreDNS service<code>forwarding the k8s cluster service to the host</code>/etc/resolv.conf<code>, which only has</code>127.0.0.1`.  the cause for CoreDNS to have CrashLoopBackOff is when a CoreDNS Pod deployed in Kubernetes detects a loop. A number of <a href="https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters" target="_blank" rel="external">workarounds are available</a> to avoid Kubernetes trying to restart the CoreDNS Pod every time CoreDNS detects the loop and exits.</p>
<p>check the <code>coreDNS configMap</code> by :</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl edit cm coredns -n kube-system</div></pre></td></tr></table></figure>
<p>we see something like:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">  prometheus :9153</div><div class="line">#  forward . /etc/resolv.conf</div><div class="line">  forward . 10.3.0.10</div><div class="line">  cache 30</div><div class="line">  loop</div><div class="line">  reload</div><div class="line">  loadbalance</div></pre></td></tr></table></figure>
<p>so modify <code>forward line</code> to <code>forward . 10.3.0.10</code>.  or to delete <code>loop</code> service there, which is not good idea.</p>
<p><a href="https://stackoverflow.com/questions/52645473/coredns-fails-to-run-in-kubernetes-cluster" target="_blank" rel="external">a very good explain</a></p>
<h4 id="test-cluster"><a href="#test-cluster" class="headerlink" title="test cluster"></a>test cluster</h4><p><a href="https://www.bookstack.cn/read/follow-me-install-kubernetes-cluster-1.8.x/08.%E9%AA%8C%E8%AF%81%E9%9B%86%E7%BE%A4%E5%8A%9F%E8%83%BD.md" target="_blank" rel="external">test sample</a></p>
<h4 id="clear-cluster"><a href="#clear-cluster" class="headerlink" title="clear cluster"></a>clear cluster</h4><p><a href="https://www.bookstack.cn/read/follow-me-install-kubernetes-cluster-1.8.x/12.%E6%B8%85%E7%90%86%E9%9B%86%E7%BE%A4.md" target="_blank" rel="external">clear test</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo systemctl stop kubelet kube-proxy flanneld docker</div></pre></td></tr></table></figure>
<h2 id="understand-CNI-container-network-interface"><a href="#understand-CNI-container-network-interface" class="headerlink" title="understand CNI (container network interface)"></a>understand CNI (container network interface)</h2><p>the following network plugin can be found from <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/" target="_blank" rel="external">k8s cluster networking</a></p>
<ul>
<li>backgroud</li>
</ul>
<p>container network is used to connect (container itself) to other containers, host machine or external network. container in runtime has a few network mode:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">none</div><div class="line"></div><div class="line">host</div><div class="line"></div><div class="line">bridge</div></pre></td></tr></table></figure>
<p>CNI brings a general network framework, used to manage network configure and network resources. </p>
<h4 id="coreDNS"><a href="#coreDNS" class="headerlink" title="coreDNS"></a>coreDNS</h4><p>first, run coreDNS as a service in the cluster. then, update <code>kubelet</code> parameters to include IP of coreDNS and the cluster domain. </p>
<p>if there is no existing running Kube-DNS, or need a different <code>CLusterIP</code> for <code>CoreDNS</code>, then need update <code>kubelet</code> configuration to set <code>cluster_dns</code> and <code>cluster_domain</code> appropriately, which can be modified at:</p>
<p><code>/etc/systemd/system/kubelet.service/10-kubeadm.conf</code> with additional options appending at <code>kubelet</code> line :</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">--cluster_dns=10.3.0.10   --cluster_domain=cluster.local</div></pre></td></tr></table></figure>
<ul>
<li>restart kubelet service </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">systemctl status kubelet </div><div class="line">systemctl daemon-reload </div><div class="line">systemctl restart docker</div></pre></td></tr></table></figure>
<h2 id="flannel-mis-usage"><a href="#flannel-mis-usage" class="headerlink" title="flannel mis-usage"></a>flannel mis-usage</h2><p>in the settings above, I manually copy <code>flannel-cni.conflist</code> and <code>/run/flannel/subnet.env</code> to worker node every time, whenever reboot the worker node. if else, the cluster bother the worker node is <code>NotReady</code>.  as we deploy the cluster with <code>kubectl</code>, which is kind of a swarm service deploy tool. so the right way to use <code>flannel</code> should have all <code>k8s.gcr.io/kube-proxy</code>, <code>quay.io/coreos/flannel</code> images at worker node as well. </p>
<p>for version1.17+, <code>flannel</code> replace the default <code>kube-proxy</code>, but it still requires to have <code>kube-proxy</code> running in each node(kubelet). </p>
<p>after restart kubelet, checking <code>pods -n kube-system</code>, it shows <code>kube-proxy</code> and <code>flannel</code> on each node has a <code>Running</code> status. <code>coreDNS</code> services has run the same size of copies as the number of nodes, but we can find that all of them are running on leader node.</p>
<h2 id="understand-pod-in-k8s"><a href="#understand-pod-in-k8s" class="headerlink" title="understand pod in k8s"></a>understand pod in k8s</h2><h4 id="accessing-k8s-pods-from-outside-of-cluster"><a href="#accessing-k8s-pods-from-outside-of-cluster" class="headerlink" title="accessing k8s pods from outside of cluster"></a><a href="http://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster/" target="_blank" rel="external">accessing k8s pods from outside of cluster</a></h4><ul>
<li>hostNetwork: true</li>
</ul>
<p>this option applies to k8s pods, which work as <code>--network=host</code> in docker env. </p>
<p>options can used for create pod: <code>name command args env resources ports stdin tty</code></p>
<p><a href="https://www.mirantis.com/blog/introduction-to-yaml-creating-a-kubernetes-deployment/" target="_blank" rel="external">create pod/deployment using yaml</a></p>
<p><a href="https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/" target="_blank" rel="external">k8s task1: define a command and args for a container</a></p>
<p><a href="https://learnk8s.io/templating-yaml-with-code" target="_blank" rel="external">templating YAML in k8s with real code</a></p>
<p>but <code>hostNetwork</code> is only yaml supported</p>
<ul>
<li>hostPort </li>
</ul>
<p>the container port is exposed to the external network at <hostip>:<hostport>. </hostport></hostip></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">spec:</div><div class="line">	containers:</div><div class="line">		ports:</div><div class="line">		- containerPort: 8086</div><div class="line">		  hostPort: 8086</div></pre></td></tr></table></figure>
<p>hostPort allows to expose a single container port on the hostIP. but the hostIP is dynamic when container restarted </p>
<ul>
<li>nodePort </li>
</ul>
<p>by default, services are accessible at ClusterIP, which is an internal IP address reachable from inside the cluster. to make the service accessible from outside of the cluster, can create a <code>NodePort</code> type service. </p>
<p>once this service is created, the kube-proxy, which runs on each node of the cluster, and listens on all network interfaces is instructed to accept connections on port 30000, (from any IP ?). the incoming traffc is forwardedby the kube-proxy to the selected pods in a round-robin fashion.</p>
<p>this service represents a static endpoint through which the selected pods can be reached. </p>
<ul>
<li>Ingress</li>
</ul>
<p>The Ingress controller is deployed as a Docker container on top of Kubernetes. Its Docker image contains a load balancer like nginx or HAProxy and a controller daemon.</p>
<h4 id="view-pods-and-nodes"><a href="#view-pods-and-nodes" class="headerlink" title="view pods and nodes"></a><a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/" target="_blank" rel="external">view pods and nodes</a></h4><ul>
<li>check running pods on which node </li>
</ul>
<h4 id="resolv-conf-in-k8s-pod"><a href="#resolv-conf-in-k8s-pod" class="headerlink" title="resolv.conf in k8s pod"></a>resolv.conf in k8s pod</h4><p>run as interactive into a k8s pod, then check its <code>resolv.conf</code>:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">nameserver 10.96.0.10</div><div class="line">search default.svc.cluster.local svc.cluster.local cluster.local</div><div class="line">options ndots:5</div></pre></td></tr></table></figure>
<p> <code>10.96.0.10</code> is the K8S DNS server IP, which is actually the service IP of <code>kube-dns</code> service.</p>
<p><strong>interesting</strong>, we can ping neither 10.96.0.10, nor 10.4.0.10, which is not existing service in the cluster, nor 10.3.0.10, which is the coreDNS forwarding IP.</p>
<p>remember during setup the k8s cluster, we had define the coreDNS forwarding to <code>10.3.0.10</code>, is this why I can’t run <code>curl http://&lt;ip&gt;:&lt;port&gt;</code> works ?</p>
<p>check coreDNS service:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">kubectl describe  pods coredns-66bff467f8-59g97 -n kube-system </div><div class="line">Name:                 coredns-66bff467f8-59g97</div><div class="line">Node:                 meng/10.20.180.12</div><div class="line">Labels:               k8s-app=kube-dns</div><div class="line">IP:                   10.4.0.6</div></pre></td></tr></table></figure>
<p>when start <code>coreDNS</code>, is actually used to relace <code>kube-dns</code>. </p>
<h2 id="understand-service-in-k8s"><a href="#understand-service-in-k8s" class="headerlink" title="understand service in k8s"></a>understand service in k8s</h2><p><a href="https://kubernetes.io/docs/concepts/services-networking/service/" target="_blank" rel="external">doc</a></p>
<p>Each Pod gets its own IP address, however in a Deployment, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later.</p>
<p>A Service in Kubernetes is a REST object, similar to a Pod. you can <code>POST</code> a Service definition to the API server to create a new instance. </p>
<p>Kubernetes assigns this Service an IP address, sometimes called the <code>clusterIP</code>, </p>
<h4 id="Virtual-IP-and-service-proxies"><a href="#Virtual-IP-and-service-proxies" class="headerlink" title="Virtual IP and service proxies"></a>Virtual IP and service proxies</h4><p>Every node in a Kubernetes cluster runs a <code>kube-proxy</code>, <code>kube-proxy</code> is responsible for implementing a form of virtual IP for Services, whose is type is any but not <code>ExternalName</code>.</p>
<h4 id="choosing-own-IP-for-service"><a href="#choosing-own-IP-for-service" class="headerlink" title="choosing own IP for service"></a>choosing own IP for service</h4><p>You can specify your own cluster IP address as part of a Service creation request. The IP address that you choose must be a valid IPv4 or IPv6 address from within the service-cluster-ip-range CIDR range that is configured for the API server</p>
<h4 id="discovering-services"><a href="#discovering-services" class="headerlink" title="discovering services"></a>discovering services</h4><ul>
<li><p>ENV variables </p>
</li>
<li><p>DNS </p>
</li>
</ul>
<h4 id="headless-services"><a href="#headless-services" class="headerlink" title="headless services"></a>headless services</h4><p>by explicitly specifying “None” for the cluster IP (<code>.spec.clusterIP</code>).</p>
<h4 id="publishing-services-ServiceTypes"><a href="#publishing-services-ServiceTypes" class="headerlink" title="publishing services(ServiceTypes)"></a>publishing services(ServiceTypes)</h4><p>expose a service to an external IP address, outside of the cluster. </p>
<p>service has four type:</p>
<ul>
<li><p>ClusterIP (default): expose the service on a cluster-internal IP, which is only reachable inside the cluster</p>
</li>
<li><p>NodePort: expose the service on each node’s IP at a static port(<code>NodePort</code>), to access : <nodeip>:<nodeport></nodeport></nodeip></p>
</li>
<li><p>ExternalName: map the services to an <code>externalName</code></p>
</li>
<li><p>LoadBalancer: expose the service externally using third-party load balancer(googl cloud, AWS, kubeadm has none LB)</p>
</li>
</ul>
<p><code>NodePort</code> and <code>LoadBalancer</code> can expose service to public, or outside of the cluster.</p>
<h4 id="external-IPs"><a href="#external-IPs" class="headerlink" title="external IPs"></a>external IPs</h4><p>if there are external IPs that route to one or more cluster nodes, services can be exposed on those externalIPs. </p>
<h2 id="yaml-deployment-of-service-pod"><a href="#yaml-deployment-of-service-pod" class="headerlink" title="yaml deployment of service/pod"></a>yaml deployment of service/pod</h2><p>the previous sample <code>busybox</code>, is running as <code>pod</code>, through  <code>kubectl run busybox</code> ?  so there is no external </p>
<ul>
<li><a href="https://www.jianshu.com/p/6fd42abd9baa" target="_blank" rel="external">deployment obj</a></li>
</ul>
<ul>
<li><a href="https://blog.csdn.net/wucong60/article/details/81699196" target="_blank" rel="external">using yaml file to create service and expose to public</a><br>some basic knowledge: </li>
</ul>
<p>1) pod is like container in docker, which assigned a dynamic IP in runtime, but this pod IP is only visible inside cluster </p>
<p>2) service is an abstract concept of pod, which has an unique exposed IP, and the running pods belong to this service are managed hidden. </p>
<ul>
<li><a href="https://stackoverflow.com/questions/41325087/what-is-the-difference-between-a-pod-and-a-deployment" target="_blank" rel="external">pod or deployment or service</a></li>
</ul>
<p>both pod and deployment are full-fledged objects in k8s API. deployment manages creating Pods by means of <code>ReplicaSets</code>, namely create pods with <code>spec</code> taken from the template. since it’s rather unlikely to create pods directly in a production env.</p>
<p>in production, you will almost never use an <code>object</code> with the type <code>pod</code>. but a <code>deployment</code> object, which needs to keep the <code>replicas(pod)</code> alive. what’s use in practice are:</p>
<p>1) Deployment object, where to specify app containers with some specifications</p>
<p>2) service object</p>
<p>you need <code>service object</code> since pods from deployment object can be killed, scaled up and down, their IP address is not persistent. </p>
<h2 id="kubectrl-commands"><a href="#kubectrl-commands" class="headerlink" title="kubectrl commands"></a>kubectrl commands</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">kubectl get [pods|nodes|namespaces|services|pv] --ns your_namespace</div><div class="line"></div><div class="line">kubectl describe [pods|nodes|namespaces]</div><div class="line"></div><div class="line">kubectl label pods your_pod  new-label=your_label</div><div class="line"></div><div class="line">kubectl apply -f [.yaml|.json]   <span class="comment">#creates and updates resources in the cluster</span></div><div class="line"></div><div class="line">kubectl create deployment service_name --imae=service_image   <span class="comment">#start a single instance of service</span></div><div class="line"></div><div class="line">kubectl rollout <span class="built_in">history</span> deployment/your_service  <span class="comment">#check history of deployment </span></div><div class="line"></div><div class="line">kubectl expose rc your_sevice --port=80 --target-port=8000</div><div class="line"></div><div class="line">kubectl autoscale deployment your_service --min=MIN_Num --max=MAX_Num</div><div class="line"></div><div class="line">kubectl edit your_service <span class="comment">#edit any API resource in your preferred editor </span></div><div class="line"></div><div class="line">kubectl scale --replicas=3 your_service </div><div class="line"></div><div class="line">kubectl delete [pod|service]</div><div class="line"></div><div class="line">kubectl logs your_pod <span class="comment"># dump pod logs </span></div><div class="line"></div><div class="line">kubectl run -i --tty busybox --image=busybox  -- sh  <span class="comment"># run pod as interactive shell </span></div><div class="line"></div><div class="line">kubectl <span class="built_in">exec</span> -ti your_pod -- ls | nslookup kubernetes.default    <span class="comment">#run command in existing pod (1 container case)</span></div></pre></td></tr></table></figure>
<p><code>kubectl</code> is pretty much like <code>docker</code> command and more. </p>
<h2 id="refere"><a href="#refere" class="headerlink" title="refere"></a>refere</h2><p><a href="https://computingforgeeks.com/how-to-setup-3-node-kubernetes-cluster-on-ubuntu-18-04-with-weave-net-cni/" target="_blank" rel="external">blog: setup k8s on 3 ubuntu nodes</a></p>
<p><a href="https://github.com/containernetworking/cni/blob/master/SPEC.md" target="_blank" rel="external">cni readme</a></p>
<p><a href="https://silenceper.com/blog/201809/flannel-in-k8s/" target="_blank" rel="external">flannel for k8s from  silenceper</a></p>
<p><a href="https://blogs.infoblox.com/community/coredns-for-kubernetes-service-discovery/" target="_blank" rel="external">coreDNS for k8s service discovery</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/28/services-deploy-in-docker-swarm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="David Z.J. Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Serious Autonomous Vehicles">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/04/28/services-deploy-in-docker-swarm/" itemprop="url">services deploy in docker swarm</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-04-28T08:54:49-04:00">
                2020-04-28
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/04/28/services-deploy-in-docker-swarm/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2020/04/28/services-deploy-in-docker-swarm/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="backgroud"><a href="#backgroud" class="headerlink" title="backgroud"></a>backgroud</h2><p>our application so far includes the following three services:</p>
<ul>
<li><p>simulator</p>
</li>
<li><p>pythonAPI</p>
</li>
<li><p>redisJQ</p>
</li>
</ul>
<p><strong>docker swarm network</strong> has the following three kind of networks, of course <code>bridge</code> to host network.</p>
<ul>
<li><p>overlay network, services in the same overlay network, can communicate to each other </p>
</li>
<li><p>routing network, the service requested can be hosted in any of the running nodes, further as load balancer.</p>
</li>
<li><p>host network </p>
</li>
</ul>
<p>usually,  <a href="https://docs.microsoft.com/en-us/dotnet/architecture/microservices/multi-container-microservice-net-applications/multi-container-applications-docker-compose" target="_blank" rel="external">multi-container apps can be deployed with docker-compose.yml</a>, check <a href="https://docs.docker.com/compose/compose-file/" target="_blank" rel="external">docker compse</a> for more details.</p>
<h2 id="DNS-service-discovery"><a href="#DNS-service-discovery" class="headerlink" title="DNS service discovery"></a>DNS service discovery</h2><p>the following is an example from (<a href="https://github.com/docker/labs/blob/master/networking/A3-overlay-networking.md" target="_blank" rel="external">overlay networking and service discovery</a>:</p>
<p>my test env includes 2 nodes, with host IP as following. when running docker services, it will generate a responding virtual IP, while which is dynamic assgined. </p>
<table>
<thead>
<tr>
<th>hostname</th>
<th>virtualIP</th>
<th>hostIP</th>
</tr>
</thead>
<tbody>
<tr>
<td>node1</td>
<td>10.0.0.4</td>
<td>xx.20.181.132</td>
</tr>
<tr>
<td>node2</td>
<td>10.0.0.2</td>
<td>xx.20.180.212</td>
</tr>
</tbody>
</table>
<p>a common issue when try first to use overlay network in swarm, e.g. ping the other service doesn’t work, check <code>/etc/resolv.conf</code> file: </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># cat /etc/resolv.conf </span></div><div class="line">nameserver 8.8.8.8</div><div class="line">nameserver 8.8.4.4</div><div class="line">``` </div><div class="line"></div><div class="line">the default `dns=8.8.8.8` can<span class="string">'t ping either hostIP or any docker0 IP. the reason can find [#moby/#23910](https://github.com/moby/moby/issues/23910): When spinning up a container, Docker will by default check for a DNS server defined in /etc/resolv.conf in the host OS, and if it doesn'</span>t find one, or finds only 127.0.0.1, will opt to use Google<span class="string">'s public DNS server 8.8.8.8. </span></div><div class="line"></div><div class="line">one solution mentioned:</div><div class="line"></div><div class="line">* cat /etc/docker/daemon.json </div><div class="line"></div><div class="line">```xml</div><div class="line">&#123;</div><div class="line"></div><div class="line">		"dns": ["172.17.0.1", "your.dns.server.ip", "8.8.8.8"]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>add a file <code>/etc/NetworkManager/dnsmasq.d/docker-bridge.conf</code></li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">listen-address=172.17.0.1</div></pre></td></tr></table></figure>
<p>so basically, the default DNS setting only listens to DNS requests from 127.0.0.1 (ie, your computer). by adding <code>listen-address=172.17.0.1</code>, it tells it to listen to the docker bridge also. very importantly, <strong>Docker DNS server is used only for the user created networks</strong>, so need create a new docker network. if use the default <code>ingress</code> overlay network, the dns setup above still doesn’t work.</p>
<p>another solution is using <strong>host network</strong>, mentioned <a href="https://l-lin.github.io/post/2018/2018-09-03-docker_ubuntu_18_dns/" target="_blank" rel="external">using host DNS in docker container with Ubuntu</a> </p>
<h4 id="test-virtualIP-network"><a href="#test-virtualIP-network" class="headerlink" title="test virtualIP network"></a>test virtualIP network</h4><ul>
<li>create a new docker network</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker network create -d overlay l3</div></pre></td></tr></table></figure>
<p>why here need a new network ?  due to <strong>Docker DNS server(172.17.0.1) is used only for the user created networks</strong></p>
<ul>
<li>start the service with the created network: </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">docker service create --name lg --replicas 2 --network l3 20.20.180.212:5000/lg</div><div class="line">``` </div><div class="line"></div><div class="line">* check vip on both nodes</div><div class="line"></div><div class="line">```sh</div><div class="line"> docker network inspect l3</div></pre></td></tr></table></figure>
<p>check vip by the line <code>IPv4Address</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">node1 vip : `10.0.1.5/24`</div><div class="line">node2 vip: `10.0.1.6/24`</div></pre></td></tr></table></figure>
<ul>
<li>go to the running container </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">docker <span class="built_in">exec</span> -it 788e667ea9cb  /bin/bash </div><div class="line">apt-get update &amp;&amp; apt-get install iputils-ping</div><div class="line">ping 10.0.1.5</div><div class="line">ping 10.0.1.6</div></pre></td></tr></table></figure>
<ul>
<li>now ping service-name directly </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ping lg </div><div class="line">PING lg (10.0.1.2) 56(84) bytes of data.</div></pre></td></tr></table></figure>
<ul>
<li>inspect service </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">docker service inspect lg </div><div class="line">            <span class="string">"VirtualIPs"</span>: [</div><div class="line">                &#123;</div><div class="line">                    <span class="string">"Addr"</span>: <span class="string">"10.0.1.2/24"</span></div><div class="line">                &#125;</div><div class="line">            ]</div></pre></td></tr></table></figure>
<ul>
<li>ping host IP from contianer vip</li>
</ul>
<p>as far as we add both <code>host dns</code> and <code>docker0 dns</code> to the dns option in <code>/etc/docker/daemon.json</code>, the container vip can ping host IP.</p>
<h4 id="assign-ENV-variable-from-script"><a href="#assign-ENV-variable-from-script" class="headerlink" title="assign ENV variable from script"></a>assign ENV variable from script</h4><ul>
<li>get services vip </li>
</ul>
<p><a href="https://serverfault.com/questions/925267/how-do-i-list-docker-vip-addresses" target="_blank" rel="external">get vip list</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">vip=`sudo docker service inspect --format <span class="string">'&#123;&#123;.Endpoint.VirtualIPs&#125;&#125;'</span>  lgsvl | awk <span class="string">'&#123;print substr($2, 1, length($2)-5)&#125;'</span>`</div><div class="line"><span class="built_in">echo</span> <span class="variable">$vip</span></div></pre></td></tr></table></figure>
<ul>
<li>create docker service with runtime env</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">ping -c 1 lg  | awk <span class="string">'NR==1 &#123;print $2&#125;'</span> </div><div class="line">``` </div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">## multi-services test</span></div><div class="line"></div><div class="line"><span class="comment">#### run all services in single docker mode</span></div><div class="line"></div><div class="line">```sh</div><div class="line">docker run -it -p 6379:6379 --mount <span class="built_in">source</span>=jq-vol,target=/job_queue redisjq /bin/bash </div><div class="line">docker run xx.xx.xx.xxx:5000/lg</div><div class="line">docker run -it --mount <span class="built_in">source</span>=jq-vol,target=/pythonAPI/job_queue xx.xx.xx.xxx:5000/redispythonapi  /bin/bash</div></pre></td></tr></table></figure>
<ul>
<li>check docker-IP of <code>lg</code> :</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">docker sh </div><div class="line">docker container inspect &lt;lg&gt; <span class="comment">#get its IP-address</span></div></pre></td></tr></table></figure>
<ul>
<li>update <code>SIMULATOR_HOST</code> for <code>redispythonapi</code> </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">docker <span class="built_in">exec</span> -it &lt;redispythonapi&gt; /bin/bash</div><div class="line"><span class="built_in">export</span> SIMULATOR_HOST=lg_ip_address <span class="comment">#from the step above</span></div><div class="line">./redis_worker.sh  <span class="comment">#where all python scenarios are running in queue</span></div></pre></td></tr></table></figure>
<p>here we can check the lg container’s IP is <code>172.17.0.3</code> and redispythonapi’s IP is <code>172.17.0.4</code>, then update <code>start_redis_worker.sh</code> with <code>SIMULATOR_HOST=172.17.0.3</code></p>
<ul>
<li>get the container IP</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">docker container ls  | grep -w xx.xx.xx.xxx:5000/lg | awk <span class="string">'&#123;print $1&#125;'</span> </div><div class="line">docker inspect --format=<span class="string">'&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;'</span>  $(docker container ls  | grep -w xx.xx.xx.xxx:5000/lg | awk <span class="string">'&#123;print $1&#125;'</span> )</div></pre></td></tr></table></figure>
<h2 id="assign-a-special-IP-to-service-in-swarm"><a href="#assign-a-special-IP-to-service-in-swarm" class="headerlink" title="assign a special IP to service in swarm"></a>assign a special IP to service in swarm</h2><p>docker network create support <a href="https://docs.docker.com/engine/reference/commandline/network_create/" target="_blank" rel="external">subnet</a>, which only ip-addressing function, namely we can use custom-defined virtual IP for our services. a sample: </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">docker network create -d overlay \</div><div class="line">  --subnet=192.168.10.0/25 \</div><div class="line">  --subnet=192.168.20.0/25 \</div><div class="line">  --gateway=192.168.10.100 \</div><div class="line">  --gateway=192.168.20.100 \</div><div class="line">  --aux-address=<span class="string">"my-router=192.168.10.5"</span> --aux-address=<span class="string">"my-switch=192.168.10.6"</span> \</div><div class="line">  --aux-address=<span class="string">"my-printer=192.168.20.5"</span> --aux-address=<span class="string">"my-nas=192.168.20.6"</span> \</div><div class="line">  my-multihost-network</div><div class="line">``` </div><div class="line"></div><div class="line">we can run our application as:</div><div class="line"></div><div class="line">```sh</div><div class="line">docker network create --driver=overlay --subnet=192.168.10.0/28  lgsvl-net</div><div class="line">docker service create --name lgsvl --replicas 2 --network lgsvl-net --host <span class="string">"host:192.168.10.2"</span>  xx.xx.xx.xxx:5000/lgsvl</div><div class="line">docker service create --name redis --replicas 1 --network lgsvl-net  -p 6379:6379 --mount <span class="built_in">source</span>=jq-vol,target=/job_queue  --constraint <span class="string">'node.hostname==ubuntu'</span> xx.xx.xx.xxx:5000/redisjq</div><div class="line">docker service create --name pythonapi --replicas 1 --network lgsvl-net --mount <span class="built_in">source</span>=jq-vol,target=/pythonAPI/job_queue  xx.xx.xx.xxx:5000/redispythonapi</div></pre></td></tr></table></figure>
<p><a href="https://www.cnblogs.com/milantgh/p/4075912.html" target="_blank" rel="external">understand subnet mask</a>. IP address include <code>master IP</code> and <code>subnet mask</code>, we choose <code>28</code> here, basically generate about <code>2^(32-28)-2= 14</code> avaiable IP address in the subnet. but in a swarm env, subnet IPs are consuming more as the nodes or replicas of service increase.</p>
<p>taking an example, with 2-nodes and 2-replicas of service, <strong>5 subnet IPs are occupied, rather than 2</strong></p>
<p>run <code>docker network inspect lgsvl-net</code> on both nodes:</p>
<ul>
<li>on node1 gives:</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">lg.1 IPV4Address: 192.168.10.11/28</div><div class="line">lgssvl-net-endpoint: 192.168.10.6/28</div></pre></td></tr></table></figure>
<ul>
<li>on node2 gives:</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">lg.2 IPV4Address: 192.168.10.4/28</div><div class="line">lgssvl-net-endpoint: 192.168.10.3/28</div><div class="line">``` </div><div class="line"></div><div class="line">* `docker service inspect lg` gives:</div><div class="line"></div><div class="line">```xml</div><div class="line">VirualIPs: 192.168.10.2/28</div></pre></td></tr></table></figure>
<p>clearly 5 IP address are occupied. and the IP for each internal service is random picked, there is no gurantee <code>service</code> will always get the first avaiable IP. </p>
<h4 id="docker-serivce-with-–ip"><a href="#docker-serivce-with-–ip" class="headerlink" title="docker serivce with –ip"></a>docker serivce with –ip</h4><p><strong>only docker run –ip</strong> works, there is no similar <code>--ip</code> option in <code>docker service create</code>. but a lot case require this feature: <a href="https://success.docker.com/article/how-do-i-publish-a-service-port-to-a-specific-ip-address" target="_blank" rel="external">how to publish a service port to a specific IP address</a>,  when publishing a port using <code>--publish</code>, the port is published to <code>0.0.0.0</code> instead of a specific interface’s assigned IP. and there is no way to assign an fixed IP to a service in swarm. </p>
<p>a few disscussion in <a href="https://github.com/moby/moby/issues/26696" target="_blank" rel="external">moby/#26696</a>, <a href="https://github.com/moby/moby/issues/25303" target="_blank" rel="external">add more options to `service create</a>, <a href="https://github.com/moby/moby/issues/24170#issuecomment-300771012" target="_blank" rel="external">a possible solution</a>, <a href="https://github.com/moby/moby/issues/24170" target="_blank" rel="external">Static/Reserved IP addresses for swarm services</a></p>
<p>mostly depend on the issues like “ip address is not known in advance, since docker service launched in swarm mode will end up on multiple docker servers”. there should not be applicable to docker swarm setup, since if one decides to go with docker swarm service, has to accept that service will run on multiple hosts with different ip addresses. I.e. trying to attach service / service instance to specific IP address somewhat contradicting with docker swarm service concept. </p>
<p><a href="https://docs.docker.com/engine/reference/commandline/service_create/" target="_blank" rel="external">docker service create</a> does have options <code>--host host:ip-address</code> and <code>--hostname</code> and similar in <a href="https://docs.docker.com/engine/reference/commandline/service_update/" target="_blank" rel="external">docker service update</a>  support <code>host-add</code> and <code>host-rm</code>.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">$ docker service create --name redis --host <span class="string">"redishost:192.168.10.2"</span> --hostname myredis redis:3.0.6</div><div class="line">``` </div><div class="line"></div><div class="line"><span class="keyword">then</span> <span class="built_in">exec</span> into the running container, we can check out `192.168.10.2 redishost` is one line <span class="keyword">in</span> `/etc/hosts` and `myredis` is <span class="keyword">in</span> `/etc/hostname`</div><div class="line"></div><div class="line">but remember, the DNS <span class="keyword">for</span> this hostIP(192.168.10.2) should be first configured <span class="keyword">in</span> the docker engine DNS list. <span class="keyword">if</span> not, even the hostIP is <span class="keyword">in</span> the arrange of the subnet, it is unreachable from the containers.</div><div class="line"></div><div class="line">[another explain](https://www.freecodecamp.org/news/docker-nginx-letsencrypt-easy-secure-reverse-proxy-40165ba3aee2/): by default docker containers are put on their own network. This means that you won’t be able to access your container by it’s hostname, <span class="keyword">if</span> you’re sitting on your laptop on your host network. It is only the containers that are able to access each other through their hostname.</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#### dnsrr vs vip </span></div><div class="line"></div><div class="line">```sh</div><div class="line">--endpoint-mode dnsrr</div></pre></td></tr></table></figure>
<p><code>dnsrr</code> mode, namely DNS round Robin mode, when query Docker’s internal DNS server to get the IP address of the service, it will return IP address of every node running the service.</p>
<p><code>vip</code> mode, return the IP address of only one of the running cntainers. </p>
<p>When you submit a DNS query for a service name to the Swarm DNS service, it will return one, or all, the IP addresses of the related containers, depending on the endpoint-mode.</p>
<p><a href="https://stackoverflow.com/questions/42875572/how-to-load-balance-websocket-apps-in-docker-swarm" target="_blank" rel="external">dnsrr vs vip</a>: Swarm defaults to use a virtual ip (endpoint-mode vip). So each service gets its own IP address, and the swarm load balancer assigns the request as it sees fit; to prevent a service from having an IP address, you can run <code>docker service update your_app --endpoint-mode dnsrr</code>, which will allow an internal load balancer to <code>run a DNS query against a service name</code>, to discover each task/container’s IP for a given service</p>
<p>in our case, we want to assign a speical IP to the service in swarm. <strong>why?</strong> because our app has websocket server/client communicataion, which is IP address based. we can’ assign <code>service name</code> for WS server/client.</p>
<p>check another issue: <a href="https://stackoverflow.com/questions/54101508/how-do-you-dockerize-a-websocket-server" target="_blank" rel="external">dockerlize a websocket server</a></p>
<h2 id="global-mode-to-run-swarm"><a href="#global-mode-to-run-swarm" class="headerlink" title="global mode to run swarm"></a>global mode to run swarm</h2><p>when deploying service with global mode, namely each node will only run one replicas of the service. the benefit of <code>global mode</code> is we can always find the node IP, no matter the IP address is in host network or user-defined overlay network/subnetwork. </p>
<h4 id="get-service’s-IP-in-global-mode"><a href="#get-service’s-IP-in-global-mode" class="headerlink" title="get service’s IP in global mode"></a>get service’s IP in global mode</h4><p><a href="https://www.cyberciti.biz/faq/unix-linux-check-if-port-is-in-use-command/" target="_blank" rel="external">get listened port</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">root@c7279faebefa:/lgsvl<span class="comment"># netstat -tulpn | grep LISTEN</span></div><div class="line">tcp        0      0 127.0.0.11:33263        0.0.0.0:*               LISTEN      -               </div><div class="line">tcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN      7/lgsvl_Core.x86_64</div><div class="line">tcp        0      0 0.0.0.0:8181            0.0.0.0:*               LISTEN      7/lgsvl_Core.x86_64</div></pre></td></tr></table></figure>
<p>both <code>8080</code> and <code>8181</code> is listening after <code>lgsvl</code> service started. on the <code>lgsvl</code> side, we can modify it to listen on all address with <code>8181</code> port. then the following <a href="https://www.jb51.net/article/173931.htm" target="_blank" rel="external">python script</a> to find node’s IP:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> socket</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_host_ip</span><span class="params">()</span>:</span></div><div class="line">  <span class="keyword">try</span>:</div><div class="line">    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)</div><div class="line">    s.connect((<span class="string">'8.8.8.8'</span>, <span class="number">80</span>))</div><div class="line">    ip = s.getsockname()[<span class="number">0</span>]</div><div class="line">  <span class="keyword">finally</span>:</div><div class="line">    s.close()</div><div class="line"> </div><div class="line">  <span class="keyword">return</span> ip</div></pre></td></tr></table></figure>
<p>in this way, no need to pre-define the <code>SIMULATOR_HOST</code> env variable at first. the pythonAPI only need to find out its own IP and detect if <code>8181</code> is listening on in runtime.</p>
<h4 id="container-vs-service"><a href="#container-vs-service" class="headerlink" title="container vs service"></a>container vs service</h4><p><a href="https://stackoverflow.com/questions/43408493/what-is-the-difference-between-docker-service-and-docker-container" target="_blank" rel="external">difference between service and container</a>:</p>
<ul>
<li><p><code>docker run</code> is used to create a standalone container</p>
</li>
<li><p><code>docker service</code> is the one run in a distributed env. when create a service, you specify which container image to use and which commands to execue inside the running containers. </p>
</li>
</ul>
<p>There is only one command(no matter the format is <code>CMD</code> <code>ENTRYPOINT</code> or <code>command in docker-compose</code>) that docker will run to start your container, and when that command exits, the container exits. in swarm service mode, with default restart option(any), the container run and exit and restart again with a different containeID. check <a href="https://stackoverflow.com/questions/56328330/dockerfile-docker-compose-and-swarm-mode-lifecycle" target="_blank" rel="external">dockerfile, docker-compose and swarm mode lifecycle</a> for details. </p>
<h4 id="docker-container-restart-policy"><a href="#docker-container-restart-policy" class="headerlink" title="docker container restart policy:"></a><a href="https://rollout.io/blog/ensuring-containers-are-always-running-with-dockers-restart-policy/" target="_blank" rel="external">docker container restart policy</a>:</h4><p><a href="https://docs.docker.com/config/containers/start-containers-automatically/" target="_blank" rel="external">docker official doc: start containers automatically</a></p>
<ul>
<li><p>no, simply doesn’t restart under any circumstance </p>
</li>
<li><p>on-failure, to restart if the exit code has error. user can specify a maximum number of times Docker will automatically restart the container; the container will not restart when app exit with a successful exit code. </p>
</li>
<li><p>unless-stopped, only stop when Docker is stopped. so most time, this policy work exactly like <code>always</code>, one exception, when a container is stopped and the server is reboot or the DOcker serivce is restarted, the container won’t restart itself. if the container was running before the reboot, the container would be restarted once the system restarted.</p>
</li>
<li><p>always, tells Docker to restart the container under any circumstance. and the service will restart even with reboot. any other policy can’t restart when system reboot.</p>
</li>
</ul>
<p>similar restart policy can be found in :</p>
<ul>
<li><a href="https://docs.docker.com/compose/compose-file/#restart_policy" target="_blank" rel="external">docker-compose restart policy</a></li>
</ul>
<ul>
<li><a href="https://docs.docker.com/engine/reference/commandline/service_create/" target="_blank" rel="external">docker service create restat-condition</a></li>
</ul>
<h4 id="keep-redisJQ-alive-in-python-script"><a href="#keep-redisJQ-alive-in-python-script" class="headerlink" title="keep redisJQ alive in python script"></a>keep redisJQ alive in python script</h4><p>by default setup, <code>redis server</code> is keep restarting and running, which make the <code>pythonapi</code> service always report: <code>redis.exceptions.ConnectionError: Error 111 connecting to xx.xxx.xxx:6379. Connection refused.</code></p>
<p>so we can keep redisJQ alive in python script level by simply a <code>while loop</code>.</p>
<p>for test purpose, we also make pythonAPI restart policy as none, so the service won’t automatically run even with empty jobQueue.</p>
<p>the final test script can run in the following:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">docker service create --name lgsvl --network lgsvl-net --mode global xx.xx.xx.xxx:5000/lgsvl</div><div class="line">docker service create --name redis -p 6379:6379 --network lgsvl-net --mount <span class="built_in">source</span>=jq-vol,target=/job_queue  --constraint <span class="string">'node.hostname==ubuntu'</span> xx.xx.xx.xxx:5000/redisjq </div><div class="line">docker service create --name pythonapi --network lgsvl-net --mode global --mount <span class="built_in">source</span>=jq-vol,target=/pythonAPI/job_queue --restart-condition none xx.xx.xx.xxx:5000/pythonapi</div></pre></td></tr></table></figure>
<h4 id="use-python-variable-in-os-system"><a href="#use-python-variable-in-os-system" class="headerlink" title="use python variable in os.system"></a>use python variable in os.system</h4><p><a href="https://stackoverflow.com/questions/27128851/how-to-use-python-variable-in-os-system" target="_blank" rel="external">sample</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">os.system(<span class="string">"ls -lt %s"</span>%your_py_variable)</div></pre></td></tr></table></figure>
<h2 id="proxy-in-docker-swarm"><a href="#proxy-in-docker-swarm" class="headerlink" title="proxy in docker swarm"></a>proxy in docker swarm</h2><p><a href="https://www.haproxy.com/blog/haproxy-on-docker-swarm-load-balancing-and-dns-service-discovery/" target="_blank" rel="external">HAProxy</a></p>
<p>Routing external traffic into the cluster, load balancing across replicas, and DNS service discovery are a few capabilities that require finesse. but proxy can’t either assign a special IP to a special service, neither can expose the service with a fixed IP, so in our case, no helpful.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/28/redis-task-queue-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="David Z.J. Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Serious Autonomous Vehicles">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/04/28/redis-task-queue-2/" itemprop="url">redis task queue (2)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-04-28T08:52:02-04:00">
                2020-04-28
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/04/28/redis-task-queue-2/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2020/04/28/redis-task-queue-2/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><p>currently, we add <code>job_queue</code> list inside Dockerfile by <code>COPY</code> the <code>job_queue</code> folder from local host to the docker image, which is not dynamically well, and can’t support additionaly scenarios.</p>
<p>to design a <code>redisJQ</code> service that can used in swarm/k8s env, need consider <code>DNS</code> to make the servie available and <code>shared volume</code> to share the data in job queue to other services. </p>
<h4 id="ceph-rbd-driver-for-docker"><a href="#ceph-rbd-driver-for-docker" class="headerlink" title="ceph rbd driver for docker"></a>ceph rbd driver for docker</h4><p>ceph can store files in three ways:</p>
<ul>
<li><p>rbd, block storage, which usually used with virtualization kvm</p>
</li>
<li><p>object storage, through <code>radosgw</code> api, or access by boto3 APIs. </p>
</li>
<li><p>cephfs, mount ceph as file system</p>
</li>
</ul>
<p>the first idea is from local host mount to remote volume(e.g. ceph storage) mount. there are a few popular <a href="https://ceph.io/geen-categorie/getting-started-with-the-docker-rbd-volume-plugin/" target="_blank" rel="external">rbd-drive plugins</a>: </p>
<ul>
<li><p><a href="https://github.com/yp-engineering/rbd-docker-plugin" target="_blank" rel="external">Yp engineering</a></p>
</li>
<li><p>AcalephStorage </p>
</li>
<li><p>Volplugin</p>
</li>
<li><p><a href="https://rexray.readthedocs.io/en/stable/" target="_blank" rel="external">rexray.io</a></p>
</li>
</ul>
<p>check <a href="https://ceph.com/geen-categorie/getting-started-with-the-docker-rbd-volume-plugin/" target="_blank" rel="external">ceph rbd driver</a> to understand more details. </p>
<p>to support rbd-driver plugin in docker, the ceph server also need support block device driver, which sometime is not avaiable, as most small ceph team would support one or another type, either objec storage or block storage. and that’s our situation. so we can’t go with <code>rbd-driver plugin</code>.</p>
<p>another way is to use <a href="https://gitlab.com/n0r1sk/docker-volume-cephfs" target="_blank" rel="external">docker volume cephfs</a>, similar reason our ceph team doesn’t support <code>cephfs</code>.  </p>
<h4 id="ceph-object-storage-access"><a href="#ceph-object-storage-access" class="headerlink" title="ceph object storage access"></a>ceph object storage access</h4><p>as the ceph team can support boto3 API to access ceph, which gives us the one and only way to access scenarios: boto3.</p>
<p>basically the <code>redis_JQ</code> first download all scenaio files from remote ceph through boto3 APIs, then scan the downloaded files into JQ, then feed into the python executors in front.</p>
<h4 id="s3-client"><a href="#s3-client" class="headerlink" title="s3 client"></a>s3 client</h4><ul>
<li><a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html" target="_blank" rel="external">aws cli</a></li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">curl <span class="string">"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip"</span> -o <span class="string">"awscliv2.zip"</span></div><div class="line">unzip awscliv2.zip</div><div class="line">sudo ./aws/install</div><div class="line">/usr/<span class="built_in">local</span>/bin/aws --version</div></pre></td></tr></table></figure>
<ul>
<li><a href="https://github.com/s3tools/s3cmd" target="_blank" rel="external">s3cmd</a></li>
</ul>
<h4 id="access-files-in-folders-in-s3-bucket"><a href="#access-files-in-folders-in-s3-bucket" class="headerlink" title="access files in folders in s3 bucket"></a>access files in folders in s3 bucket</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_files</span><span class="params">(self, bucket_name, folder_name)</span>:</span></div><div class="line">    files_with_prefix = self.s3_client.list_objects_v2(Bucket=bucket_name, Prefix=folder_name)</div><div class="line">    scenario_basename = <span class="string">"/pythonAPI/job_queue/scenario"</span></div><div class="line">    i = <span class="number">0</span> </div><div class="line">    <span class="keyword">for</span> file_ <span class="keyword">in</span> files_with_prefix[<span class="string">"Contents"</span>]:</div><div class="line">        scenario_name = scenario_basename + <span class="string">"%d"</span>%i + <span class="string">".py"</span></div><div class="line">        print(scenario_name)</div><div class="line">        self.download_file(bucket_name, file_[<span class="string">'Key'</span>], scenario_name, <span class="keyword">False</span>)</div><div class="line">        time.sleep(<span class="number">0.01</span>)</div><div class="line">        i += <span class="number">1</span></div></pre></td></tr></table></figure>
<h2 id="manage-python-modules"><a href="#manage-python-modules" class="headerlink" title="manage python modules"></a>manage python modules</h2><p>during the project, we really need take care python packages installed by <code>apt-get</code>, <code>pip</code> and <code>conda</code>, if not there will conflicts among different version of modules:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import websockets</div><div class="line">Trackbac:</div><div class="line">  File <span class="string">"/usr/lib/python3/dist-packages/websockets/compatibility.py"</span>, line 8</div><div class="line">    asyncio_ensure_future = asyncio.async           <span class="comment"># Python &lt; 3.5</span></div></pre></td></tr></table></figure>
<p>so it’s better to use conda or python virtual-env to separate the different running envs. and install packages by <code>conda install</code> is better choice, than the global <code>apt-get install</code>:</p>
<ul>
<li><p><a href="https://anaconda.org/conda-forge/websockets" target="_blank" rel="external">conda install ws</a></p>
</li>
<li><p><a href="https://anaconda.org/anaconda/pandas" target="_blank" rel="external">conda install pandas</a> </p>
</li>
<li><p><a href="https://github.com/danielhrisca/asammdf" target="_blank" rel="external">conda install asammdf</a></p>
</li>
<li><p><a href="https://anaconda.org/conda-forge/botocore" target="_blank" rel="external">conda install botocore</a></p>
</li>
<li><p><a href="https://anaconda.org/anaconda/sqlalchemy" target="_blank" rel="external">conda install sqlalchemy</a> </p>
</li>
<li><p><a href="https://anaconda.org/anaconda/websocket-client" target="_blank" rel="external">conda install websocket-client</a></p>
</li>
<li><p><a href="https://anaconda.org/anaconda/redis" target="_blank" rel="external">conda install redis</a></p>
</li>
<li><p><a href="https://anaconda.org/anaconda/boto3" target="_blank" rel="external">conda install boto3</a></p>
</li>
</ul>
<h4 id="basic-of-python-import"><a href="#basic-of-python-import" class="headerlink" title="basic of python import"></a>basic of python import</h4><ul>
<li><p>module,  any <code>*.py</code> file, where its name is the file name </p>
</li>
<li><p>package,  any folder containing a file named <code>__init__.py</code> in i, its name is the name of the folder.</p>
</li>
</ul>
<p>When a module named <code>module1</code> is imported, the interpreter first searches for a built-in module with that name. If not found, it then searches for a file named <code>module1.py</code> or a folder named <code>module1</code> in a list of directories given by the variable <code>sys.path</code></p>
<p><code>sys.path</code> is initialized from 3 locations:</p>
<ul>
<li><p>the directory containing the input script, or the current directory</p>
</li>
<li><p><code>PYTHONPATH</code> </p>
</li>
<li><p>the installation-dependent default</p>
</li>
</ul>
<p>if using <code>export PYTHONPATH</code> directly, it works. but once defined in <code>~/.bashrc</code> it doesn’t actually triggered in <code>conda</code> env.<br>it simpler to add the root directory of the project to the <code>PYTHONPATH</code> environment variable and then running all the scripts from that directory’s level and changing the import statements accordingly. <code>import</code> search for your packages in specific places, listed in sys.path. and The current directory is always appended to this list</p>
<h2 id="redis-service"><a href="#redis-service" class="headerlink" title="redis service"></a>redis service</h2><p>the common error: <code>redis.exceptions.ConnectionError: Error 111 connecting to 10.20.181.132:6379. Connection refused.</code> , which basically means the system can’t connect to redis server, due to by default redis <a href="https://www.cnblogs.com/likwo/p/5903377.html" target="_blank" rel="external">only allow localhost</a> to access. so we need configure non-localhost IP to access redis db. </p>
<ul>
<li>check redis-server running status </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ps aux | grep redis-server</div><div class="line">netstat -tunple | grep 6379</div><div class="line">redis-cli info</div></pre></td></tr></table></figure>
<ul>
<li>shutdown redis-server</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo <span class="built_in">kill</span> -9 <span class="variable">$pid</span></div></pre></td></tr></table></figure>
<h4 id="redis-server-amp-redis-cli"><a href="#redis-server-amp-redis-cli" class="headerlink" title="redis-server &amp; redis-cli"></a>redis-server &amp; redis-cli</h4><p><code>redis-server</code> start redis server with a default config file at <code>/etc/redis/redis.config</code></p>
<p>a few item in the configure file need take care:</p>
<ul>
<li>bind, check <a href="https://stackoverflow.com/questions/16120287/redis-bind-to-more-than-one-ip" target="_blank" rel="external">here</a></li>
</ul>
<p>the default setting is to <code>bind 127.0.0.1</code>,which means redis db is stored and only can be access through <code>localhost</code>.  for our case, to allow hostIP(10.20.181.132), or even any IP to access, need :</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bind 0.0.0.0</div></pre></td></tr></table></figure>
<ul>
<li>redislog, default place at <code>/var/log/redis/redis-server.log</code> </li>
</ul>
<ul>
<li><p>requirepass, for <a href="https://blog.csdn.net/qq_40460909/article/details/84838245" target="_blank" rel="external">security issues</a>, please consider this item</p>
</li>
<li><p>login client with hostIP</p>
</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">redis-cli -h 10.20.181.132</div></pre></td></tr></table></figure>
<ul>
<li><a href="https://www.tutorialspoint.com/redis/redis_lists.htm" target="_blank" rel="external">basic operation of redis-cli</a></li>
</ul>
<p>log in redis-cli first, then run the following: </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">LPUSH your_list_name item1 </div><div class="line">LPUSH your_list_name item2 </div><div class="line">LLEN your_list_name</div><div class="line">EXISTS your_list_name</div></pre></td></tr></table></figure>
<h4 id="redis-service-in-docker"><a href="#redis-service-in-docker" class="headerlink" title="redis service in docker"></a>redis service in docker</h4><p>the following is an example from <a href="https://docker-doc.readthedocs.io/zh_CN/stable/examples/running_redis_service.html" target="_blank" rel="external">create a redis service</a></p>
<ul>
<li>connect to the redis container directly</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker run -it redis-image /usr/bin/redis-server /etc/redis/myconfig.conf</div></pre></td></tr></table></figure>
<p>in this way, <code>redis service</code> will use its docker VIP, which can be checked from:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">docker ps </div><div class="line">docker inspect &lt;container_id&gt;</div></pre></td></tr></table></figure>
<p>which will give somehing like:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">"bridge": &#123;</div><div class="line">    "Gateway": "172.17.0.1",</div><div class="line">    "IPAddress": "172.17.0.2",</div></pre></td></tr></table></figure>
<p>then the redis-server can connect by:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">redis-cli -h 172.17.0.2</div></pre></td></tr></table></figure>
<ul>
<li>connect to the host os </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker run -it -p 6379 redis-image /usr/bin/redis-server /etc/redis/myconfig.conf</div></pre></td></tr></table></figure>
<p><strong>the redis container has exported 6379, which may map to another port on host os</strong>, check:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">docker ps </div><div class="line">docker port &lt;container_id&gt; 6379  <span class="comment">#gives the &lt;exernal_port&gt; on host</span></div><div class="line">rdis-cli -h 10.20.181.132 -p &lt;external_port&gt;</div></pre></td></tr></table></figure>
<ul>
<li>run redis service with host network</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker run -it --network=host redis-image /usr/bin/redis-server /etc/redis/myconfig.conf</div></pre></td></tr></table></figure>
<p>in this way, there is no bridge network, or docker VIP. the host IP and port is directly used. so the following works</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">redis-cli -h 10.20.181.132 -p 6379</div></pre></td></tr></table></figure>
<p>A good way now, is to map host redis_port to container redis_port, and use the second way to access redis.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker run -it -p 6379:6379 redisjq /bin/bash</div></pre></td></tr></table></figure>
<p>tips, need to confirm <code>6379</code> port at host machine is free.</p>
<h4 id="share-volumes-among-multi-volumes"><a href="#share-volumes-among-multi-volumes" class="headerlink" title="share volumes among multi volumes"></a>share volumes among multi volumes</h4><p>the problem is <code>redisjq service</code> download all scenarios scripts in its own docker container, and only store the scenario name in <code>redis db</code>. when <code>redis_worker</code> access the db, there is no real python scripts. so need to share this <code>job-queue</code> to all <code>redis_worker</code>s</p>
<p><a href="https://docs.docker.com/storage/volumes/" target="_blank" rel="external">mount volume</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker run -it -p 6379:6379 --mount <span class="built_in">source</span>=jq-vol,target=/job_queue redisjq /bin/bash</div></pre></td></tr></table></figure>
<h4 id="start-pythonapi-to-access-the-shared-volume"><a href="#start-pythonapi-to-access-the-shared-volume" class="headerlink" title="start pythonapi to access the shared volume"></a>start pythonapi to access the shared volume</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker run -it --mount <span class="built_in">source</span>=jq-vol,target=/pythonAPI/job_queue  redispythonapi  /bin/bash</div></pre></td></tr></table></figure>
<h2 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h2><p><a href="https://www.cnblogs.com/sammyliu/p/5095976.html" target="_blank" rel="external">qemu/kvm &amp; ceph: rbd drver in qemu</a></p>
<p><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-ceph-rbd-docker-storage/index.html" target="_blank" rel="external">基于 Ceph RBD 实现 Docker 集群的分布式存储</a></p>
<p><a href="https://my.oschina.net/u/561758/blog/1813161" target="_blank" rel="external">rexray/rbd 参考</a></p>
<p><a href="https://stackoverflow.com/questions/22359132/access-cephfs-inside-docker-container-without-mounting-cephfs-on-the-host" target="_blank" rel="external">access cephFS inside docker container without mounting cephFS in host</a></p>
<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/user-guide/using-folders.html" target="_blank" rel="external">how to use folders in s3 bucket</a></p>
<p><a href="https://chrisyeh96.github.io/2017/08/08/definitive-guide-python-imports.html" target="_blank" rel="external">the definitive guide to python import statements</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/09/play-with-k8s/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="David Z.J. Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Serious Autonomous Vehicles">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/04/09/play-with-k8s/" itemprop="url">play with k8s</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-04-09T03:31:23-04:00">
                2020-04-09
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/04/09/play-with-k8s/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2020/04/09/play-with-k8s/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="basic-k8s"><a href="#basic-k8s" class="headerlink" title="basic k8s"></a>basic k8s</h2><h4 id="kubeadm-init"><a href="#kubeadm-init" class="headerlink" title="kubeadm init"></a>kubeadm init</h4><p>init options can be: </p>
<ul>
<li><p><code>--apiserver-bind-port int32</code>, by default, port=6443</p>
</li>
<li><p><code>--config string</code>, can pass in a kubeadm.config file to create a kube master node </p>
</li>
<li><p><code>--node-name string</code>, attach node name </p>
</li>
<li><p><code>--pod-network-cidr string</code>,  used to set the IP address range for all Pods. </p>
</li>
<li><p><code>--service-cide string</code>,  set service CIDRs, default value is <code>10.96.0.0/12</code></p>
</li>
<li><p><code>--service-dns-domain string</code>, default value is <code>cluster.local</code> </p>
</li>
<li><code>--apiserver-advertise-address string</code>, the broadcast listened address by API Server</li>
</ul>
<h4 id="nodes-components"><a href="#nodes-components" class="headerlink" title="nodes components"></a>nodes components</h4><table>
<thead>
<tr>
<th>IP</th>
<th>hostname</th>
<th>components</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.0.1</td>
<td>master</td>
<td>kube-apiserver, kube-controller-manager, kube-scheduler, etcd, kubelet, docker, flannel, dashboard</td>
</tr>
<tr>
<td>192.168.0.2</td>
<td>worker</td>
<td>kubelet, docker, flannel</td>
</tr>
</tbody>
</table>
<h4 id="ApiServer"><a href="#ApiServer" class="headerlink" title="ApiServer"></a>ApiServer</h4><p>when first launch Kubelet, it will send the Bootstrapping request to kube-apiserver, which then verify the sent token is matched or not. </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">--advertise-address = <span class="variable">$&#123;master_ip&#125;</span></div><div class="line"></div><div class="line">--<span class="built_in">bind</span>-address = <span class="variable">$&#123;master_ip&#125;</span>  <span class="comment">#can't be 127.0.0.1 </span></div><div class="line"></div><div class="line">--insecure-bind-address = <span class="variable">$&#123;master_ip&#125;</span></div><div class="line"></div><div class="line">--token-auth-file = /etc/kubernets/token.csv</div><div class="line"></div><div class="line">--service-node-port-range=<span class="variable">$&#123;NODE_PORT_RANGE&#125;</span></div></pre></td></tr></table></figure>
<p><a href="https://jimmysong.io/posts/kubernetes-installation-on-centos/" target="_blank" rel="external">how to configure master node</a></p>
<h4 id="cluster-IP"><a href="#cluster-IP" class="headerlink" title="cluster IP"></a>cluster IP</h4><p>it’s the service IP, which is internal, usually expose the service name.</p>
<p>the cluse IP default values as following:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">--service-cluster-ip-range=10.254.0.0/16</div><div class="line">--service-node-port-range=30000-32767</div></pre></td></tr></table></figure>
<h2 id="k8s-in-practice"><a href="#k8s-in-practice" class="headerlink" title="k8s in practice"></a>k8s in practice</h2><p><img src="https://jimmysong.io/kubernetes-handbook/images/kubernetes-high-level-component-archtecture.jpg" alt="image"></p>
<p><img src="https://jimmysong.io/kubernetes-handbook/images/kubernetes-whole-arch.png" alt="image"></p>
<p><img src="https://jimmysong.io/kubernetes-handbook/images/kubernetes-layers-arch.png" alt="image"></p>
<p>blueKing is a k8s solution from TenCent. here is a <a href="https://bk.tencent.com/docs/document/5.1/13/652" target="_blank" rel="external">quickstart</a>: </p>
<ul>
<li><p>create a task</p>
</li>
<li><p>add agnet for the task </p>
</li>
<li><p>run the task &amp; check the sys log </p>
</li>
<li><p>create task pipeline (CI/CD)</p>
</li>
</ul>
<h4 id="create-a-new-service-in-k8s"><a href="#create-a-new-service-in-k8s" class="headerlink" title="create a new service in k8s"></a>create a new service in k8s</h4><ul>
<li>create namespace for speical bussiness</li>
<li>create serivces, <a href="https://www.jianshu.com/p/3f24bbee72ad" target="_blank" rel="external">pull images from private registry hub</a></li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl create -f  my-nginx-2.yaml</div><div class="line">kubctl get pods -o wide</div></pre></td></tr></table></figure>
<h4 id="how-external-access-to-k8s-pod-service"><a href="#how-external-access-to-k8s-pod-service" class="headerlink" title="how external access to k8s pod service ?"></a>how external access to k8s pod service ?</h4><p>pod has itw own special IP and a lifecyle period. once the node shutdown, the controller manager can transfer the pod to another node. when multi pods, provides the same service for front-end users, the front end users doesn’t care which pod is exactaly running, then here is the concept of <code>service</code>:</p>
<pre><code>service is an abstraction which defines a logical set of Pods and a policy by which to access them
</code></pre><p>service can be defined by <code>yaml</code>, <code>json</code>, the target pod can be define by <code>LabelSeletor</code>. a few ways to expose service:</p>
<ul>
<li><p><code>ClusterIP</code>, which is the default way, which only  works inside k8s cluster</p>
</li>
<li><p><code>NodePort</code>, which use NAT to provide external access through a special port, the port should be among <code>8400~9000</code>. then in this way, no matter where exactly the pod is running on, when access <code>*.portID</code>, we can get the serivce. </p>
</li>
<li><p><code>LoadBalancer</code></p>
</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">kubectl get services</div><div class="line">kubectl expose your_service --<span class="built_in">type</span>=<span class="string">"NodePort"</span> --port 8889</div><div class="line">kubctl describe your_service</div></pre></td></tr></table></figure>
<h4 id="use-persistent-volume"><a href="#use-persistent-volume" class="headerlink" title="use persistent volume"></a>use persistent volume</h4><ul>
<li><p>access external sql </p>
</li>
<li><p>use volume </p>
</li>
</ul>
<p><code>volume</code> is for persistent, <code>k8s volume</code> is similar like <code>docker volume</code>, working as dictory, when mount a volume to a pod, all containers in that pod can access that volume. </p>
<ul>
<li>EmptyDir</li>
<li>hostPath</li>
<li>external storage service(aws, azure), k8s can directly use cloud storage as volume, or distributed storage system(ceph):</li>
</ul>
<p><a href="https://www.cnblogs.com/benjamin77/p/9940266.html" target="_blank" rel="external">sample</a></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">apiVersion: v1</div><div class="line">kind: Pod</div><div class="line">metadata: </div><div class="line">	name: using-ebs</div><div class="line">metadata:</div><div class="line">	name: using-ceph</div><div class="line">spec:</div><div class="line">	containers:</div><div class="line">		-image: busybox1</div><div class="line">		 name: using-ebs</div><div class="line">		 volumeMounts:</div><div class="line">		 	-mountPath: /test-ebs</div><div class="line">		 	name: ebs-volume</div><div class="line">		-image: busybox2</div><div class="line">			name: using-ceph</div><div class="line">			volumeMounts:</div><div class="line">				-name: ceph-volume</div><div class="line">				mountPath: /test-ceph</div><div class="line">				</div><div class="line">		volumes:</div><div class="line">		-name: ebs-volume</div><div class="line">		awsElasticBlockStore:</div><div class="line">			volumeID:  <span class="tag">&lt;<span class="name">volume_id</span>&gt;</span></div><div class="line">			fsType: ext4</div><div class="line">		</div><div class="line">		-name: ceph-volume</div><div class="line">		cephfs:</div><div class="line">			path: /path/in/ceph</div><div class="line">			monitors: "10.20.181.112:6679"</div><div class="line">			secretFile: "/etc/ceph/admin/secret"</div></pre></td></tr></table></figure>
<h4 id="containers-communication-in-same-pod"><a href="#containers-communication-in-same-pod" class="headerlink" title="containers communication in same pod"></a>containers communication in same pod</h4><p>first, <a href="https://www.mirantis.com/blog/multi-container-pods-and-container-communication-in-kubernetes/" target="_blank" rel="external">containers in the same pod</a>, the share same network namespace and same iPC namespace, and shared volumes. </p>
<ul>
<li>shared volumes in a pod </li>
</ul>
<p>when one container writes logs or other files to the shared directory, and the other container reads from the shared directory. </p>
<p><img src="https://cdn.mirantis.com/wp-content/uploads/2017/08/MultiContainerPods.png?resize=565x288" alt="image"></p>
<ul>
<li>inter-process communication(IPC)</li>
</ul>
<p>as they share the same IPC namespace, they can communicate with each other using standard ipc, e.g. POSIX shared memory, SystemV semaphores</p>
<p><img src="https://cdn.mirantis.com/wp-content/uploads/2017/08/multicontainerpodproducerconsumer.png?resize=565x288" alt="image"></p>
<ul>
<li>inter-container network communication</li>
</ul>
<p>containers in a pod are accessible via <code>localhost</code>, as they share the same network namespace. for externals, the observable host name is the pod’s name, as containers all have the same IP and port space, so need differnt ports for each container for incoming connections.</p>
<p><img src="https://cdn.mirantis.com/wp-content/uploads/2017/08/multicontainerwebapp.png?resize=565x288" alt="image"></p>
<p>basically, the external incoming HTTP request to port 80 is forwarded to port 5000 on localhost, in pod, and which is not visiable to external. </p>
<h4 id="how-two-services-communicate"><a href="#how-two-services-communicate" class="headerlink" title="how two services communicate"></a><a href="https://stackoverflow.com/questions/45720084/how-to-make-two-kubernetes-services-talk-to-each-other" target="_blank" rel="external">how two services communicate</a></h4><ul>
<li>ads_runner</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">	name: ads_runner</div><div class="line">spec:</div><div class="line">	selector:</div><div class="line">		app: ads</div><div class="line">		tier: api</div><div class="line">	ports:</div><div class="line">		-protocol: TCP</div><div class="line">		 port: 5000</div><div class="line">		 nodePort: 30400</div><div class="line">	type: NodePort</div></pre></td></tr></table></figure>
<p>if there is a need to autoscale the service, check<br><a href="https://stackoverflow.com/questions/37377119/in-kubernetes-how-do-i-autoscale-based-on-the-size-of-a-queue" target="_blank" rel="external">k8s autoscale based on the size of queue</a>. </p>
<ul>
<li>redis-job-queue</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">	name: redis-job-queue</div><div class="line">spec:</div><div class="line">	selector:</div><div class="line">		app: redis</div><div class="line">		tier: broker</div><div class="line">	ports:</div><div class="line">		-portocol: TCP </div><div class="line">		port: 6379</div><div class="line">		targetPort: [the port exposed by Redis pod]</div></pre></td></tr></table></figure>
<p>ads_runner can reach Redis by address: <code>redis-server:6379</code> in the k8s cluster.</p>
<p><a href="https://redis.io/topics/replication" target="_blank" rel="external">redis replication</a> has great async mechanism to support multi redis instance simutanously, when need scale the redis service, it is ok to start a few replicas of <code>redis</code> service as well.</p>
<h4 id="redis-work-queue"><a href="#redis-work-queue" class="headerlink" title="redis work queue"></a>redis work queue</h4><p>check <a href="https://zjli2013.github.io/2020/04/09/redis-task-queue/" target="_blank" rel="external">redis task queue</a>: </p>
<ul>
<li>start a storage service(redis) to hold the work queue</li>
<li>create a queu, and fill it with messages, each message represents one task to be done</li>
<li>start a job that works on tasks from the queue</li>
</ul>
<h3 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h3><p><a href="https://jimmysong.io/kubernetes-handbook/cloud-native/play-with-kubernetes.html" target="_blank" rel="external">jimmysong</a></p>
<p><a href="https://github.com/Tencent/bk-cmdb" target="_blank" rel="external">BlueKing configure manage DB</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1501395" target="_blank" rel="external">k8s: volumes and persistent storage</a></p>
<p><a href="https://cdn.mirantis.com/wp-content/uploads/2017/08/MultiContainerPods.png?resize=565x288" target="_blank" rel="external">multi-container pods and container communication in k8s</a></p>
<p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/" target="_blank" rel="external">k8s doc: communicate between containers in the same pod using a shared volume</a></p>
<p><a href="https://kubemq.io" target="_blank" rel="external">kubeMQ: k8s message queue broker</a></p>
<p><a href="https://blog.cloudthat.com/3-types-of-cluster-networking-in-kubernetes/" target="_blank" rel="external">3 types of cluster networking in k8s</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="David Z.J. Lee" />
          <p class="site-author-name" itemprop="name">David Z.J. Lee</p>
           
              <p class="site-description motion-element" itemprop="description">what I don't know</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">179</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">48</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/ZJLi2013" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/zhengjia13/" target="_blank" title="LinkedIn">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  LinkedIn
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">David Z.J. Lee</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    
      <script id="dsq-count-scr" src="https://zjlee.disqus.com/count.js" async></script>
    

    

  




	





  








  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

</body>
</html>
